{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "605131bc-9ffd-4577-989c-d2a351154b7c",
   "metadata": {
    "id": "605131bc-9ffd-4577-989c-d2a351154b7c"
   },
   "source": [
    "# Optimización de performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64202104-0539-47d8-b0fe-07fddf947f63",
   "metadata": {
    "id": "64202104-0539-47d8-b0fe-07fddf947f63"
   },
   "source": [
    "## Preparación de ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e561d81-f99a-4381-8c64-6a421060dcea",
   "metadata": {
    "id": "9e561d81-f99a-4381-8c64-6a421060dcea"
   },
   "source": [
    "### Carga de módulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c205970-93d6-403b-94bc-8bce9d409ef3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:47:47.439337Z",
     "start_time": "2024-10-05T13:47:47.428353Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:29:17.530746Z",
     "iopub.status.busy": "2023-02-25T15:29:17.530604Z",
     "iopub.status.idle": "2023-02-25T15:29:18.307641Z",
     "shell.execute_reply": "2023-02-25T15:29:18.307212Z",
     "shell.execute_reply.started": "2023-02-25T15:29:17.530728Z"
    },
    "id": "4c205970-93d6-403b-94bc-8bce9d409ef3",
    "outputId": "ca43f93e-9173-47b4-c5bd-85ee139db1f2",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.4.2.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.4.2.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data Wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Data Viz\n",
    "import cufflinks as cf\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, BayesianRidge, Lars\n",
    "\n",
    "# Enviroment setup\n",
    "cf.go_offline()\n",
    "# pd.set_option(\"max_columns\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca8f218-5c8f-4371-8644-45dec9dfe22c",
   "metadata": {
    "id": "4ca8f218-5c8f-4371-8644-45dec9dfe22c"
   },
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb0a962-8e93-4847-8e72-2bd63ea2a891",
   "metadata": {
    "id": "ccb0a962-8e93-4847-8e72-2bd63ea2a891"
   },
   "source": [
    "### Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "917ed613-3a95-4160-a774-3c1174fcf83c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:47:48.285166Z",
     "start_time": "2024-10-05T13:47:48.276458Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:29:18.311066Z",
     "iopub.status.busy": "2023-02-25T15:29:18.310659Z",
     "iopub.status.idle": "2023-02-25T15:29:18.316365Z",
     "shell.execute_reply": "2023-02-25T15:29:18.315826Z",
     "shell.execute_reply.started": "2023-02-25T15:29:18.311045Z"
    },
    "id": "917ed613-3a95-4160-a774-3c1174fcf83c",
    "outputId": "e4a41ff2-e1f6-420d-e6e9-bb65b4da4edb",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning:\n",
      "\n",
      "Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this case special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows:\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and:\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f2ca8f-c9f2-429d-b7bb-057513f38d12",
   "metadata": {
    "id": "12f2ca8f-c9f2-429d-b7bb-057513f38d12"
   },
   "source": [
    "#### Creación de TAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23fadb75-c298-47b4-8455-6d918d6e1326",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:47:48.733388Z",
     "start_time": "2024-10-05T13:47:48.729097Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:29:18.419577Z",
     "iopub.status.busy": "2023-02-25T15:29:18.419326Z",
     "iopub.status.idle": "2023-02-25T15:29:18.423693Z",
     "shell.execute_reply": "2023-02-25T15:29:18.423167Z",
     "shell.execute_reply.started": "2023-02-25T15:29:18.419559Z"
    },
    "id": "23fadb75-c298-47b4-8455-6d918d6e1326",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = boston[\"data\"], columns=boston[\"feature_names\"])\n",
    "df[\"target\"] = boston[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aea13d6-5404-46a0-adcb-d47aa99019ad",
   "metadata": {
    "id": "2aea13d6-5404-46a0-adcb-d47aa99019ad"
   },
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1a2cc48-e227-4170-b561-169c00e8e8a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:47:49.214531Z",
     "start_time": "2024-10-05T13:47:49.193450Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:29:20.058414Z",
     "iopub.status.busy": "2023-02-25T15:29:20.057966Z",
     "iopub.status.idle": "2023-02-25T15:29:20.072533Z",
     "shell.execute_reply": "2023-02-25T15:29:20.072081Z",
     "shell.execute_reply.started": "2023-02-25T15:29:20.058396Z"
    },
    "id": "f1a2cc48-e227-4170-b561-169c00e8e8a4",
    "outputId": "564d0ccd-c155-44b7-f5d7-3013d0ccef0f",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  target  \n",
       "0     15.3  396.90   4.98    24.0  \n",
       "1     17.8  396.90   9.14    21.6  \n",
       "2     17.8  392.83   4.03    34.7  \n",
       "3     18.7  394.63   2.94    33.4  \n",
       "4     18.7  396.90   5.33    36.2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74f4ab8e-d774-4c23-8253-d8f51ec4dbf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:47:49.495301Z",
     "start_time": "2024-10-05T13:47:49.461650Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:29:20.278668Z",
     "iopub.status.busy": "2023-02-25T15:29:20.278473Z",
     "iopub.status.idle": "2023-02-25T15:29:20.309887Z",
     "shell.execute_reply": "2023-02-25T15:29:20.309325Z",
     "shell.execute_reply.started": "2023-02-25T15:29:20.278652Z"
    },
    "id": "74f4ab8e-d774-4c23-8253-d8f51ec4dbf1",
    "outputId": "4104ce1c-18e3-447d-acc3-1488cf0785f5",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.613524</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.601545</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.677083</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.613524   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.601545   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.677083   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT      target  \n",
       "count  506.000000  506.000000  \n",
       "mean    12.653063   22.532806  \n",
       "std      7.141062    9.197104  \n",
       "min      1.730000    5.000000  \n",
       "25%      6.950000   17.025000  \n",
       "50%     11.360000   21.200000  \n",
       "75%     16.955000   25.000000  \n",
       "max     37.970000   50.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "326d96fd-5730-42fc-a23b-f4529609f4d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:47:49.717189Z",
     "start_time": "2024-10-05T13:47:49.714548Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:29:20.415836Z",
     "iopub.status.busy": "2023-02-25T15:29:20.415230Z",
     "iopub.status.idle": "2023-02-25T15:29:20.418947Z",
     "shell.execute_reply": "2023-02-25T15:29:20.418415Z",
     "shell.execute_reply.started": "2023-02-25T15:29:20.415817Z"
    },
    "id": "326d96fd-5730-42fc-a23b-f4529609f4d0",
    "outputId": "763e9622-cb5c-41ae-fab3-49906a51f776",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(boston[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6f7697-425b-4c95-aa2c-492ac795b0a7",
   "metadata": {
    "id": "1c6f7697-425b-4c95-aa2c-492ac795b0a7"
   },
   "source": [
    "\n",
    "### Segmentación de sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a77dbc3-4c78-46a4-ae94-4256f6b750a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:47:50.208932Z",
     "start_time": "2024-10-05T13:47:50.206047Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:29:20.898894Z",
     "iopub.status.busy": "2023-02-25T15:29:20.898337Z",
     "iopub.status.idle": "2023-02-25T15:29:20.901532Z",
     "shell.execute_reply": "2023-02-25T15:29:20.900834Z",
     "shell.execute_reply.started": "2023-02-25T15:29:20.898875Z"
    },
    "id": "7a77dbc3-4c78-46a4-ae94-4256f6b750a8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tgt = \"target\"\n",
    "ls_pred = [x for x in df.columns if x not in [tgt]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c3afcf8-55ea-41aa-9204-1666ac6e00b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:47:50.466976Z",
     "start_time": "2024-10-05T13:47:50.463173Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:29:21.157139Z",
     "iopub.status.busy": "2023-02-25T15:29:21.156789Z",
     "iopub.status.idle": "2023-02-25T15:29:21.162779Z",
     "shell.execute_reply": "2023-02-25T15:29:21.162121Z",
     "shell.execute_reply.started": "2023-02-25T15:29:21.157116Z"
    },
    "id": "5c3afcf8-55ea-41aa-9204-1666ac6e00b9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df[ls_pred]\n",
    "y = df[[tgt]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15aaa226-0969-4b5c-80d5-0f519a684f7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:47:50.710403Z",
     "start_time": "2024-10-05T13:47:50.700730Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:29:21.380212Z",
     "iopub.status.busy": "2023-02-25T15:29:21.380013Z",
     "iopub.status.idle": "2023-02-25T15:29:21.387445Z",
     "shell.execute_reply": "2023-02-25T15:29:21.386952Z",
     "shell.execute_reply.started": "2023-02-25T15:29:21.380195Z"
    },
    "id": "15aaa226-0969-4b5c-80d5-0f519a684f7e",
    "outputId": "1c049582-033e-4210-d7d7-5ed53699882d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     target\n",
       "0      24.0\n",
       "1      21.6\n",
       "2      34.7\n",
       "3      33.4\n",
       "4      36.2\n",
       "..      ...\n",
       "501    22.4\n",
       "502    20.6\n",
       "503    23.9\n",
       "504    22.0\n",
       "505    11.9\n",
       "\n",
       "[506 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[tgt]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e877b7f0-d943-4158-ae72-bb08ab8d7f32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:47:50.933008Z",
     "start_time": "2024-10-05T13:47:50.927229Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:29:21.615855Z",
     "iopub.status.busy": "2023-02-25T15:29:21.615571Z",
     "iopub.status.idle": "2023-02-25T15:29:21.620646Z",
     "shell.execute_reply": "2023-02-25T15:29:21.620081Z",
     "shell.execute_reply.started": "2023-02-25T15:29:21.615834Z"
    },
    "id": "e877b7f0-d943-4158-ae72-bb08ab8d7f32",
    "outputId": "9b1b153b-e5ca-4787-e752-4476d631f5ea",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      24.0\n",
       "1      21.6\n",
       "2      34.7\n",
       "3      33.4\n",
       "4      36.2\n",
       "       ... \n",
       "501    22.4\n",
       "502    20.6\n",
       "503    23.9\n",
       "504    22.0\n",
       "505    11.9\n",
       "Name: target, Length: 506, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[tgt]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b23ae1-d441-47cb-a559-ae4495dd4ec6",
   "metadata": {
    "id": "38b23ae1-d441-47cb-a559-ae4495dd4ec6"
   },
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6d5c1d0-91a8-4e36-b130-9acea77d00e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:47:51.443135Z",
     "start_time": "2024-10-05T13:47:51.439040Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:31:59.701819Z",
     "iopub.status.busy": "2023-02-25T15:31:59.701361Z",
     "iopub.status.idle": "2023-02-25T15:31:59.705383Z",
     "shell.execute_reply": "2023-02-25T15:31:59.704932Z",
     "shell.execute_reply.started": "2023-02-25T15:31:59.701797Z"
    },
    "id": "b6d5c1d0-91a8-4e36-b130-9acea77d00e9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd711ab7-8584-4b86-af72-a17aa7a29427",
   "metadata": {
    "id": "fd711ab7-8584-4b86-af72-a17aa7a29427"
   },
   "source": [
    "#### Escalamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba488f86-c124-4ced-9303-df7c6de3dd7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:47:51.940294Z",
     "start_time": "2024-10-05T13:47:51.937650Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:32:01.259010Z",
     "iopub.status.busy": "2023-02-25T15:32:01.258608Z",
     "iopub.status.idle": "2023-02-25T15:32:01.261717Z",
     "shell.execute_reply": "2023-02-25T15:32:01.261078Z",
     "shell.execute_reply.started": "2023-02-25T15:32:01.258991Z"
    },
    "id": "ba488f86-c124-4ced-9303-df7c6de3dd7b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sc = MinMaxScaler()\n",
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd567f34-1ae7-4c51-8b0b-ffdb30872ca2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:47:52.179526Z",
     "start_time": "2024-10-05T13:47:52.172480Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:32:01.415712Z",
     "iopub.status.busy": "2023-02-25T15:32:01.415488Z",
     "iopub.status.idle": "2023-02-25T15:32:01.421362Z",
     "shell.execute_reply": "2023-02-25T15:32:01.420487Z",
     "shell.execute_reply.started": "2023-02-25T15:32:01.415697Z"
    },
    "id": "cd567f34-1ae7-4c51-8b0b-ffdb30872ca2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "Xs = pd.DataFrame(data=sc.fit_transform(X_train), columns=X_train.columns, index=X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff6ef708-15f4-43b4-aeb2-447d81f65e6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:47:52.428211Z",
     "start_time": "2024-10-05T13:47:52.408753Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:32:01.628281Z",
     "iopub.status.busy": "2023-02-25T15:32:01.627905Z",
     "iopub.status.idle": "2023-02-25T15:32:01.642752Z",
     "shell.execute_reply": "2023-02-25T15:32:01.642159Z",
     "shell.execute_reply.started": "2023-02-25T15:32:01.628250Z"
    },
    "id": "ff6ef708-15f4-43b4-aeb2-447d81f65e6e",
    "outputId": "36b5c1b2-91ee-433a-9fc6-864f313a521c",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>-0.220274</td>\n",
       "      <td>-0.483582</td>\n",
       "      <td>1.313864</td>\n",
       "      <td>3.627671</td>\n",
       "      <td>0.446644</td>\n",
       "      <td>3.048254</td>\n",
       "      <td>0.895969</td>\n",
       "      <td>-0.804421</td>\n",
       "      <td>-0.497260</td>\n",
       "      <td>0.016469</td>\n",
       "      <td>-1.693467</td>\n",
       "      <td>0.332031</td>\n",
       "      <td>-1.327544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>-0.397055</td>\n",
       "      <td>3.085086</td>\n",
       "      <td>-1.347196</td>\n",
       "      <td>-0.275659</td>\n",
       "      <td>-1.170539</td>\n",
       "      <td>1.932224</td>\n",
       "      <td>-1.887187</td>\n",
       "      <td>1.250462</td>\n",
       "      <td>-0.854802</td>\n",
       "      <td>-0.325732</td>\n",
       "      <td>-1.693467</td>\n",
       "      <td>0.415574</td>\n",
       "      <td>-1.358196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.379386</td>\n",
       "      <td>-0.483582</td>\n",
       "      <td>-0.607255</td>\n",
       "      <td>-0.275659</td>\n",
       "      <td>-0.889660</td>\n",
       "      <td>-0.880465</td>\n",
       "      <td>-1.243004</td>\n",
       "      <td>0.665410</td>\n",
       "      <td>-0.735621</td>\n",
       "      <td>-1.041243</td>\n",
       "      <td>-0.235082</td>\n",
       "      <td>0.433898</td>\n",
       "      <td>-0.321864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>-0.372506</td>\n",
       "      <td>-0.483582</td>\n",
       "      <td>-0.049266</td>\n",
       "      <td>-0.275659</td>\n",
       "      <td>-0.540689</td>\n",
       "      <td>0.059043</td>\n",
       "      <td>-0.577467</td>\n",
       "      <td>0.292500</td>\n",
       "      <td>-0.616440</td>\n",
       "      <td>-0.767482</td>\n",
       "      <td>0.083940</td>\n",
       "      <td>0.409425</td>\n",
       "      <td>-0.210932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>-0.334678</td>\n",
       "      <td>0.381550</td>\n",
       "      <td>-1.053039</td>\n",
       "      <td>-0.275659</td>\n",
       "      <td>0.804127</td>\n",
       "      <td>1.342843</td>\n",
       "      <td>0.814112</td>\n",
       "      <td>-0.920421</td>\n",
       "      <td>-0.497260</td>\n",
       "      <td>-0.848366</td>\n",
       "      <td>-2.468233</td>\n",
       "      <td>0.325280</td>\n",
       "      <td>-0.629844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>-0.397995</td>\n",
       "      <td>2.976944</td>\n",
       "      <td>-1.585251</td>\n",
       "      <td>-0.275659</td>\n",
       "      <td>-1.110959</td>\n",
       "      <td>2.318822</td>\n",
       "      <td>-1.307066</td>\n",
       "      <td>0.939528</td>\n",
       "      <td>-0.616440</td>\n",
       "      <td>-0.904362</td>\n",
       "      <td>-1.830190</td>\n",
       "      <td>0.401710</td>\n",
       "      <td>-1.378631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.382918</td>\n",
       "      <td>-0.483582</td>\n",
       "      <td>-0.607255</td>\n",
       "      <td>-0.275659</td>\n",
       "      <td>-0.889660</td>\n",
       "      <td>-0.169999</td>\n",
       "      <td>-2.211058</td>\n",
       "      <td>0.975794</td>\n",
       "      <td>-0.735621</td>\n",
       "      <td>-1.041243</td>\n",
       "      <td>-0.235082</td>\n",
       "      <td>0.270790</td>\n",
       "      <td>-0.964098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>-0.386606</td>\n",
       "      <td>0.597833</td>\n",
       "      <td>-0.877152</td>\n",
       "      <td>-0.275659</td>\n",
       "      <td>-0.847103</td>\n",
       "      <td>0.248695</td>\n",
       "      <td>-0.032936</td>\n",
       "      <td>1.728417</td>\n",
       "      <td>-0.139717</td>\n",
       "      <td>-0.723929</td>\n",
       "      <td>0.585259</td>\n",
       "      <td>0.433898</td>\n",
       "      <td>-0.829813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>-0.387264</td>\n",
       "      <td>1.246681</td>\n",
       "      <td>-0.683068</td>\n",
       "      <td>3.627671</td>\n",
       "      <td>-0.898172</td>\n",
       "      <td>1.431833</td>\n",
       "      <td>-0.702032</td>\n",
       "      <td>0.508743</td>\n",
       "      <td>-0.616440</td>\n",
       "      <td>-0.910584</td>\n",
       "      <td>-0.371806</td>\n",
       "      <td>0.341675</td>\n",
       "      <td>-0.929067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.313930</td>\n",
       "      <td>-0.483582</td>\n",
       "      <td>-0.420753</td>\n",
       "      <td>-0.275659</td>\n",
       "      <td>-0.123626</td>\n",
       "      <td>-0.814816</td>\n",
       "      <td>0.027567</td>\n",
       "      <td>0.013180</td>\n",
       "      <td>-0.616440</td>\n",
       "      <td>-0.580827</td>\n",
       "      <td>1.177728</td>\n",
       "      <td>0.362169</td>\n",
       "      <td>-0.165684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>354 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
       "163 -0.220274 -0.483582  1.313864  3.627671  0.446644  3.048254  0.895969   \n",
       "202 -0.397055  3.085086 -1.347196 -0.275659 -1.170539  1.932224 -1.887187   \n",
       "45  -0.379386 -0.483582 -0.607255 -0.275659 -0.889660 -0.880465 -1.243004   \n",
       "206 -0.372506 -0.483582 -0.049266 -0.275659 -0.540689  0.059043 -0.577467   \n",
       "264 -0.334678  0.381550 -1.053039 -0.275659  0.804127  1.342843  0.814112   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "195 -0.397995  2.976944 -1.585251 -0.275659 -1.110959  2.318822 -1.307066   \n",
       "42  -0.382918 -0.483582 -0.607255 -0.275659 -0.889660 -0.169999 -2.211058   \n",
       "62  -0.386606  0.597833 -0.877152 -0.275659 -0.847103  0.248695 -0.032936   \n",
       "276 -0.387264  1.246681 -0.683068  3.627671 -0.898172  1.431833 -0.702032   \n",
       "19  -0.313930 -0.483582 -0.420753 -0.275659 -0.123626 -0.814816  0.027567   \n",
       "\n",
       "          DIS       RAD       TAX   PTRATIO         B     LSTAT  \n",
       "163 -0.804421 -0.497260  0.016469 -1.693467  0.332031 -1.327544  \n",
       "202  1.250462 -0.854802 -0.325732 -1.693467  0.415574 -1.358196  \n",
       "45   0.665410 -0.735621 -1.041243 -0.235082  0.433898 -0.321864  \n",
       "206  0.292500 -0.616440 -0.767482  0.083940  0.409425 -0.210932  \n",
       "264 -0.920421 -0.497260 -0.848366 -2.468233  0.325280 -0.629844  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "195  0.939528 -0.616440 -0.904362 -1.830190  0.401710 -1.378631  \n",
       "42   0.975794 -0.735621 -1.041243 -0.235082  0.270790 -0.964098  \n",
       "62   1.728417 -0.139717 -0.723929  0.585259  0.433898 -0.829813  \n",
       "276  0.508743 -0.616440 -0.910584 -0.371806  0.341675 -0.929067  \n",
       "19   0.013180 -0.616440 -0.580827  1.177728  0.362169 -0.165684  \n",
       "\n",
       "[354 rows x 13 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414c39c6-a5cc-4e5b-a3af-bef70c1eebdf",
   "metadata": {
    "id": "414c39c6-a5cc-4e5b-a3af-bef70c1eebdf"
   },
   "source": [
    "## Modelado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bb0778-bd80-459c-9558-f1d3cd603d51",
   "metadata": {
    "id": "63bb0778-bd80-459c-9558-f1d3cd603d51"
   },
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83a691da-4c10-467a-9b09-302db4a7775e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:47:54.640772Z",
     "start_time": "2024-10-05T13:47:54.638207Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:32:04.561893Z",
     "iopub.status.busy": "2023-02-25T15:32:04.561377Z",
     "iopub.status.idle": "2023-02-25T15:32:04.564680Z",
     "shell.execute_reply": "2023-02-25T15:32:04.563936Z",
     "shell.execute_reply.started": "2023-02-25T15:32:04.561874Z"
    },
    "id": "83a691da-4c10-467a-9b09-302db4a7775e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "lasso = Lasso()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b943b85f-ffcc-497c-99d8-c7cf75e337d6",
   "metadata": {
    "id": "b943b85f-ffcc-497c-99d8-c7cf75e337d6"
   },
   "source": [
    "#### Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e766151c-2517-4c9b-87ee-e74eb33e5280",
   "metadata": {
    "id": "e766151c-2517-4c9b-87ee-e74eb33e5280"
   },
   "source": [
    "#### Entrenamiento con datos escalados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58aaa88c-9db0-437f-a526-d09991055116",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:47:58.104171Z",
     "start_time": "2024-10-05T13:47:58.100593Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:35:00.698068Z",
     "iopub.status.busy": "2023-02-25T15:35:00.697857Z",
     "iopub.status.idle": "2023-02-25T15:35:00.701304Z",
     "shell.execute_reply": "2023-02-25T15:35:00.700659Z",
     "shell.execute_reply.started": "2023-02-25T15:35:00.698052Z"
    },
    "id": "58aaa88c-9db0-437f-a526-d09991055116",
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"alpha\": [x for x in range(1, 100)] + [y/10 for y in range(10)],\n",
    "    \"tol\": [0.00001, 0.0000001, 0.01],\n",
    "    \"selection\": ['cyclic', 'random']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab8aa3e6-79f8-494e-94d7-21fcff738ca0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:48:00.676141Z",
     "start_time": "2024-10-05T13:48:00.671660Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:36:14.591045Z",
     "iopub.status.busy": "2023-02-25T15:36:14.590779Z",
     "iopub.status.idle": "2023-02-25T15:36:14.595011Z",
     "shell.execute_reply": "2023-02-25T15:36:14.594411Z",
     "shell.execute_reply.started": "2023-02-25T15:36:14.591029Z"
    },
    "id": "ab8aa3e6-79f8-494e-94d7-21fcff738ca0",
    "outputId": "ffba6e2d-aade-45bc-8ed8-4f5d224fb8b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "654"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod(list(map(len, param_grid.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff8d6282-3cb1-4380-a4a4-f5bf1975152c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:48:00.959529Z",
     "start_time": "2024-10-05T13:48:00.956334Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:36:30.707439Z",
     "iopub.status.busy": "2023-02-25T15:36:30.707255Z",
     "iopub.status.idle": "2023-02-25T15:36:30.710478Z",
     "shell.execute_reply": "2023-02-25T15:36:30.709854Z",
     "shell.execute_reply.started": "2023-02-25T15:36:30.707424Z"
    },
    "id": "ff8d6282-3cb1-4380-a4a4-f5bf1975152c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "search = GridSearchCV(lasso, param_grid, cv=4, error_score=-1000, n_jobs=-1, scoring=\"r2\", verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2308fc9b-151e-452e-b0aa-4ac17199176e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:48:02.740054Z",
     "start_time": "2024-10-05T13:48:01.175519Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:36:31.238376Z",
     "iopub.status.busy": "2023-02-25T15:36:31.237972Z",
     "iopub.status.idle": "2023-02-25T15:36:33.033020Z",
     "shell.execute_reply": "2023-02-25T15:36:33.032426Z",
     "shell.execute_reply.started": "2023-02-25T15:36:31.238350Z"
    },
    "id": "2308fc9b-151e-452e-b0aa-4ac17199176e",
    "outputId": "2bf50a37-4e7e-4f27-e192-c01b27f211c5",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 654 candidates, totalling 2616 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/4] END alpha=1, selection=cyclic, tol=1e-05;, score=-0.303 total time=   0.0s\n",
      "[CV 2/4] END alpha=1, selection=cyclic, tol=0.01;, score=0.520 total time=   0.0s\n",
      "[CV 1/4] END alpha=1, selection=random, tol=1e-07;, score=0.542 total time=   0.0s\n",
      "[CV 2/4] END alpha=1, selection=cyclic, tol=1e-07;, score=0.520 total time=   0.0s\n",
      "[CV 2/4] END alpha=1, selection=random, tol=1e-07;, score=0.520 total time=   0.0s\n",
      "[CV 3/4] END alpha=1, selection=random, tol=1e-07;, score=0.382 total time=   0.0s\n",
      "[CV 4/4] END alpha=1, selection=random, tol=1e-07;, score=-0.303 total time=   0.0s\n",
      "[CV 1/4] END alpha=1, selection=cyclic, tol=1e-05;, score=0.542 total time=   0.0s\n",
      "[CV 1/4] END alpha=1, selection=random, tol=0.01;, score=0.543 total time=   0.0s\n",
      "[CV 3/4] END alpha=1, selection=random, tol=0.01;, score=0.380 total time=   0.0s\n",
      "[CV 2/4] END alpha=1, selection=random, tol=0.01;, score=0.519 total time=   0.0s\n",
      "[CV 4/4] END alpha=1, selection=random, tol=0.01;, score=-0.304 total time=   0.0s\n",
      "[CV 1/4] END alpha=2, selection=cyclic, tol=1e-05;, score=0.404 total time=   0.0s\n",
      "[CV 3/4] END alpha=1, selection=cyclic, tol=1e-05;, score=0.382 total time=   0.0s\n",
      "[CV 2/4] END alpha=2, selection=cyclic, tol=1e-05;, score=0.473 total time=   0.0s\n",
      "[CV 4/4] END alpha=2, selection=cyclic, tol=1e-05;, score=0.024 total time=   0.0s\n",
      "[CV 3/4] END alpha=2, selection=cyclic, tol=1e-05;, score=0.279 total time=   0.0s\n",
      "[CV 2/4] END alpha=2, selection=cyclic, tol=1e-07;, score=0.473 total time=   0.0s\n",
      "[CV 1/4] END alpha=2, selection=cyclic, tol=1e-07;, score=0.404 total time=   0.0s\n",
      "[CV 4/4] END alpha=2, selection=cyclic, tol=1e-07;, score=0.024 total time=   0.0s\n",
      "[CV 1/4] END alpha=2, selection=cyclic, tol=0.01;, score=0.401 total time=   0.0s\n",
      "[CV 3/4] END alpha=2, selection=cyclic, tol=1e-07;, score=0.279 total time=   0.0s\n",
      "[CV 2/4] END alpha=2, selection=cyclic, tol=0.01;, score=0.472 total time=   0.0s\n",
      "[CV 3/4] END alpha=2, selection=cyclic, tol=0.01;, score=0.281 total time=   0.0s\n",
      "[CV 1/4] END alpha=1, selection=cyclic, tol=1e-07;, score=0.542 total time=   0.0s\n",
      "[CV 2/4] END alpha=2, selection=random, tol=1e-05;, score=0.473 total time=   0.0s[CV 3/4] END alpha=2, selection=random, tol=1e-05;, score=0.279 total time=   0.0s\n",
      "\n",
      "[CV 1/4] END alpha=2, selection=random, tol=1e-05;, score=0.404 total time=   0.0s\n",
      "[CV 4/4] END alpha=2, selection=cyclic, tol=0.01;, score=0.026 total time=   0.0s\n",
      "[CV 4/4] END alpha=1, selection=cyclic, tol=0.01;, score=-0.299 total time=   0.0s\n",
      "[CV 1/4] END alpha=2, selection=random, tol=1e-07;, score=0.404 total time=   0.0s\n",
      "[CV 3/4] END alpha=2, selection=random, tol=1e-07;, score=0.279 total time=   0.0s\n",
      "[CV 4/4] END alpha=2, selection=random, tol=1e-05;, score=0.024 total time=   0.0s\n",
      "[CV 1/4] END alpha=2, selection=random, tol=0.01;, score=0.405 total time=   0.0s\n",
      "[CV 2/4] END alpha=2, selection=random, tol=1e-07;, score=0.473 total time=   0.0s\n",
      "[CV 4/4] END alpha=2, selection=random, tol=1e-07;, score=0.024 total time=   0.0s\n",
      "[CV 4/4] END alpha=1, selection=cyclic, tol=1e-07;, score=-0.303 total time=   0.0s\n",
      "[CV 1/4] END alpha=1, selection=cyclic, tol=0.01;, score=0.542 total time=   0.0s\n",
      "[CV 2/4] END alpha=2, selection=random, tol=0.01;, score=0.473 total time=   0.0s[CV 4/4] END alpha=1, selection=random, tol=1e-05;, score=-0.303 total time=   0.0s\n",
      "[CV 3/4] END alpha=2, selection=random, tol=0.01;, score=0.279 total time=   0.0s\n",
      "\n",
      "[CV 1/4] END alpha=3, selection=cyclic, tol=1e-05;, score=0.282 total time=   0.0s\n",
      "[CV 4/4] END alpha=2, selection=random, tol=0.01;, score=0.031 total time=   0.0s\n",
      "[CV 2/4] END alpha=3, selection=cyclic, tol=1e-05;, score=0.428 total time=   0.0s\n",
      "[CV 4/4] END alpha=3, selection=cyclic, tol=1e-05;, score=0.106 total time=   0.0s\n",
      "[CV 3/4] END alpha=1, selection=cyclic, tol=0.01;, score=0.379 total time=   0.0s\n",
      "[CV 1/4] END alpha=3, selection=cyclic, tol=1e-07;, score=0.282 total time=   0.0s\n",
      "[CV 3/4] END alpha=3, selection=cyclic, tol=1e-05;, score=0.225 total time=   0.0s\n",
      "[CV 2/4] END alpha=1, selection=cyclic, tol=1e-05;, score=0.520 total time=   0.0s\n",
      "[CV 3/4] END alpha=3, selection=cyclic, tol=1e-07;, score=0.225 total time=   0.0s\n",
      "[CV 4/4] END alpha=3, selection=cyclic, tol=1e-07;, score=0.106 total time=   0.0s\n",
      "[CV 2/4] END alpha=3, selection=cyclic, tol=1e-07;, score=0.428 total time=   0.0s\n",
      "[CV 1/4] END alpha=3, selection=cyclic, tol=0.01;, score=0.280 total time=   0.0s\n",
      "[CV 4/4] END alpha=3, selection=cyclic, tol=0.01;, score=0.110 total time=   0.0s\n",
      "[CV 3/4] END alpha=3, selection=cyclic, tol=0.01;, score=0.225 total time=   0.0s[CV 1/4] END alpha=3, selection=random, tol=1e-05;, score=0.282 total time=   0.0s\n",
      "\n",
      "[CV 2/4] END alpha=1, selection=random, tol=1e-05;, score=0.520 total time=   0.0s\n",
      "[CV 2/4] END alpha=3, selection=random, tol=1e-05;, score=0.428 total time=   0.0s\n",
      "[CV 3/4] END alpha=3, selection=random, tol=1e-05;, score=0.225 total time=   0.0s\n",
      "[CV 2/4] END alpha=3, selection=cyclic, tol=0.01;, score=0.428 total time=   0.0s\n",
      "[CV 4/4] END alpha=3, selection=random, tol=1e-05;, score=0.106 total time=   0.0s\n",
      "[CV 1/4] END alpha=3, selection=random, tol=1e-07;, score=0.282 total time=   0.0s\n",
      "[CV 2/4] END alpha=3, selection=random, tol=1e-07;, score=0.428 total time=   0.0s\n",
      "[CV 3/4] END alpha=3, selection=random, tol=1e-07;, score=0.225 total time=   0.0s\n",
      "[CV 4/4] END alpha=3, selection=random, tol=1e-07;, score=0.106 total time=   0.0s\n",
      "[CV 2/4] END alpha=3, selection=random, tol=0.01;, score=0.429 total time=   0.0s\n",
      "[CV 1/4] END alpha=3, selection=random, tol=0.01;, score=0.279 total time=   0.0s\n",
      "[CV 3/4] END alpha=3, selection=random, tol=0.01;, score=0.226 total time=   0.0s\n",
      "[CV 4/4] END alpha=3, selection=random, tol=0.01;, score=0.116 total time=   0.0s\n",
      "[CV 3/4] END alpha=1, selection=random, tol=1e-05;, score=0.382 total time=   0.0s[CV 1/4] END alpha=4, selection=cyclic, tol=1e-05;, score=0.192 total time=   0.0s\n",
      "\n",
      "[CV 2/4] END alpha=4, selection=cyclic, tol=1e-05;, score=0.394 total time=   0.0s\n",
      "[CV 3/4] END alpha=1, selection=cyclic, tol=1e-07;, score=0.382 total time=   0.0s\n",
      "[CV 3/4] END alpha=4, selection=cyclic, tol=1e-05;, score=0.219 total time=   0.0s\n",
      "[CV 4/4] END alpha=4, selection=cyclic, tol=1e-05;, score=0.163 total time=   0.0s\n",
      "[CV 1/4] END alpha=4, selection=cyclic, tol=1e-07;, score=0.192 total time=   0.0s\n",
      "[CV 3/4] END alpha=4, selection=cyclic, tol=1e-07;, score=0.219 total time=   0.0s\n",
      "[CV 4/4] END alpha=4, selection=cyclic, tol=1e-07;, score=0.163 total time=   0.0s\n",
      "[CV 2/4] END alpha=4, selection=cyclic, tol=1e-07;, score=0.394 total time=   0.0s\n",
      "[CV 1/4] END alpha=4, selection=cyclic, tol=0.01;, score=0.193 total time=   0.0s\n",
      "[CV 2/4] END alpha=4, selection=cyclic, tol=0.01;, score=0.394 total time=   0.0s\n",
      "[CV 4/4] END alpha=4, selection=cyclic, tol=0.01;, score=0.162 total time=   0.0s\n",
      "[CV 3/4] END alpha=4, selection=cyclic, tol=0.01;, score=0.219 total time=   0.0s\n",
      "[CV 1/4] END alpha=4, selection=random, tol=1e-05;, score=0.192 total time=   0.0s\n",
      "[CV 3/4] END alpha=4, selection=random, tol=1e-05;, score=0.219 total time=   0.0s\n",
      "[CV 2/4] END alpha=4, selection=random, tol=1e-05;, score=0.394 total time=   0.0s\n",
      "[CV 1/4] END alpha=4, selection=random, tol=1e-07;, score=0.192 total time=   0.0s\n",
      "[CV 4/4] END alpha=4, selection=random, tol=1e-05;, score=0.162 total time=   0.0s\n",
      "[CV 1/4] END alpha=1, selection=random, tol=1e-05;, score=0.542 total time=   0.0s\n",
      "[CV 3/4] END alpha=4, selection=random, tol=1e-07;, score=0.219 total time=   0.0s\n",
      "[CV 4/4] END alpha=4, selection=random, tol=1e-07;, score=0.163 total time=   0.0s\n",
      "[CV 2/4] END alpha=4, selection=random, tol=1e-07;, score=0.394 total time=   0.0s\n",
      "[CV 1/4] END alpha=4, selection=random, tol=0.01;, score=0.192 total time=   0.0s\n",
      "[CV 2/4] END alpha=4, selection=random, tol=0.01;, score=0.394 total time=   0.0s\n",
      "[CV 1/4] END alpha=5, selection=cyclic, tol=1e-05;, score=0.182 total time=   0.0s\n",
      "[CV 3/4] END alpha=4, selection=random, tol=0.01;, score=0.219 total time=   0.0s\n",
      "[CV 4/4] END alpha=4, selection=random, tol=0.01;, score=0.163 total time=   0.0s\n",
      "[CV 3/4] END alpha=5, selection=cyclic, tol=1e-05;, score=0.212 total time=   0.0s\n",
      "[CV 1/4] END alpha=5, selection=cyclic, tol=1e-07;, score=0.182 total time=   0.0s\n",
      "[CV 2/4] END alpha=5, selection=cyclic, tol=1e-05;, score=0.381 total time=   0.0s\n",
      "[CV 4/4] END alpha=5, selection=cyclic, tol=1e-05;, score=0.194 total time=   0.0s\n",
      "[CV 3/4] END alpha=5, selection=cyclic, tol=1e-07;, score=0.212 total time=   0.0s\n",
      "[CV 2/4] END alpha=5, selection=cyclic, tol=1e-07;, score=0.381 total time=   0.0s\n",
      "[CV 1/4] END alpha=5, selection=cyclic, tol=0.01;, score=0.183 total time=   0.0s\n",
      "[CV 3/4] END alpha=5, selection=cyclic, tol=0.01;, score=0.212 total time=   0.0s\n",
      "[CV 3/4] END alpha=5, selection=random, tol=1e-05;, score=0.212 total time=   0.0s\n",
      "[CV 1/4] END alpha=5, selection=random, tol=1e-05;, score=0.182 total time=   0.0s\n",
      "[CV 4/4] END alpha=5, selection=cyclic, tol=1e-07;, score=0.194 total time=   0.0s\n",
      "[CV 3/4] END alpha=5, selection=random, tol=1e-07;, score=0.212 total time=   0.0s\n",
      "[CV 2/4] END alpha=5, selection=cyclic, tol=0.01;, score=0.381 total time=   0.0s\n",
      "[CV 4/4] END alpha=5, selection=cyclic, tol=0.01;, score=0.194 total time=   0.0s\n",
      "[CV 1/4] END alpha=5, selection=random, tol=1e-07;, score=0.182 total time=   0.0s\n",
      "[CV 4/4] END alpha=5, selection=random, tol=1e-05;, score=0.194 total time=   0.0s\n",
      "[CV 4/4] END alpha=5, selection=random, tol=1e-07;, score=0.194 total time=   0.0s\n",
      "[CV 2/4] END alpha=5, selection=random, tol=1e-05;, score=0.381 total time=   0.0s\n",
      "[CV 2/4] END alpha=5, selection=random, tol=1e-07;, score=0.381 total time=   0.0s\n",
      "[CV 1/4] END alpha=5, selection=random, tol=0.01;, score=0.180 total time=   0.0s\n",
      "[CV 3/4] END alpha=5, selection=random, tol=0.01;, score=0.213 total time=   0.0s\n",
      "[CV 2/4] END alpha=5, selection=random, tol=0.01;, score=0.380 total time=   0.0s\n",
      "[CV 1/4] END alpha=6, selection=cyclic, tol=1e-05;, score=0.173 total time=   0.0s\n",
      "[CV 1/4] END alpha=6, selection=cyclic, tol=1e-07;, score=0.173 total time=   0.0s\n",
      "[CV 3/4] END alpha=6, selection=cyclic, tol=1e-05;, score=0.204 total time=   0.0s\n",
      "[CV 4/4] END alpha=5, selection=random, tol=0.01;, score=0.195 total time=   0.0s\n",
      "[CV 1/4] END alpha=6, selection=cyclic, tol=0.01;, score=0.174 total time=   0.0s\n",
      "[CV 3/4] END alpha=6, selection=cyclic, tol=1e-07;, score=0.204 total time=   0.0s\n",
      "[CV 2/4] END alpha=6, selection=cyclic, tol=1e-05;, score=0.365 total time=   0.0s\n",
      "[CV 3/4] END alpha=6, selection=cyclic, tol=0.01;, score=0.205 total time=   0.0s\n",
      "[CV 2/4] END alpha=6, selection=cyclic, tol=1e-07;, score=0.365 total time=   0.0s\n",
      "[CV 4/4] END alpha=6, selection=cyclic, tol=1e-05;, score=0.199 total time=   0.0s\n",
      "[CV 4/4] END alpha=6, selection=cyclic, tol=1e-07;, score=0.199 total time=   0.0s\n",
      "[CV 3/4] END alpha=6, selection=random, tol=1e-05;, score=0.204 total time=   0.0s\n",
      "[CV 2/4] END alpha=6, selection=cyclic, tol=0.01;, score=0.366 total time=   0.0s\n",
      "[CV 1/4] END alpha=6, selection=random, tol=1e-05;, score=0.173 total time=   0.0s\n",
      "[CV 4/4] END alpha=6, selection=cyclic, tol=0.01;, score=0.198 total time=   0.0s\n",
      "[CV 1/4] END alpha=6, selection=random, tol=1e-07;, score=0.173 total time=   0.0s\n",
      "[CV 3/4] END alpha=6, selection=random, tol=1e-07;, score=0.204 total time=   0.0s\n",
      "[CV 1/4] END alpha=6, selection=random, tol=0.01;, score=0.172 total time=   0.0s\n",
      "[CV 4/4] END alpha=6, selection=random, tol=1e-05;, score=0.199 total time=   0.0s\n",
      "[CV 3/4] END alpha=6, selection=random, tol=0.01;, score=0.205 total time=   0.0s\n",
      "[CV 2/4] END alpha=6, selection=random, tol=1e-05;, score=0.365 total time=   0.0s\n",
      "[CV 2/4] END alpha=6, selection=random, tol=1e-07;, score=0.365 total time=   0.0s\n",
      "[CV 1/4] END alpha=7, selection=cyclic, tol=1e-05;, score=0.161 total time=   0.0s\n",
      "[CV 4/4] END alpha=6, selection=random, tol=1e-07;, score=0.199 total time=   0.0s\n",
      "[CV 2/4] END alpha=6, selection=random, tol=0.01;, score=0.361 total time=   0.0s\n",
      "[CV 4/4] END alpha=6, selection=random, tol=0.01;, score=0.204 total time=   0.0s\n",
      "[CV 3/4] END alpha=7, selection=cyclic, tol=1e-05;, score=0.196 total time=   0.0s\n",
      "[CV 2/4] END alpha=7, selection=cyclic, tol=1e-05;, score=0.349 total time=   0.0s\n",
      "[CV 1/4] END alpha=7, selection=cyclic, tol=1e-07;, score=0.161 total time=   0.0s\n",
      "[CV 4/4] END alpha=7, selection=cyclic, tol=1e-05;, score=0.196 total time=   0.0s\n",
      "[CV 3/4] END alpha=7, selection=cyclic, tol=1e-07;, score=0.196 total time=   0.0s\n",
      "[CV 1/4] END alpha=7, selection=random, tol=1e-05;, score=0.161 total time=   0.0s\n",
      "[CV 1/4] END alpha=7, selection=cyclic, tol=0.01;, score=0.159 total time=   0.0s\n",
      "[CV 2/4] END alpha=7, selection=cyclic, tol=1e-07;, score=0.349 total time=   0.0s\n",
      "[CV 3/4] END alpha=7, selection=cyclic, tol=0.01;, score=0.197 total time=   0.0s\n",
      "[CV 3/4] END alpha=7, selection=random, tol=1e-05;, score=0.196 total time=   0.0s\n",
      "[CV 4/4] END alpha=7, selection=cyclic, tol=1e-07;, score=0.196 total time=   0.0s\n",
      "[CV 1/4] END alpha=7, selection=random, tol=1e-07;, score=0.161 total time=   0.0s\n",
      "[CV 2/4] END alpha=7, selection=cyclic, tol=0.01;, score=0.350 total time=   0.0s\n",
      "[CV 2/4] END alpha=7, selection=random, tol=1e-05;, score=0.349 total time=   0.0s\n",
      "[CV 4/4] END alpha=7, selection=cyclic, tol=0.01;, score=0.195 total time=   0.0s\n",
      "[CV 1/4] END alpha=7, selection=random, tol=0.01;, score=0.163 total time=   0.0s\n",
      "[CV 4/4] END alpha=7, selection=random, tol=1e-05;, score=0.196 total time=   0.0s\n",
      "[CV 1/4] END alpha=8, selection=cyclic, tol=1e-05;, score=0.146 total time=   0.0s\n",
      "[CV 2/4] END alpha=7, selection=random, tol=1e-07;, score=0.349 total time=   0.0s\n",
      "[CV 2/4] END alpha=7, selection=random, tol=0.01;, score=0.348 total time=   0.0s\n",
      "[CV 1/4] END alpha=8, selection=cyclic, tol=1e-07;, score=0.146 total time=   0.0s\n",
      "[CV 2/4] END alpha=8, selection=cyclic, tol=1e-05;, score=0.330 total time=   0.0s\n",
      "[CV 1/4] END alpha=8, selection=cyclic, tol=0.01;, score=0.148 total time=   0.0s\n",
      "[CV 1/4] END alpha=8, selection=random, tol=1e-05;, score=0.146 total time=   0.0s\n",
      "[CV 1/4] END alpha=8, selection=random, tol=1e-07;, score=0.146 total time=   0.0s\n",
      "[CV 3/4] END alpha=7, selection=random, tol=1e-07;, score=0.196 total time=   0.0s\n",
      "[CV 3/4] END alpha=7, selection=random, tol=0.01;, score=0.190 total time=   0.0s\n",
      "[CV 2/4] END alpha=8, selection=cyclic, tol=1e-07;, score=0.330 total time=   0.0s\n",
      "[CV 3/4] END alpha=8, selection=cyclic, tol=1e-05;, score=0.188 total time=   0.0s\n",
      "[CV 2/4] END alpha=8, selection=cyclic, tol=0.01;, score=0.333 total time=   0.0s\n",
      "[CV 1/4] END alpha=8, selection=random, tol=0.01;, score=0.145 total time=   0.0s\n",
      "[CV 1/4] END alpha=9, selection=cyclic, tol=1e-05;, score=0.129 total time=   0.0s\n",
      "[CV 2/4] END alpha=8, selection=random, tol=1e-05;, score=0.330 total time=   0.0s\n",
      "[CV 1/4] END alpha=9, selection=cyclic, tol=1e-07;, score=0.129 total time=   0.0s\n",
      "[CV 2/4] END alpha=8, selection=random, tol=1e-07;, score=0.330 total time=   0.0s\n",
      "[CV 4/4] END alpha=7, selection=random, tol=1e-07;, score=0.196 total time=   0.0s\n",
      "[CV 4/4] END alpha=7, selection=random, tol=0.01;, score=0.196 total time=   0.0s\n",
      "[CV 3/4] END alpha=8, selection=cyclic, tol=0.01;, score=0.188 total time=   0.0s\n",
      "[CV 4/4] END alpha=8, selection=cyclic, tol=1e-05;, score=0.185 total time=   0.0s\n",
      "[CV 3/4] END alpha=8, selection=cyclic, tol=1e-07;, score=0.188 total time=   0.0s\n",
      "[CV 2/4] END alpha=8, selection=random, tol=0.01;, score=0.330 total time=   0.0s\n",
      "[CV 2/4] END alpha=9, selection=cyclic, tol=1e-05;, score=0.316 total time=   0.0s\n",
      "[CV 1/4] END alpha=9, selection=cyclic, tol=0.01;, score=0.131 total time=   0.0s\n",
      "[CV 3/4] END alpha=8, selection=random, tol=1e-05;, score=0.188 total time=   0.0s\n",
      "[CV 2/4] END alpha=9, selection=cyclic, tol=1e-07;, score=0.316 total time=   0.0s\n",
      "[CV 4/4] END alpha=8, selection=cyclic, tol=0.01;, score=0.184 total time=   0.0s\n",
      "[CV 3/4] END alpha=8, selection=random, tol=0.01;, score=0.187 total time=   0.0s\n",
      "[CV 3/4] END alpha=8, selection=random, tol=1e-07;, score=0.188 total time=   0.0s\n",
      "[CV 4/4] END alpha=8, selection=cyclic, tol=1e-07;, score=0.185 total time=   0.0s\n",
      "[CV 2/4] END alpha=9, selection=cyclic, tol=0.01;, score=0.316 total time=   0.0s\n",
      "[CV 3/4] END alpha=9, selection=cyclic, tol=1e-05;, score=0.175 total time=   0.0s\n",
      "[CV 4/4] END alpha=8, selection=random, tol=1e-05;, score=0.185 total time=   0.0s\n",
      "[CV 4/4] END alpha=8, selection=random, tol=0.01;, score=0.161 total time=   0.0s\n",
      "[CV 3/4] END alpha=9, selection=cyclic, tol=1e-07;, score=0.175 total time=   0.0s\n",
      "[CV 4/4] END alpha=8, selection=random, tol=1e-07;, score=0.185 total time=   0.0s\n",
      "[CV 3/4] END alpha=9, selection=cyclic, tol=0.01;, score=0.175 total time=   0.0s\n",
      "[CV 4/4] END alpha=9, selection=cyclic, tol=1e-05;, score=0.165 total time=   0.0s\n",
      "[CV 4/4] END alpha=9, selection=cyclic, tol=1e-07;, score=0.165 total time=   0.0s\n",
      "[CV 4/4] END alpha=9, selection=cyclic, tol=0.01;, score=0.164 total time=   0.0s\n",
      "[CV 1/4] END alpha=9, selection=random, tol=1e-05;, score=0.129 total time=   0.0s\n",
      "[CV 2/4] END alpha=9, selection=random, tol=1e-05;, score=0.316 total time=   0.0s\n",
      "[CV 1/4] END alpha=9, selection=random, tol=1e-07;, score=0.129 total time=   0.0s\n",
      "[CV 3/4] END alpha=9, selection=random, tol=1e-05;, score=0.175 total time=   0.0s\n",
      "[CV 2/4] END alpha=9, selection=random, tol=1e-07;, score=0.316 total time=   0.0s\n",
      "[CV 1/4] END alpha=9, selection=random, tol=0.01;, score=0.130 total time=   0.0s\n",
      "[CV 4/4] END alpha=9, selection=random, tol=1e-05;, score=0.165 total time=   0.0s\n",
      "[CV 1/4] END alpha=10, selection=cyclic, tol=1e-05;, score=0.110 total time=   0.0s\n",
      "[CV 1/4] END alpha=10, selection=cyclic, tol=1e-07;, score=0.110 total time=   0.0s\n",
      "[CV 1/4] END alpha=10, selection=random, tol=1e-05;, score=0.110 total time=   0.0s\n",
      "[CV 3/4] END alpha=9, selection=random, tol=1e-07;, score=0.175 total time=   0.0s\n",
      "[CV 1/4] END alpha=10, selection=cyclic, tol=0.01;, score=0.104 total time=   0.0s\n",
      "[CV 2/4] END alpha=9, selection=random, tol=0.01;, score=0.316 total time=   0.0s\n",
      "[CV 1/4] END alpha=10, selection=random, tol=0.01;, score=0.109 total time=   0.0s\n",
      "[CV 2/4] END alpha=10, selection=cyclic, tol=1e-05;, score=0.302 total time=   0.0s\n",
      "[CV 1/4] END alpha=10, selection=random, tol=1e-07;, score=0.110 total time=   0.0s\n",
      "[CV 1/4] END alpha=11, selection=cyclic, tol=1e-05;, score=0.089 total time=   0.0s\n",
      "[CV 1/4] END alpha=11, selection=cyclic, tol=1e-07;, score=0.089 total time=   0.0s\n",
      "[CV 4/4] END alpha=9, selection=random, tol=1e-07;, score=0.165 total time=   0.0s[CV 2/4] END alpha=10, selection=cyclic, tol=1e-07;, score=0.302 total time=   0.0s\n",
      "\n",
      "[CV 2/4] END alpha=10, selection=random, tol=1e-05;, score=0.302 total time=   0.0s\n",
      "[CV 2/4] END alpha=10, selection=cyclic, tol=0.01;, score=0.302 total time=   0.0s\n",
      "[CV 1/4] END alpha=11, selection=cyclic, tol=0.01;, score=0.087 total time=   0.0s\n",
      "[CV 3/4] END alpha=9, selection=random, tol=0.01;, score=0.172 total time=   0.0s\n",
      "[CV 2/4] END alpha=10, selection=random, tol=0.01;, score=0.304 total time=   0.0s\n",
      "[CV 2/4] END alpha=10, selection=random, tol=1e-07;, score=0.302 total time=   0.0s\n",
      "[CV 2/4] END alpha=11, selection=cyclic, tol=1e-05;, score=0.288 total time=   0.0s\n",
      "[CV 3/4] END alpha=10, selection=cyclic, tol=1e-05;, score=0.160 total time=   0.0s\n",
      "[CV 2/4] END alpha=11, selection=cyclic, tol=1e-07;, score=0.288 total time=   0.0s\n",
      "[CV 3/4] END alpha=10, selection=random, tol=1e-05;, score=0.160 total time=   0.0s\n",
      "[CV 3/4] END alpha=10, selection=cyclic, tol=0.01;, score=0.160 total time=   0.0s\n",
      "[CV 2/4] END alpha=11, selection=cyclic, tol=0.01;, score=0.288 total time=   0.0s\n",
      "[CV 3/4] END alpha=10, selection=cyclic, tol=1e-07;, score=0.160 total time=   0.0s\n",
      "[CV 3/4] END alpha=10, selection=random, tol=0.01;, score=0.161 total time=   0.0s\n",
      "[CV 4/4] END alpha=9, selection=random, tol=0.01;, score=0.156 total time=   0.0s\n",
      "[CV 3/4] END alpha=10, selection=random, tol=1e-07;, score=0.160 total time=   0.0s\n",
      "[CV 3/4] END alpha=11, selection=cyclic, tol=1e-07;, score=0.144 total time=   0.0s\n",
      "[CV 3/4] END alpha=11, selection=cyclic, tol=1e-05;, score=0.144 total time=   0.0s\n",
      "[CV 4/4] END alpha=10, selection=random, tol=1e-05;, score=0.138 total time=   0.0s\n",
      "[CV 4/4] END alpha=10, selection=cyclic, tol=1e-05;, score=0.138 total time=   0.0s\n",
      "[CV 4/4] END alpha=10, selection=cyclic, tol=0.01;, score=0.137 total time=   0.0s\n",
      "[CV 4/4] END alpha=10, selection=cyclic, tol=1e-07;, score=0.138 total time=   0.0s\n",
      "[CV 3/4] END alpha=11, selection=cyclic, tol=0.01;, score=0.145 total time=   0.0s\n",
      "[CV 4/4] END alpha=10, selection=random, tol=1e-07;, score=0.138 total time=   0.0s\n",
      "[CV 4/4] END alpha=10, selection=random, tol=0.01;, score=0.138 total time=   0.0s\n",
      "[CV 1/4] END alpha=11, selection=random, tol=1e-05;, score=0.089 total time=   0.0s\n",
      "[CV 4/4] END alpha=11, selection=cyclic, tol=1e-05;, score=0.103 total time=   0.0s\n",
      "[CV 4/4] END alpha=11, selection=cyclic, tol=1e-07;, score=0.103 total time=   0.0s\n",
      "[CV 1/4] END alpha=11, selection=random, tol=1e-07;, score=0.089 total time=   0.0s\n",
      "[CV 4/4] END alpha=11, selection=cyclic, tol=0.01;, score=0.102 total time=   0.0s\n",
      "[CV 1/4] END alpha=11, selection=random, tol=0.01;, score=0.090 total time=   0.0s\n",
      "[CV 2/4] END alpha=11, selection=random, tol=1e-05;, score=0.288 total time=   0.0s\n",
      "[CV 2/4] END alpha=11, selection=random, tol=1e-07;, score=0.288 total time=   0.0s\n",
      "[CV 2/4] END alpha=11, selection=random, tol=0.01;, score=0.287 total time=   0.0s\n",
      "[CV 3/4] END alpha=11, selection=random, tol=1e-05;, score=0.144 total time=   0.0s\n",
      "[CV 3/4] END alpha=11, selection=random, tol=1e-07;, score=0.144 total time=   0.0s\n",
      "[CV 1/4] END alpha=12, selection=cyclic, tol=1e-05;, score=0.065 total time=   0.0s\n",
      "[CV 3/4] END alpha=11, selection=random, tol=0.01;, score=0.145 total time=   0.0s\n",
      "[CV 4/4] END alpha=11, selection=random, tol=1e-05;, score=0.103 total time=   0.0s\n",
      "[CV 4/4] END alpha=11, selection=random, tol=1e-07;, score=0.103 total time=   0.0s\n",
      "[CV 2/4] END alpha=12, selection=cyclic, tol=1e-05;, score=0.273 total time=   0.0s\n",
      "[CV 4/4] END alpha=11, selection=random, tol=0.01;, score=0.102 total time=   0.0s\n",
      "[CV 1/4] END alpha=12, selection=cyclic, tol=1e-07;, score=0.065 total time=   0.0s\n",
      "[CV 1/4] END alpha=12, selection=cyclic, tol=0.01;, score=0.067 total time=   0.0s\n",
      "[CV 1/4] END alpha=12, selection=random, tol=1e-05;, score=0.065 total time=   0.0s\n",
      "[CV 3/4] END alpha=12, selection=cyclic, tol=1e-05;, score=0.128 total time=   0.0s\n",
      "[CV 1/4] END alpha=12, selection=random, tol=0.01;, score=0.066 total time=   0.0s\n",
      "[CV 1/4] END alpha=12, selection=random, tol=1e-07;, score=0.065 total time=   0.0s\n",
      "[CV 2/4] END alpha=12, selection=cyclic, tol=1e-07;, score=0.273 total time=   0.0s\n",
      "[CV 1/4] END alpha=13, selection=cyclic, tol=1e-05;, score=0.043 total time=   0.0s\n",
      "[CV 1/4] END alpha=13, selection=cyclic, tol=0.01;, score=0.040 total time=   0.0s\n",
      "[CV 2/4] END alpha=12, selection=cyclic, tol=0.01;, score=0.273 total time=   0.0s\n",
      "[CV 2/4] END alpha=12, selection=random, tol=0.01;, score=0.233 total time=   0.0s\n",
      "[CV 4/4] END alpha=12, selection=cyclic, tol=1e-05;, score=0.059 total time=   0.0s\n",
      "[CV 2/4] END alpha=12, selection=random, tol=1e-05;, score=0.273 total time=   0.0s\n",
      "[CV 2/4] END alpha=12, selection=random, tol=1e-07;, score=0.273 total time=   0.0s\n",
      "[CV 3/4] END alpha=12, selection=cyclic, tol=1e-07;, score=0.128 total time=   0.0s\n",
      "[CV 2/4] END alpha=13, selection=cyclic, tol=1e-05;, score=0.257 total time=   0.0s\n",
      "[CV 1/4] END alpha=14, selection=cyclic, tol=1e-05;, score=0.019 total time=   0.0s\n",
      "[CV 3/4] END alpha=12, selection=random, tol=0.01;, score=0.123 total time=   0.0s\n",
      "[CV 3/4] END alpha=12, selection=cyclic, tol=0.01;, score=0.129 total time=   0.0s\n",
      "[CV 2/4] END alpha=13, selection=cyclic, tol=0.01;, score=0.257 total time=   0.0s\n",
      "[CV 3/4] END alpha=12, selection=random, tol=1e-05;, score=0.128 total time=   0.0s\n",
      "[CV 3/4] END alpha=12, selection=random, tol=1e-07;, score=0.128 total time=   0.0s\n",
      "[CV 1/4] END alpha=14, selection=random, tol=1e-07;, score=0.019 total time=   0.0s\n",
      "[CV 1/4] END alpha=13, selection=random, tol=1e-07;, score=0.043 total time=   0.0s\n",
      "[CV 1/4] END alpha=14, selection=cyclic, tol=0.01;, score=0.015 total time=   0.0s\n",
      "[CV 4/4] END alpha=12, selection=cyclic, tol=1e-07;, score=0.059 total time=   0.0s\n",
      "[CV 3/4] END alpha=13, selection=cyclic, tol=1e-05;, score=0.111 total time=   0.0s\n",
      "[CV 4/4] END alpha=12, selection=cyclic, tol=0.01;, score=0.058 total time=   0.0s\n",
      "[CV 4/4] END alpha=12, selection=random, tol=0.01;, score=0.055 total time=   0.0s\n",
      "[CV 4/4] END alpha=12, selection=random, tol=1e-05;, score=0.059 total time=   0.0s\n",
      "[CV 3/4] END alpha=13, selection=cyclic, tol=0.01;, score=0.112 total time=   0.0s\n",
      "[CV 2/4] END alpha=14, selection=cyclic, tol=1e-05;, score=0.241 total time=   0.0s\n",
      "[CV 1/4] END alpha=15, selection=cyclic, tol=0.01;, score=-0.011 total time=   0.0s\n",
      "[CV 4/4] END alpha=12, selection=random, tol=1e-07;, score=0.059 total time=   0.0s\n",
      "[CV 1/4] END alpha=15, selection=cyclic, tol=1e-05;, score=-0.007 total time=   0.0s\n",
      "[CV 1/4] END alpha=15, selection=random, tol=1e-07;, score=-0.007 total time=   0.0s\n",
      "[CV 2/4] END alpha=13, selection=random, tol=1e-07;, score=0.257 total time=   0.0s\n",
      "[CV 2/4] END alpha=14, selection=random, tol=1e-07;, score=0.241 total time=   0.0s\n",
      "[CV 4/4] END alpha=13, selection=cyclic, tol=1e-05;, score=0.011 total time=   0.0s\n",
      "[CV 3/4] END alpha=14, selection=cyclic, tol=1e-05;, score=0.094 total time=   0.0s\n",
      "[CV 4/4] END alpha=13, selection=cyclic, tol=0.01;, score=0.004 total time=   0.0s\n",
      "[CV 2/4] END alpha=15, selection=cyclic, tol=1e-05;, score=0.223 total time=   0.0s\n",
      "[CV 3/4] END alpha=14, selection=random, tol=1e-07;, score=0.094 total time=   0.0s\n",
      "[CV 3/4] END alpha=13, selection=random, tol=1e-07;, score=0.111 total time=   0.0s\n",
      "[CV 2/4] END alpha=14, selection=cyclic, tol=0.01;, score=0.240 total time=   0.0s\n",
      "[CV 4/4] END alpha=14, selection=cyclic, tol=1e-05;, score=-0.026 total time=   0.0s\n",
      "[CV 2/4] END alpha=15, selection=cyclic, tol=0.01;, score=0.223 total time=   0.0s\n",
      "[CV 2/4] END alpha=15, selection=random, tol=1e-07;, score=0.223 total time=   0.0s\n",
      "[CV 1/4] END alpha=13, selection=cyclic, tol=1e-07;, score=0.043 total time=   0.0s\n",
      "[CV 3/4] END alpha=15, selection=cyclic, tol=1e-05;, score=0.076 total time=   0.0s\n",
      "[CV 1/4] END alpha=13, selection=random, tol=1e-05;, score=0.043 total time=   0.0s\n",
      "[CV 1/4] END alpha=16, selection=cyclic, tol=1e-05;, score=-0.035 total time=   0.0s\n",
      "[CV 4/4] END alpha=14, selection=random, tol=1e-07;, score=-0.026 total time=   0.0s\n",
      "[CV 1/4] END alpha=14, selection=cyclic, tol=1e-07;, score=0.019 total time=   0.0s\n",
      "[CV 3/4] END alpha=15, selection=cyclic, tol=0.01;, score=0.076 total time=   0.0s\n",
      "[CV 4/4] END alpha=13, selection=random, tol=1e-07;, score=0.011 total time=   0.0s\n",
      "[CV 3/4] END alpha=14, selection=cyclic, tol=0.01;, score=0.094 total time=   0.0s\n",
      "[CV 2/4] END alpha=13, selection=cyclic, tol=1e-07;, score=0.257 total time=   0.0s\n",
      "[CV 3/4] END alpha=15, selection=random, tol=1e-07;, score=0.076 total time=   0.0s\n",
      "[CV 4/4] END alpha=15, selection=cyclic, tol=1e-05;, score=-0.067 total time=   0.0s\n",
      "[CV 2/4] END alpha=14, selection=cyclic, tol=1e-07;, score=0.241 total time=   0.0s\n",
      "[CV 2/4] END alpha=13, selection=random, tol=1e-05;, score=0.257 total time=   0.0s\n",
      "[CV 1/4] END alpha=14, selection=random, tol=0.01;, score=0.022 total time=   0.0s\n",
      "[CV 4/4] END alpha=15, selection=cyclic, tol=0.01;, score=-0.070 total time=   0.0s\n",
      "[CV 2/4] END alpha=16, selection=cyclic, tol=1e-05;, score=0.204 total time=   0.0s\n",
      "[CV 1/4] END alpha=13, selection=random, tol=0.01;, score=0.043 total time=   0.0s\n",
      "[CV 4/4] END alpha=14, selection=cyclic, tol=0.01;, score=-0.029 total time=   0.0s\n",
      "[CV 4/4] END alpha=15, selection=random, tol=1e-07;, score=-0.067 total time=   0.0s\n",
      "[CV 3/4] END alpha=13, selection=cyclic, tol=1e-07;, score=0.111 total time=   0.0s\n",
      "[CV 1/4] END alpha=15, selection=cyclic, tol=1e-07;, score=-0.007 total time=   0.0s\n",
      "[CV 3/4] END alpha=14, selection=cyclic, tol=1e-07;, score=0.094 total time=   0.0s\n",
      "[CV 1/4] END alpha=15, selection=random, tol=1e-05;, score=-0.007 total time=   0.0s\n",
      "[CV 3/4] END alpha=13, selection=random, tol=1e-05;, score=0.111 total time=   0.0s\n",
      "[CV 2/4] END alpha=14, selection=random, tol=0.01;, score=0.243 total time=   0.0s\n",
      "[CV 3/4] END alpha=16, selection=cyclic, tol=1e-05;, score=0.057 total time=   0.0s\n",
      "[CV 4/4] END alpha=14, selection=cyclic, tol=1e-07;, score=-0.026 total time=   0.0s\n",
      "[CV 2/4] END alpha=15, selection=cyclic, tol=1e-07;, score=0.223 total time=   0.0s\n",
      "[CV 2/4] END alpha=15, selection=random, tol=1e-05;, score=0.223 total time=   0.0s\n",
      "[CV 2/4] END alpha=13, selection=random, tol=0.01;, score=0.257 total time=   0.0s\n",
      "[CV 1/4] END alpha=14, selection=random, tol=1e-05;, score=0.019 total time=   0.0s\n",
      "[CV 4/4] END alpha=13, selection=cyclic, tol=1e-07;, score=0.011 total time=   0.0s\n",
      "[CV 1/4] END alpha=15, selection=random, tol=0.01;, score=-0.009 total time=   0.0s\n",
      "[CV 1/4] END alpha=16, selection=cyclic, tol=0.01;, score=-0.039 total time=   0.0s\n",
      "[CV 4/4] END alpha=13, selection=random, tol=1e-05;, score=0.011 total time=   0.0s\n",
      "[CV 4/4] END alpha=16, selection=cyclic, tol=1e-05;, score=-0.111 total time=   0.0s\n",
      "[CV 3/4] END alpha=14, selection=random, tol=0.01;, score=0.096 total time=   0.0s\n",
      "[CV 3/4] END alpha=15, selection=random, tol=1e-05;, score=0.076 total time=   0.0s\n",
      "[CV 3/4] END alpha=15, selection=cyclic, tol=1e-07;, score=0.076 total time=   0.0s\n",
      "[CV 3/4] END alpha=13, selection=random, tol=0.01;, score=0.123 total time=   0.0s\n",
      "[CV 2/4] END alpha=15, selection=random, tol=0.01;, score=0.224 total time=   0.0s\n",
      "[CV 2/4] END alpha=14, selection=random, tol=1e-05;, score=0.241 total time=   0.0s\n",
      "[CV 2/4] END alpha=16, selection=cyclic, tol=0.01;, score=0.204 total time=   0.0s\n",
      "[CV 4/4] END alpha=15, selection=random, tol=1e-05;, score=-0.067 total time=   0.0s\n",
      "[CV 1/4] END alpha=16, selection=cyclic, tol=1e-07;, score=-0.035 total time=   0.0s\n",
      "[CV 1/4] END alpha=16, selection=random, tol=1e-07;, score=-0.035 total time=   0.0s\n",
      "[CV 4/4] END alpha=14, selection=random, tol=0.01;, score=-0.029 total time=   0.0s\n",
      "[CV 4/4] END alpha=15, selection=cyclic, tol=1e-07;, score=-0.067 total time=   0.0s\n",
      "[CV 3/4] END alpha=15, selection=random, tol=0.01;, score=0.074 total time=   0.0s\n",
      "[CV 3/4] END alpha=14, selection=random, tol=1e-05;, score=0.094 total time=   0.0s\n",
      "[CV 4/4] END alpha=13, selection=random, tol=0.01;, score=0.022 total time=   0.0s\n",
      "[CV 2/4] END alpha=16, selection=random, tol=1e-07;, score=0.204 total time=   0.0s\n",
      "[CV 2/4] END alpha=16, selection=cyclic, tol=1e-07;, score=0.204 total time=   0.0s\n",
      "[CV 3/4] END alpha=16, selection=cyclic, tol=0.01;, score=0.058 total time=   0.0s\n",
      "[CV 1/4] END alpha=17, selection=cyclic, tol=1e-05;, score=-0.065 total time=   0.0s\n",
      "[CV 4/4] END alpha=15, selection=random, tol=0.01;, score=-0.060 total time=   0.0s\n",
      "[CV 4/4] END alpha=14, selection=random, tol=1e-05;, score=-0.026 total time=   0.0s\n",
      "[CV 3/4] END alpha=16, selection=random, tol=1e-07;, score=0.057 total time=   0.0s\n",
      "[CV 3/4] END alpha=16, selection=cyclic, tol=1e-07;, score=0.057 total time=   0.0s\n",
      "[CV 4/4] END alpha=16, selection=cyclic, tol=0.01;, score=-0.113 total time=   0.0s\n",
      "[CV 4/4] END alpha=16, selection=random, tol=1e-07;, score=-0.111 total time=   0.0s\n",
      "[CV 2/4] END alpha=17, selection=cyclic, tol=1e-05;, score=0.185 total time=   0.0s\n",
      "[CV 4/4] END alpha=16, selection=cyclic, tol=1e-07;, score=-0.111 total time=   0.0s\n",
      "[CV 1/4] END alpha=16, selection=random, tol=1e-05;, score=-0.035 total time=   0.0s\n",
      "[CV 1/4] END alpha=16, selection=random, tol=0.01;, score=-0.030 total time=   0.0s\n",
      "[CV 3/4] END alpha=17, selection=cyclic, tol=1e-05;, score=0.038 total time=   0.0s\n",
      "[CV 1/4] END alpha=17, selection=cyclic, tol=0.01;, score=-0.069 total time=   0.0s\n",
      "[CV 2/4] END alpha=16, selection=random, tol=1e-05;, score=0.204 total time=   0.0s\n",
      "[CV 2/4] END alpha=16, selection=random, tol=0.01;, score=0.206 total time=   0.0s\n",
      "[CV 4/4] END alpha=17, selection=cyclic, tol=1e-05;, score=-0.158 total time=   0.0s\n",
      "[CV 2/4] END alpha=17, selection=cyclic, tol=0.01;, score=0.185 total time=   0.0s\n",
      "[CV 3/4] END alpha=16, selection=random, tol=0.01;, score=0.053 total time=   0.0s\n",
      "[CV 3/4] END alpha=16, selection=random, tol=1e-05;, score=0.057 total time=   0.0s\n",
      "[CV 1/4] END alpha=17, selection=cyclic, tol=1e-07;, score=-0.065 total time=   0.0s\n",
      "[CV 4/4] END alpha=16, selection=random, tol=0.01;, score=-0.110 total time=   0.0s\n",
      "[CV 3/4] END alpha=17, selection=cyclic, tol=0.01;, score=0.038 total time=   0.0s\n",
      "[CV 1/4] END alpha=17, selection=random, tol=1e-07;, score=-0.065 total time=   0.0s\n",
      "[CV 1/4] END alpha=18, selection=cyclic, tol=1e-05;, score=-0.097 total time=   0.0s\n",
      "[CV 2/4] END alpha=17, selection=cyclic, tol=1e-07;, score=0.185 total time=   0.0s\n",
      "[CV 4/4] END alpha=16, selection=random, tol=1e-05;, score=-0.111 total time=   0.0s\n",
      "[CV 1/4] END alpha=18, selection=cyclic, tol=0.01;, score=-0.100 total time=   0.0s\n",
      "[CV 2/4] END alpha=17, selection=random, tol=1e-07;, score=0.185 total time=   0.0s\n",
      "[CV 1/4] END alpha=18, selection=random, tol=1e-07;, score=-0.097 total time=   0.0s\n",
      "[CV 4/4] END alpha=17, selection=cyclic, tol=0.01;, score=-0.161 total time=   0.0s\n",
      "[CV 1/4] END alpha=19, selection=random, tol=1e-07;, score=-0.130 total time=   0.0s\n",
      "[CV 2/4] END alpha=18, selection=cyclic, tol=1e-05;, score=0.165 total time=   0.0s\n",
      "[CV 1/4] END alpha=19, selection=cyclic, tol=1e-05;, score=-0.130 total time=   0.0s\n",
      "[CV 1/4] END alpha=19, selection=cyclic, tol=0.01;, score=-0.134 total time=   0.0s\n",
      "[CV 3/4] END alpha=17, selection=cyclic, tol=1e-07;, score=0.038 total time=   0.0s\n",
      "[CV 2/4] END alpha=18, selection=cyclic, tol=0.01;, score=0.165 total time=   0.0s\n",
      "[CV 1/4] END alpha=20, selection=cyclic, tol=1e-05;, score=-0.166 total time=   0.0s\n",
      "[CV 2/4] END alpha=19, selection=random, tol=1e-07;, score=0.144 total time=   0.0s\n",
      "[CV 2/4] END alpha=18, selection=random, tol=1e-07;, score=0.165 total time=   0.0s[CV 2/4] END alpha=19, selection=cyclic, tol=1e-05;, score=0.144 total time=   0.0s\n",
      "\n",
      "[CV 3/4] END alpha=17, selection=random, tol=1e-07;, score=0.038 total time=   0.0s\n",
      "[CV 1/4] END alpha=17, selection=random, tol=1e-05;, score=-0.065 total time=   0.0s\n",
      "[CV 2/4] END alpha=19, selection=cyclic, tol=0.01;, score=0.144 total time=   0.0s\n",
      "[CV 1/4] END alpha=20, selection=cyclic, tol=0.01;, score=-0.169 total time=   0.0s\n",
      "[CV 4/4] END alpha=17, selection=cyclic, tol=1e-07;, score=-0.158 total time=   0.0s\n",
      "[CV 3/4] END alpha=19, selection=random, tol=1e-07;, score=-0.003 total time=   0.0s\n",
      "[CV 2/4] END alpha=20, selection=cyclic, tol=1e-05;, score=0.122 total time=   0.0s\n",
      "[CV 3/4] END alpha=18, selection=cyclic, tol=1e-05;, score=0.018 total time=   0.0s\n",
      "[CV 3/4] END alpha=18, selection=cyclic, tol=0.01;, score=0.018 total time=   0.0s\n",
      "[CV 1/4] END alpha=20, selection=random, tol=1e-07;, score=-0.166 total time=   0.0s\n",
      "[CV 3/4] END alpha=19, selection=cyclic, tol=1e-05;, score=-0.003 total time=   0.0s\n",
      "[CV 4/4] END alpha=17, selection=random, tol=1e-07;, score=-0.158 total time=   0.0s\n",
      "[CV 2/4] END alpha=17, selection=random, tol=1e-05;, score=0.185 total time=   0.0s\n",
      "[CV 3/4] END alpha=19, selection=cyclic, tol=0.01;, score=-0.003 total time=   0.0s\n",
      "[CV 3/4] END alpha=18, selection=random, tol=1e-07;, score=0.018 total time=   0.0s\n",
      "[CV 4/4] END alpha=19, selection=random, tol=1e-07;, score=-0.262 total time=   0.0s\n",
      "[CV 3/4] END alpha=20, selection=cyclic, tol=1e-05;, score=-0.025 total time=   0.0s\n",
      "[CV 4/4] END alpha=18, selection=cyclic, tol=0.01;, score=-0.211 total time=   0.0s\n",
      "[CV 4/4] END alpha=18, selection=cyclic, tol=1e-05;, score=-0.208 total time=   0.0s\n",
      "[CV 2/4] END alpha=20, selection=cyclic, tol=0.01;, score=0.122 total time=   0.0s\n",
      "[CV 4/4] END alpha=19, selection=cyclic, tol=1e-05;, score=-0.262 total time=   0.0s\n",
      "[CV 2/4] END alpha=20, selection=random, tol=1e-07;, score=0.122 total time=   0.0s\n",
      "[CV 1/4] END alpha=17, selection=random, tol=0.01;, score=-0.066 total time=   0.0s\n",
      "[CV 1/4] END alpha=21, selection=cyclic, tol=1e-05;, score=-0.203 total time=   0.0s\n",
      "[CV 4/4] END alpha=19, selection=cyclic, tol=0.01;, score=-0.264 total time=   0.0s\n",
      "[CV 4/4] END alpha=20, selection=cyclic, tol=1e-05;, score=-0.319 total time=   0.0s\n",
      "[CV 3/4] END alpha=17, selection=random, tol=1e-05;, score=0.038 total time=   0.0s\n",
      "[CV 4/4] END alpha=18, selection=random, tol=1e-07;, score=-0.208 total time=   0.0s\n",
      "[CV 1/4] END alpha=21, selection=cyclic, tol=0.01;, score=-0.207 total time=   0.0s\n",
      "[CV 1/4] END alpha=18, selection=random, tol=1e-05;, score=-0.097 total time=   0.0s\n",
      "[CV 1/4] END alpha=18, selection=cyclic, tol=1e-07;, score=-0.097 total time=   0.0s\n",
      "[CV 1/4] END alpha=21, selection=random, tol=1e-07;, score=-0.203 total time=   0.0s\n",
      "[CV 2/4] END alpha=21, selection=cyclic, tol=1e-05;, score=0.099 total time=   0.0s\n",
      "[CV 2/4] END alpha=17, selection=random, tol=0.01;, score=0.185 total time=   0.0s\n",
      "[CV 3/4] END alpha=20, selection=random, tol=1e-07;, score=-0.025 total time=   0.0s\n",
      "[CV 3/4] END alpha=20, selection=cyclic, tol=0.01;, score=-0.024 total time=   0.0s\n",
      "[CV 1/4] END alpha=19, selection=random, tol=1e-05;, score=-0.130 total time=   0.0s[CV 2/4] END alpha=21, selection=cyclic, tol=0.01;, score=0.099 total time=   0.0s\n",
      "\n",
      "[CV 1/4] END alpha=18, selection=random, tol=0.01;, score=-0.095 total time=   0.0s\n",
      "[CV 1/4] END alpha=20, selection=cyclic, tol=1e-07;, score=-0.166 total time=   0.0s\n",
      "[CV 1/4] END alpha=22, selection=cyclic, tol=1e-05;, score=-0.242 total time=   0.0s\n",
      "[CV 2/4] END alpha=18, selection=random, tol=1e-05;, score=0.165 total time=   0.0s\n",
      "[CV 4/4] END alpha=17, selection=random, tol=1e-05;, score=-0.158 total time=   0.0s[CV 2/4] END alpha=18, selection=cyclic, tol=1e-07;, score=0.165 total time=   0.0s\n",
      "\n",
      "[CV 3/4] END alpha=21, selection=cyclic, tol=1e-05;, score=-0.047 total time=   0.0s[CV 2/4] END alpha=21, selection=random, tol=1e-07;, score=0.099 total time=   0.0s\n",
      "\n",
      "[CV 4/4] END alpha=20, selection=random, tol=1e-07;, score=-0.319 total time=   0.0s[CV 3/4] END alpha=21, selection=cyclic, tol=0.01;, score=-0.046 total time=   0.0s\n",
      "\n",
      "[CV 2/4] END alpha=19, selection=random, tol=1e-05;, score=0.144 total time=   0.0s[CV 2/4] END alpha=18, selection=random, tol=0.01;, score=0.162 total time=   0.0s\n",
      "\n",
      "[CV 3/4] END alpha=17, selection=random, tol=0.01;, score=0.055 total time=   0.0s[CV 2/4] END alpha=20, selection=cyclic, tol=1e-07;, score=0.122 total time=   0.0s\n",
      "[CV 3/4] END alpha=18, selection=random, tol=1e-05;, score=0.018 total time=   0.0s\n",
      "[CV 3/4] END alpha=18, selection=cyclic, tol=1e-07;, score=0.018 total time=   0.0s[CV 1/4] END alpha=19, selection=random, tol=0.01;, score=-0.124 total time=   0.0s\n",
      "\n",
      "\n",
      "[CV 4/4] END alpha=20, selection=cyclic, tol=0.01;, score=-0.321 total time=   0.0s\n",
      "[CV 4/4] END alpha=21, selection=cyclic, tol=1e-05;, score=-0.379 total time=   0.0s\n",
      "[CV 1/4] END alpha=20, selection=random, tol=0.01;, score=-0.159 total time=   0.0s[CV 1/4] END alpha=22, selection=cyclic, tol=0.01;, score=-0.242 total time=   0.0s\n",
      "[CV 3/4] END alpha=19, selection=random, tol=1e-05;, score=-0.003 total time=   0.0s\n",
      "\n",
      "[CV 1/4] END alpha=19, selection=cyclic, tol=1e-07;, score=-0.130 total time=   0.0s\n",
      "[CV 3/4] END alpha=20, selection=cyclic, tol=1e-07;, score=-0.025 total time=   0.0s\n",
      "[CV 4/4] END alpha=18, selection=random, tol=1e-05;, score=-0.208 total time=   0.0s\n",
      "[CV 3/4] END alpha=18, selection=random, tol=0.01;, score=0.013 total time=   0.0s\n",
      "[CV 2/4] END alpha=19, selection=random, tol=0.01;, score=0.142 total time=   0.0s\n",
      "[CV 4/4] END alpha=18, selection=cyclic, tol=1e-07;, score=-0.208 total time=   0.0s\n",
      "[CV 4/4] END alpha=21, selection=cyclic, tol=0.01;, score=-0.381 total time=   0.0s\n",
      "[CV 4/4] END alpha=17, selection=random, tol=0.01;, score=-0.191 total time=   0.0s\n",
      "[CV 2/4] END alpha=22, selection=cyclic, tol=1e-05;, score=0.075 total time=   0.0s\n",
      "[CV 4/4] END alpha=20, selection=cyclic, tol=1e-07;, score=-0.319 total time=   0.0s\n",
      "[CV 4/4] END alpha=19, selection=random, tol=1e-05;, score=-0.262 total time=   0.0s\n",
      "[CV 3/4] END alpha=19, selection=random, tol=0.01;, score=-0.010 total time=   0.0s\n",
      "[CV 2/4] END alpha=20, selection=random, tol=0.01;, score=0.122 total time=   0.0s\n",
      "[CV 2/4] END alpha=22, selection=cyclic, tol=0.01;, score=0.075 total time=   0.0s\n",
      "[CV 2/4] END alpha=19, selection=cyclic, tol=1e-07;, score=0.144 total time=   0.0s\n",
      "[CV 4/4] END alpha=18, selection=random, tol=0.01;, score=-0.171 total time=   0.0s\n",
      "[CV 1/4] END alpha=20, selection=random, tol=1e-05;, score=-0.166 total time=   0.0s\n",
      "[CV 1/4] END alpha=23, selection=cyclic, tol=1e-05;, score=-0.283 total time=   0.0s\n",
      "[CV 3/4] END alpha=22, selection=cyclic, tol=1e-05;, score=-0.069 total time=   0.0s\n",
      "[CV 1/4] END alpha=22, selection=random, tol=1e-07;, score=-0.242 total time=   0.0s\n",
      "[CV 3/4] END alpha=21, selection=random, tol=1e-07;, score=-0.047 total time=   0.0s\n",
      "[CV 1/4] END alpha=21, selection=random, tol=1e-05;, score=-0.203 total time=   0.0s\n",
      "[CV 4/4] END alpha=19, selection=random, tol=0.01;, score=-0.265 total time=   0.0s\n",
      "[CV 3/4] END alpha=22, selection=cyclic, tol=0.01;, score=-0.069 total time=   0.0s\n",
      "[CV 3/4] END alpha=20, selection=random, tol=0.01;, score=-0.021 total time=   0.0s\n",
      "[CV 1/4] END alpha=23, selection=random, tol=1e-07;, score=-0.283 total time=   0.0s\n",
      "[CV 3/4] END alpha=19, selection=cyclic, tol=1e-07;, score=-0.003 total time=   0.0s\n",
      "[CV 1/4] END alpha=24, selection=cyclic, tol=0.01;, score=-0.326 total time=   0.0s\n",
      "[CV 1/4] END alpha=21, selection=cyclic, tol=1e-07;, score=-0.203 total time=   0.0s\n",
      "[CV 2/4] END alpha=23, selection=cyclic, tol=1e-05;, score=0.051 total time=   0.0s\n",
      "[CV 2/4] END alpha=20, selection=random, tol=1e-05;, score=0.122 total time=   0.0s\n",
      "[CV 4/4] END alpha=22, selection=cyclic, tol=1e-05;, score=-0.443 total time=   0.0s\n",
      "[CV 2/4] END alpha=22, selection=random, tol=1e-07;, score=0.075 total time=   0.0s\n",
      "[CV 4/4] END alpha=22, selection=cyclic, tol=0.01;, score=-0.445 total time=   0.0s\n",
      "[CV 4/4] END alpha=20, selection=random, tol=0.01;, score=-0.322 total time=   0.0s[CV 2/4] END alpha=23, selection=random, tol=1e-07;, score=0.051 total time=   0.0s\n",
      "\n",
      "[CV 2/4] END alpha=21, selection=cyclic, tol=1e-07;, score=0.099 total time=   0.0s\n",
      "[CV 4/4] END alpha=21, selection=random, tol=1e-07;, score=-0.379 total time=   0.0s\n",
      "[CV 1/4] END alpha=25, selection=cyclic, tol=1e-05;, score=-0.371 total time=   0.0s\n",
      "[CV 3/4] END alpha=23, selection=cyclic, tol=1e-05;, score=-0.093 total time=   0.0s\n",
      "[CV 2/4] END alpha=24, selection=cyclic, tol=0.01;, score=0.025 total time=   0.0s\n",
      "[CV 1/4] END alpha=22, selection=cyclic, tol=1e-07;, score=-0.242 total time=   0.0s\n",
      "[CV 3/4] END alpha=20, selection=random, tol=1e-05;, score=-0.025 total time=   0.0s\n",
      "[CV 3/4] END alpha=22, selection=random, tol=1e-07;, score=-0.069 total time=   0.0s\n",
      "[CV 1/4] END alpha=23, selection=cyclic, tol=0.01;, score=-0.283 total time=   0.0s\n",
      "[CV 1/4] END alpha=22, selection=random, tol=1e-05;, score=-0.242 total time=   0.0s\n",
      "[CV 3/4] END alpha=23, selection=random, tol=1e-07;, score=-0.093 total time=   0.0s\n",
      "[CV 3/4] END alpha=21, selection=cyclic, tol=1e-07;, score=-0.047 total time=   0.0s\n",
      "[CV 4/4] END alpha=19, selection=cyclic, tol=1e-07;, score=-0.262 total time=   0.0s\n",
      "[CV 1/4] END alpha=21, selection=random, tol=0.01;, score=-0.203 total time=   0.0s\n",
      "[CV 2/4] END alpha=21, selection=random, tol=1e-05;, score=0.099 total time=   0.0s\n",
      "[CV 4/4] END alpha=23, selection=cyclic, tol=1e-05;, score=-0.509 total time=   0.0s[CV 4/4] END alpha=20, selection=random, tol=1e-05;, score=-0.319 total time=   0.0s\n",
      "\n",
      "[CV 4/4] END alpha=22, selection=random, tol=1e-07;, score=-0.443 total time=   0.0s\n",
      "[CV 2/4] END alpha=22, selection=cyclic, tol=1e-07;, score=0.075 total time=   0.0s\n",
      "[CV 2/4] END alpha=22, selection=random, tol=1e-05;, score=0.075 total time=   0.0s\n",
      "[CV 3/4] END alpha=24, selection=cyclic, tol=0.01;, score=-0.116 total time=   0.0s\n",
      "[CV 2/4] END alpha=23, selection=cyclic, tol=0.01;, score=0.051 total time=   0.0s\n",
      "[CV 1/4] END alpha=25, selection=random, tol=1e-07;, score=-0.371 total time=   0.0s\n",
      "[CV 4/4] END alpha=23, selection=random, tol=1e-07;, score=-0.509 total time=   0.0s\n",
      "[CV 3/4] END alpha=21, selection=random, tol=1e-05;, score=-0.047 total time=   0.0s\n",
      "[CV 4/4] END alpha=21, selection=cyclic, tol=1e-07;, score=-0.379 total time=   0.0s\n",
      "[CV 3/4] END alpha=22, selection=random, tol=1e-05;, score=-0.069 total time=   0.0s\n",
      "[CV 1/4] END alpha=23, selection=cyclic, tol=1e-07;, score=-0.283 total time=   0.0s\n",
      "[CV 1/4] END alpha=22, selection=random, tol=0.01;, score=-0.243 total time=   0.0s\n",
      "[CV 3/4] END alpha=22, selection=cyclic, tol=1e-07;, score=-0.069 total time=   0.0s\n",
      "[CV 3/4] END alpha=23, selection=cyclic, tol=0.01;, score=-0.092 total time=   0.0s\n",
      "[CV 4/4] END alpha=24, selection=cyclic, tol=0.01;, score=-0.581 total time=   0.0s\n",
      "[CV 1/4] END alpha=23, selection=random, tol=0.01;, score=-0.290 total time=   0.0s\n",
      "[CV 1/4] END alpha=27, selection=cyclic, tol=1e-05;, score=-0.467 total time=   0.0s\n",
      "[CV 2/4] END alpha=21, selection=random, tol=0.01;, score=0.103 total time=   0.0s\n",
      "[CV 1/4] END alpha=26, selection=cyclic, tol=0.01;, score=-0.418 total time=   0.0s\n",
      "[CV 4/4] END alpha=22, selection=random, tol=1e-05;, score=-0.443 total time=   0.0s\n",
      "[CV 4/4] END alpha=21, selection=random, tol=1e-05;, score=-0.379 total time=   0.0s\n",
      "[CV 2/4] END alpha=23, selection=cyclic, tol=1e-07;, score=0.051 total time=   0.0s\n",
      "[CV 2/4] END alpha=22, selection=random, tol=0.01;, score=0.076 total time=   0.0s\n",
      "[CV 4/4] END alpha=22, selection=cyclic, tol=1e-07;, score=-0.443 total time=   0.0s\n",
      "[CV 4/4] END alpha=23, selection=cyclic, tol=0.01;, score=-0.511 total time=   0.0s\n",
      "[CV 2/4] END alpha=25, selection=random, tol=1e-07;, score=-0.001 total time=   0.0s\n",
      "[CV 1/4] END alpha=24, selection=random, tol=1e-05;, score=-0.326 total time=   0.0s\n",
      "[CV 2/4] END alpha=25, selection=cyclic, tol=1e-05;, score=-0.001 total time=   0.0s\n",
      "[CV 2/4] END alpha=26, selection=cyclic, tol=0.01;, score=-0.016 total time=   0.0s\n",
      "[CV 2/4] END alpha=23, selection=random, tol=0.01;, score=0.056 total time=   0.0s\n",
      "[CV 3/4] END alpha=21, selection=random, tol=0.01;, score=-0.043 total time=   0.0s\n",
      "[CV 2/4] END alpha=27, selection=cyclic, tol=1e-05;, score=-0.019 total time=   0.0s\n",
      "[CV 3/4] END alpha=23, selection=cyclic, tol=1e-07;, score=-0.093 total time=   0.0s\n",
      "[CV 3/4] END alpha=26, selection=cyclic, tol=0.01;, score=-0.167 total time=   0.0s\n",
      "[CV 3/4] END alpha=22, selection=random, tol=0.01;, score=-0.064 total time=   0.0s\n",
      "[CV 3/4] END alpha=25, selection=cyclic, tol=1e-05;, score=-0.141 total time=   0.0s[CV 2/4] END alpha=24, selection=random, tol=1e-05;, score=0.025 total time=   0.0s\n",
      "\n",
      "[CV 3/4] END alpha=25, selection=random, tol=1e-07;, score=-0.141 total time=   0.0s\n",
      "[CV 1/4] END alpha=23, selection=random, tol=1e-05;, score=-0.283 total time=   0.0s\n",
      "[CV 1/4] END alpha=28, selection=cyclic, tol=0.01;, score=-0.517 total time=   0.0s\n",
      "[CV 4/4] END alpha=21, selection=random, tol=0.01;, score=-0.374 total time=   0.0s\n",
      "[CV 3/4] END alpha=23, selection=random, tol=0.01;, score=-0.093 total time=   0.0s\n",
      "[CV 3/4] END alpha=27, selection=cyclic, tol=1e-05;, score=-0.180 total time=   0.0s\n",
      "[CV 1/4] END alpha=27, selection=random, tol=1e-07;, score=-0.467 total time=   0.0s\n",
      "[CV 4/4] END alpha=26, selection=cyclic, tol=0.01;, score=-0.731 total time=   0.0s\n",
      "[CV 1/4] END alpha=29, selection=cyclic, tol=1e-05;, score=-0.569 total time=   0.0s\n",
      "[CV 4/4] END alpha=23, selection=cyclic, tol=1e-07;, score=-0.509 total time=   0.0s\n",
      "[CV 4/4] END alpha=22, selection=random, tol=0.01;, score=-0.320 total time=   0.0s\n",
      "[CV 2/4] END alpha=28, selection=cyclic, tol=0.01;, score=-0.021 total time=   0.0s\n",
      "[CV 4/4] END alpha=25, selection=random, tol=1e-07;, score=-0.653 total time=   0.0s\n",
      "[CV 4/4] END alpha=25, selection=cyclic, tol=1e-05;, score=-0.653 total time=   0.0s\n",
      "[CV 3/4] END alpha=24, selection=random, tol=1e-05;, score=-0.117 total time=   0.0s\n",
      "[CV 2/4] END alpha=23, selection=random, tol=1e-05;, score=0.051 total time=   0.0s\n",
      "[CV 4/4] END alpha=27, selection=cyclic, tol=1e-05;, score=-0.809 total time=   0.0s\n",
      "[CV 4/4] END alpha=23, selection=random, tol=0.01;, score=-0.492 total time=   0.0s\n",
      "[CV 1/4] END alpha=26, selection=random, tol=1e-05;, score=-0.418 total time=   0.0s\n",
      "[CV 3/4] END alpha=28, selection=cyclic, tol=0.01;, score=-0.181 total time=   0.0s\n",
      "[CV 3/4] END alpha=23, selection=random, tol=1e-05;, score=-0.093 total time=   0.0s\n",
      "[CV 4/4] END alpha=24, selection=random, tol=1e-05;, score=-0.579 total time=   0.0s\n",
      "[CV 1/4] END alpha=25, selection=cyclic, tol=1e-07;, score=-0.371 total time=   0.0s\n",
      "[CV 1/4] END alpha=27, selection=cyclic, tol=1e-07;, score=-0.467 total time=   0.0s\n",
      "[CV 1/4] END alpha=24, selection=cyclic, tol=1e-05;, score=-0.326 total time=   0.0s\n",
      "[CV 2/4] END alpha=29, selection=cyclic, tol=1e-05;, score=-0.024 total time=   0.0s\n",
      "[CV 2/4] END alpha=26, selection=random, tol=1e-05;, score=-0.016 total time=   0.0s\n",
      "[CV 2/4] END alpha=27, selection=random, tol=1e-07;, score=-0.019 total time=   0.0s\n",
      "[CV 1/4] END alpha=30, selection=cyclic, tol=0.01;, score=-0.624 total time=   0.0s\n",
      "[CV 2/4] END alpha=27, selection=cyclic, tol=1e-07;, score=-0.019 total time=   0.0s\n",
      "[CV 4/4] END alpha=23, selection=random, tol=1e-05;, score=-0.509 total time=   0.0s\n",
      "[CV 4/4] END alpha=28, selection=cyclic, tol=0.01;, score=-0.894 total time=   0.0s\n",
      "[CV 3/4] END alpha=26, selection=random, tol=1e-05;, score=-0.167 total time=   0.0s[CV 1/4] END alpha=24, selection=random, tol=1e-07;, score=-0.326 total time=   0.0s\n",
      "\n",
      "[CV 1/4] END alpha=31, selection=cyclic, tol=1e-05;, score=-0.680 total time=   0.0s[CV 2/4] END alpha=25, selection=cyclic, tol=1e-07;, score=-0.001 total time=   0.0s\n",
      "\n",
      "[CV 2/4] END alpha=24, selection=cyclic, tol=1e-05;, score=0.025 total time=   0.0s\n",
      "[CV 3/4] END alpha=29, selection=cyclic, tol=1e-05;, score=-0.182 total time=   0.0s\n",
      "[CV 3/4] END alpha=27, selection=random, tol=1e-07;, score=-0.180 total time=   0.0s\n",
      "[CV 2/4] END alpha=30, selection=cyclic, tol=0.01;, score=-0.026 total time=   0.0s\n",
      "[CV 3/4] END alpha=24, selection=cyclic, tol=1e-05;, score=-0.117 total time=   0.0s\n",
      "[CV 3/4] END alpha=27, selection=cyclic, tol=1e-07;, score=-0.180 total time=   0.0s\n",
      "[CV 4/4] END alpha=26, selection=random, tol=1e-05;, score=-0.729 total time=   0.0s\n",
      "[CV 1/4] END alpha=28, selection=random, tol=1e-05;, score=-0.517 total time=   0.0s\n",
      "[CV 3/4] END alpha=25, selection=cyclic, tol=1e-07;, score=-0.141 total time=   0.0s\n",
      "[CV 2/4] END alpha=24, selection=random, tol=1e-07;, score=0.025 total time=   0.0s\n",
      "[CV 2/4] END alpha=31, selection=cyclic, tol=1e-05;, score=-0.029 total time=   0.0s\n",
      "[CV 1/4] END alpha=29, selection=random, tol=1e-07;, score=-0.569 total time=   0.0s\n",
      "[CV 4/4] END alpha=29, selection=cyclic, tol=1e-05;, score=-0.979 total time=   0.0s\n",
      "[CV 1/4] END alpha=25, selection=random, tol=0.01;, score=-0.380 total time=   0.0s\n",
      "[CV 4/4] END alpha=24, selection=cyclic, tol=1e-05;, score=-0.579 total time=   0.0s\n",
      "[CV 4/4] END alpha=27, selection=random, tol=1e-07;, score=-0.809 total time=   0.0s\n",
      "[CV 4/4] END alpha=27, selection=cyclic, tol=1e-07;, score=-0.809 total time=   0.0s\n",
      "[CV 2/4] END alpha=28, selection=random, tol=1e-05;, score=-0.021 total time=   0.0s\n",
      "[CV 1/4] END alpha=26, selection=random, tol=1e-07;, score=-0.418 total time=   0.0s\n",
      "[CV 3/4] END alpha=30, selection=cyclic, tol=0.01;, score=-0.183 total time=   0.0s\n",
      "[CV 4/4] END alpha=25, selection=cyclic, tol=1e-07;, score=-0.653 total time=   0.0s\n",
      "[CV 3/4] END alpha=24, selection=random, tol=1e-07;, score=-0.117 total time=   0.0s\n",
      "[CV 1/4] END alpha=31, selection=random, tol=1e-07;, score=-0.680 total time=   0.0s\n",
      "[CV 1/4] END alpha=24, selection=cyclic, tol=1e-07;, score=-0.326 total time=   0.0s\n",
      "[CV 2/4] END alpha=29, selection=random, tol=1e-07;, score=-0.024 total time=   0.0s\n",
      "[CV 1/4] END alpha=29, selection=cyclic, tol=1e-07;, score=-0.569 total time=   0.0s\n",
      "[CV 1/4] END alpha=27, selection=cyclic, tol=0.01;, score=-0.466 total time=   0.0s[CV 1/4] END alpha=27, selection=random, tol=0.01;, score=-0.550 total time=   0.0s\n",
      "\n",
      "[CV 4/4] END alpha=30, selection=cyclic, tol=0.01;, score=-1.069 total time=   0.0s\n",
      "[CV 2/4] END alpha=26, selection=random, tol=1e-07;, score=-0.016 total time=   0.0s\n",
      "[CV 3/4] END alpha=28, selection=random, tol=1e-05;, score=-0.181 total time=   0.0s\n",
      "[CV 2/4] END alpha=24, selection=cyclic, tol=1e-07;, score=0.025 total time=   0.0s\n",
      "[CV 4/4] END alpha=24, selection=random, tol=1e-07;, score=-0.579 total time=   0.0s\n",
      "[CV 1/4] END alpha=25, selection=cyclic, tol=0.01;, score=-0.371 total time=   0.0s\n",
      "[CV 2/4] END alpha=25, selection=random, tol=0.01;, score=-0.001 total time=   0.0s\n",
      "[CV 3/4] END alpha=31, selection=cyclic, tol=1e-05;, score=-0.184 total time=   0.0s\n",
      "[CV 2/4] END alpha=29, selection=cyclic, tol=1e-07;, score=-0.024 total time=   0.0s\n",
      "[CV 3/4] END alpha=29, selection=random, tol=1e-07;, score=-0.182 total time=   0.0s\n",
      "[CV 3/4] END alpha=24, selection=cyclic, tol=1e-07;, score=-0.117 total time=   0.0s\n",
      "[CV 3/4] END alpha=26, selection=random, tol=1e-07;, score=-0.167 total time=   0.0s[CV 2/4] END alpha=27, selection=cyclic, tol=0.01;, score=-0.019 total time=   0.0s\n",
      "\n",
      "[CV 2/4] END alpha=27, selection=random, tol=0.01;, score=-0.019 total time=   0.0s\n",
      "[CV 4/4] END alpha=28, selection=random, tol=1e-05;, score=-0.892 total time=   0.0s\n",
      "[CV 1/4] END alpha=24, selection=random, tol=0.01;, score=-0.295 total time=   0.0s\n",
      "[CV 2/4] END alpha=25, selection=cyclic, tol=0.01;, score=-0.001 total time=   0.0s\n",
      "[CV 4/4] END alpha=31, selection=cyclic, tol=1e-05;, score=-1.161 total time=   0.0s\n",
      "[CV 4/4] END alpha=24, selection=cyclic, tol=1e-07;, score=-0.579 total time=   0.0s\n",
      "[CV 3/4] END alpha=29, selection=cyclic, tol=1e-07;, score=-0.182 total time=   0.0s\n",
      "[CV 2/4] END alpha=31, selection=random, tol=1e-07;, score=-0.029 total time=   0.0s\n",
      "[CV 4/4] END alpha=29, selection=random, tol=1e-07;, score=-0.979 total time=   0.0s\n",
      "[CV 4/4] END alpha=26, selection=random, tol=1e-07;, score=-0.729 total time=   0.0s\n",
      "[CV 3/4] END alpha=27, selection=cyclic, tol=0.01;, score=-0.180 total time=   0.0s\n",
      "[CV 3/4] END alpha=27, selection=random, tol=0.01;, score=-0.184 total time=   0.0s\n",
      "[CV 1/4] END alpha=28, selection=random, tol=1e-07;, score=-0.517 total time=   0.0s\n",
      "[CV 3/4] END alpha=25, selection=random, tol=0.01;, score=-0.142 total time=   0.0s\n",
      "[CV 2/4] END alpha=24, selection=random, tol=0.01;, score=0.028 total time=   0.0s\n",
      "[CV 3/4] END alpha=25, selection=cyclic, tol=0.01;, score=-0.141 total time=   0.0s\n",
      "[CV 1/4] END alpha=30, selection=random, tol=1e-05;, score=-0.624 total time=   0.0s\n",
      "[CV 1/4] END alpha=31, selection=cyclic, tol=1e-07;, score=-0.680 total time=   0.0s\n",
      "[CV 1/4] END alpha=26, selection=random, tol=0.01;, score=-0.304 total time=   0.0s\n",
      "[CV 1/4] END alpha=29, selection=random, tol=0.01;, score=-0.564 total time=   0.0s\n",
      "[CV 4/4] END alpha=29, selection=cyclic, tol=1e-07;, score=-0.979 total time=   0.0s\n",
      "[CV 4/4] END alpha=27, selection=cyclic, tol=0.01;, score=-0.811 total time=   0.0s\n",
      "[CV 2/4] END alpha=28, selection=random, tol=1e-07;, score=-0.021 total time=   0.0s\n",
      "[CV 3/4] END alpha=24, selection=random, tol=0.01;, score=-0.115 total time=   0.0s[CV 4/4] END alpha=25, selection=random, tol=0.01;, score=-0.649 total time=   0.0s\n",
      "\n",
      "[CV 1/4] END alpha=33, selection=cyclic, tol=1e-05;, score=-0.716 total time=   0.0s[CV 1/4] END alpha=32, selection=cyclic, tol=0.01;, score=-0.708 total time=   0.0s\n",
      "\n",
      "[CV 4/4] END alpha=27, selection=random, tol=0.01;, score=-0.738 total time=   0.0s\n",
      "[CV 4/4] END alpha=25, selection=cyclic, tol=0.01;, score=-0.655 total time=   0.0s\n",
      "[CV 2/4] END alpha=31, selection=cyclic, tol=1e-07;, score=-0.029 total time=   0.0s\n",
      "[CV 2/4] END alpha=26, selection=random, tol=0.01;, score=-0.016 total time=   0.0s\n",
      "[CV 1/4] END alpha=34, selection=cyclic, tol=0.01;, score=-0.724 total time=   0.0s\n",
      "[CV 2/4] END alpha=29, selection=random, tol=0.01;, score=-0.022 total time=   0.0s\n",
      "[CV 1/4] END alpha=27, selection=random, tol=1e-05;, score=-0.467 total time=   0.0s\n",
      "[CV 1/4] END alpha=29, selection=cyclic, tol=0.01;, score=-0.569 total time=   0.0s\n",
      "[CV 4/4] END alpha=24, selection=random, tol=0.01;, score=-0.560 total time=   0.0s\n",
      "[CV 3/4] END alpha=28, selection=random, tol=1e-07;, score=-0.181 total time=   0.0s\n",
      "[CV 1/4] END alpha=26, selection=cyclic, tol=1e-05;, score=-0.418 total time=   0.0s\n",
      "[CV 2/4] END alpha=34, selection=cyclic, tol=0.01;, score=-0.038 total time=   0.0s\n",
      "[CV 3/4] END alpha=31, selection=random, tol=1e-07;, score=-0.184 total time=   0.0s\n",
      "[CV 1/4] END alpha=25, selection=random, tol=1e-05;, score=-0.371 total time=   0.0s\n",
      "[CV 3/4] END alpha=26, selection=random, tol=0.01;, score=-0.167 total time=   0.0s\n",
      "[CV 1/4] END alpha=28, selection=cyclic, tol=1e-05;, score=-0.517 total time=   0.0s\n",
      "[CV 1/4] END alpha=33, selection=random, tol=1e-07;, score=-0.716 total time=   0.0s\n",
      "[CV 3/4] END alpha=31, selection=cyclic, tol=1e-07;, score=-0.184 total time=   0.0s\n",
      "[CV 2/4] END alpha=27, selection=random, tol=1e-05;, score=-0.019 total time=   0.0s\n",
      "[CV 3/4] END alpha=29, selection=random, tol=0.01;, score=-0.182 total time=   0.0s\n",
      "[CV 2/4] END alpha=30, selection=random, tol=1e-05;, score=-0.026 total time=   0.0s\n",
      "[CV 4/4] END alpha=28, selection=random, tol=1e-07;, score=-0.892 total time=   0.0s\n",
      "[CV 3/4] END alpha=34, selection=cyclic, tol=0.01;, score=-0.182 total time=   0.0s\n",
      "[CV 2/4] END alpha=29, selection=cyclic, tol=0.01;, score=-0.024 total time=   0.0s\n",
      "[CV 4/4] END alpha=31, selection=random, tol=1e-07;, score=-1.161 total time=   0.0s\n",
      "[CV 1/4] END alpha=35, selection=cyclic, tol=1e-05;, score=-0.732 total time=   0.0s[CV 2/4] END alpha=26, selection=cyclic, tol=1e-05;, score=-0.016 total time=   0.0s\n",
      "\n",
      "[CV 2/4] END alpha=32, selection=cyclic, tol=0.01;, score=-0.032 total time=   0.0s\n",
      "[CV 4/4] END alpha=26, selection=random, tol=0.01;, score=-0.726 total time=   0.0s\n",
      "[CV 2/4] END alpha=25, selection=random, tol=1e-05;, score=-0.001 total time=   0.0s\n",
      "[CV 2/4] END alpha=28, selection=cyclic, tol=1e-05;, score=-0.021 total time=   0.0s\n",
      "[CV 3/4] END alpha=27, selection=random, tol=1e-05;, score=-0.180 total time=   0.0s\n",
      "[CV 4/4] END alpha=31, selection=cyclic, tol=1e-07;, score=-1.161 total time=   0.0s\n",
      "[CV 2/4] END alpha=33, selection=cyclic, tol=1e-05;, score=-0.035 total time=   0.0s\n",
      "[CV 1/4] END alpha=28, selection=random, tol=0.01;, score=-0.514 total time=   0.0s\n",
      "[CV 4/4] END alpha=29, selection=random, tol=0.01;, score=-0.987 total time=   0.0s\n",
      "[CV 4/4] END alpha=34, selection=cyclic, tol=0.01;, score=-1.285 total time=   0.0s\n",
      "[CV 2/4] END alpha=35, selection=cyclic, tol=1e-05;, score=-0.041 total time=   0.0s\n",
      "[CV 3/4] END alpha=25, selection=random, tol=1e-05;, score=-0.141 total time=   0.0s\n",
      "[CV 3/4] END alpha=26, selection=cyclic, tol=1e-05;, score=-0.167 total time=   0.0s\n",
      "[CV 3/4] END alpha=29, selection=cyclic, tol=0.01;, score=-0.182 total time=   0.0s\n",
      "[CV 2/4] END alpha=33, selection=random, tol=1e-07;, score=-0.035 total time=   0.0s\n",
      "[CV 4/4] END alpha=27, selection=random, tol=1e-05;, score=-0.809 total time=   0.0s\n",
      "[CV 1/4] END alpha=35, selection=random, tol=1e-07;, score=-0.732 total time=   0.0s\n",
      "[CV 3/4] END alpha=28, selection=cyclic, tol=1e-05;, score=-0.181 total time=   0.0s\n",
      "[CV 1/4] END alpha=31, selection=cyclic, tol=0.01;, score=-0.680 total time=   0.0s\n",
      "[CV 2/4] END alpha=28, selection=random, tol=0.01;, score=-0.020 total time=   0.0s\n",
      "[CV 1/4] END alpha=34, selection=random, tol=1e-05;, score=-0.724 total time=   0.0s\n",
      "[CV 3/4] END alpha=30, selection=random, tol=1e-05;, score=-0.183 total time=   0.0s\n",
      "[CV 3/4] END alpha=35, selection=cyclic, tol=1e-05;, score=-0.181 total time=   0.0s\n",
      "[CV 4/4] END alpha=25, selection=random, tol=1e-05;, score=-0.653 total time=   0.0s[CV 1/4] END alpha=30, selection=cyclic, tol=1e-05;, score=-0.624 total time=   0.0s\n",
      "\n",
      "[CV 4/4] END alpha=26, selection=cyclic, tol=1e-05;, score=-0.729 total time=   0.0s\n",
      "[CV 4/4] END alpha=28, selection=cyclic, tol=1e-05;, score=-0.892 total time=   0.0s\n",
      "[CV 2/4] END alpha=34, selection=random, tol=1e-05;, score=-0.038 total time=   0.0s\n",
      "[CV 2/4] END alpha=31, selection=cyclic, tol=0.01;, score=-0.029 total time=   0.0s\n",
      "[CV 3/4] END alpha=28, selection=random, tol=0.01;, score=-0.182 total time=   0.0s\n",
      "[CV 2/4] END alpha=35, selection=random, tol=1e-07;, score=-0.041 total time=   0.0s\n",
      "[CV 1/4] END alpha=36, selection=cyclic, tol=0.01;, score=-0.741 total time=   0.0s\n",
      "[CV 4/4] END alpha=29, selection=cyclic, tol=0.01;, score=-0.980 total time=   0.0s\n",
      "[CV 3/4] END alpha=32, selection=cyclic, tol=0.01;, score=-0.185 total time=   0.0s\n",
      "[CV 3/4] END alpha=33, selection=cyclic, tol=1e-05;, score=-0.184 total time=   0.0s\n",
      "[CV 4/4] END alpha=35, selection=cyclic, tol=1e-05;, score=-1.314 total time=   0.0s\n",
      "[CV 1/4] END alpha=31, selection=random, tol=0.01;, score=-0.666 total time=   0.0s\n",
      "[CV 3/4] END alpha=34, selection=random, tol=1e-05;, score=-0.182 total time=   0.0s\n",
      "[CV 2/4] END alpha=30, selection=cyclic, tol=1e-05;, score=-0.026 total time=   0.0s\n",
      "[CV 1/4] END alpha=37, selection=cyclic, tol=1e-05;, score=-0.749 total time=   0.0s\n",
      "[CV 1/4] END alpha=28, selection=cyclic, tol=1e-07;, score=-0.517 total time=   0.0s\n",
      "[CV 3/4] END alpha=31, selection=cyclic, tol=0.01;, score=-0.184 total time=   0.0s\n",
      "[CV 4/4] END alpha=28, selection=random, tol=0.01;, score=-0.893 total time=   0.0s\n",
      "[CV 3/4] END alpha=33, selection=random, tol=1e-07;, score=-0.184 total time=   0.0s\n",
      "[CV 1/4] END alpha=26, selection=cyclic, tol=1e-07;, score=-0.418 total time=   0.0s\n",
      "[CV 2/4] END alpha=36, selection=cyclic, tol=0.01;, score=-0.044 total time=   0.0s\n",
      "[CV 3/4] END alpha=35, selection=random, tol=1e-07;, score=-0.181 total time=   0.0s\n",
      "[CV 4/4] END alpha=34, selection=random, tol=1e-05;, score=-1.285 total time=   0.0s\n",
      "[CV 1/4] END alpha=35, selection=cyclic, tol=1e-07;, score=-0.732 total time=   0.0s\n",
      "[CV 1/4] END alpha=29, selection=random, tol=1e-05;, score=-0.570 total time=   0.0s\n",
      "[CV 2/4] END alpha=31, selection=random, tol=0.01;, score=-0.029 total time=   0.0s\n",
      "[CV 3/4] END alpha=30, selection=cyclic, tol=1e-05;, score=-0.183 total time=   0.0s\n",
      "[CV 4/4] END alpha=31, selection=cyclic, tol=0.01;, score=-1.162 total time=   0.0s\n",
      "[CV 2/4] END alpha=28, selection=cyclic, tol=1e-07;, score=-0.021 total time=   0.0s\n",
      "[CV 4/4] END alpha=32, selection=cyclic, tol=0.01;, score=-1.230 total time=   0.0s\n",
      "[CV 2/4] END alpha=37, selection=cyclic, tol=1e-05;, score=-0.047 total time=   0.0s\n",
      "[CV 3/4] END alpha=36, selection=cyclic, tol=0.01;, score=-0.179 total time=   0.0s\n",
      "[CV 1/4] END alpha=34, selection=random, tol=1e-07;, score=-0.724 total time=   0.0s\n",
      "[CV 4/4] END alpha=35, selection=random, tol=1e-07;, score=-1.314 total time=   0.0s\n",
      "[CV 2/4] END alpha=26, selection=cyclic, tol=1e-07;, score=-0.016 total time=   0.0s\n",
      "[CV 4/4] END alpha=33, selection=cyclic, tol=1e-05;, score=-1.257 total time=   0.0s\n",
      "[CV 1/4] END alpha=37, selection=random, tol=1e-07;, score=-0.749 total time=   0.0s\n",
      "[CV 2/4] END alpha=35, selection=cyclic, tol=1e-07;, score=-0.041 total time=   0.0s\n",
      "[CV 2/4] END alpha=29, selection=random, tol=1e-05;, score=-0.024 total time=   0.0s\n",
      "[CV 2/4] END alpha=34, selection=random, tol=1e-07;, score=-0.038 total time=   0.0s\n",
      "[CV 3/4] END alpha=28, selection=cyclic, tol=1e-07;, score=-0.181 total time=   0.0s\n",
      "[CV 4/4] END alpha=30, selection=cyclic, tol=1e-05;, score=-1.069 total time=   0.0s\n",
      "[CV 1/4] END alpha=31, selection=random, tol=1e-05;, score=-0.680 total time=   0.0s\n",
      "[CV 3/4] END alpha=37, selection=cyclic, tol=1e-05;, score=-0.178 total time=   0.0s\n",
      "[CV 1/4] END alpha=32, selection=random, tol=1e-05;, score=-0.708 total time=   0.0s\n",
      "[CV 4/4] END alpha=36, selection=cyclic, tol=0.01;, score=-1.343 total time=   0.0s\n",
      "[CV 1/4] END alpha=35, selection=random, tol=0.01;, score=-0.723 total time=   0.0s\n",
      "[CV 4/4] END alpha=33, selection=random, tol=1e-07;, score=-1.257 total time=   0.0s\n",
      "[CV 3/4] END alpha=26, selection=cyclic, tol=1e-07;, score=-0.167 total time=   0.0s[CV 4/4] END alpha=30, selection=random, tol=1e-05;, score=-1.069 total time=   0.0s\n",
      "\n",
      "[CV 3/4] END alpha=35, selection=cyclic, tol=1e-07;, score=-0.181 total time=   0.0s\n",
      "[CV 3/4] END alpha=34, selection=random, tol=1e-07;, score=-0.182 total time=   0.0s\n",
      "[CV 3/4] END alpha=29, selection=random, tol=1e-05;, score=-0.182 total time=   0.0s\n",
      "[CV 1/4] END alpha=30, selection=cyclic, tol=1e-07;, score=-0.624 total time=   0.0s\n",
      "[CV 2/4] END alpha=31, selection=random, tol=1e-05;, score=-0.029 total time=   0.0s\n",
      "[CV 4/4] END alpha=28, selection=cyclic, tol=1e-07;, score=-0.892 total time=   0.0s\n",
      "[CV 4/4] END alpha=37, selection=cyclic, tol=1e-05;, score=-1.372 total time=   0.0s\n",
      "[CV 3/4] END alpha=31, selection=random, tol=0.01;, score=-0.183 total time=   0.0s\n",
      "[CV 2/4] END alpha=32, selection=random, tol=1e-05;, score=-0.032 total time=   0.0s[CV 2/4] END alpha=35, selection=random, tol=0.01;, score=-0.041 total time=   0.0s\n",
      "[CV 1/4] END alpha=36, selection=random, tol=1e-05;, score=-0.741 total time=   0.0s\n",
      "[CV 2/4] END alpha=37, selection=random, tol=1e-07;, score=-0.047 total time=   0.0s\n",
      "[CV 4/4] END alpha=26, selection=cyclic, tol=1e-07;, score=-0.729 total time=   0.0s\n",
      "\n",
      "[CV 4/4] END alpha=34, selection=random, tol=1e-07;, score=-1.285 total time=   0.0s\n",
      "[CV 4/4] END alpha=35, selection=cyclic, tol=1e-07;, score=-1.314 total time=   0.0s\n",
      "[CV 1/4] END alpha=37, selection=cyclic, tol=1e-07;, score=-0.749 total time=   0.0s\n",
      "[CV 3/4] END alpha=31, selection=random, tol=1e-05;, score=-0.184 total time=   0.0s\n",
      "[CV 1/4] END alpha=33, selection=cyclic, tol=1e-07;, score=-0.716 total time=   0.0s\n",
      "[CV 4/4] END alpha=29, selection=random, tol=1e-05;, score=-0.979 total time=   0.0s\n",
      "[CV 3/4] END alpha=35, selection=random, tol=0.01;, score=-0.181 total time=   0.0s\n",
      "[CV 1/4] END alpha=30, selection=random, tol=1e-07;, score=-0.624 total time=   0.0s\n",
      "[CV 2/4] END alpha=36, selection=random, tol=1e-05;, score=-0.044 total time=   0.0s\n",
      "[CV 2/4] END alpha=30, selection=cyclic, tol=1e-07;, score=-0.026 total time=   0.0s\n",
      "[CV 3/4] END alpha=32, selection=random, tol=1e-05;, score=-0.185 total time=   0.0s\n",
      "[CV 1/4] END alpha=34, selection=random, tol=0.01;, score=-0.724 total time=   0.0s\n",
      "[CV 1/4] END alpha=38, selection=cyclic, tol=0.01;, score=-0.758 total time=   0.0s\n",
      "[CV 4/4] END alpha=31, selection=random, tol=1e-05;, score=-1.161 total time=   0.0s\n",
      "[CV 2/4] END alpha=37, selection=cyclic, tol=1e-07;, score=-0.047 total time=   0.0s\n",
      "[CV 4/4] END alpha=35, selection=random, tol=0.01;, score=-1.275 total time=   0.0s\n",
      "[CV 2/4] END alpha=30, selection=random, tol=1e-07;, score=-0.026 total time=   0.0s\n",
      "[CV 3/4] END alpha=37, selection=random, tol=1e-07;, score=-0.178 total time=   0.0s\n",
      "[CV 3/4] END alpha=36, selection=random, tol=1e-05;, score=-0.179 total time=   0.0s\n",
      "[CV 1/4] END alpha=33, selection=random, tol=0.01;, score=-0.722 total time=   0.0s\n",
      "[CV 1/4] END alpha=35, selection=cyclic, tol=0.01;, score=-0.733 total time=   0.0s\n",
      "[CV 4/4] END alpha=31, selection=random, tol=0.01;, score=-1.161 total time=   0.0s\n",
      "[CV 4/4] END alpha=32, selection=random, tol=1e-05;, score=-1.230 total time=   0.0s\n",
      "[CV 1/4] END alpha=39, selection=cyclic, tol=1e-05;, score=-0.766 total time=   0.0s\n",
      "[CV 1/4] END alpha=36, selection=cyclic, tol=1e-05;, score=-0.741 total time=   0.0s\n",
      "[CV 3/4] END alpha=37, selection=cyclic, tol=1e-07;, score=-0.178 total time=   0.0s\n",
      "[CV 4/4] END alpha=36, selection=random, tol=1e-05;, score=-1.343 total time=   0.0s\n",
      "[CV 2/4] END alpha=38, selection=cyclic, tol=0.01;, score=-0.050 total time=   0.0s\n",
      "[CV 2/4] END alpha=35, selection=cyclic, tol=0.01;, score=-0.041 total time=   0.0s\n",
      "[CV 3/4] END alpha=30, selection=random, tol=1e-07;, score=-0.183 total time=   0.0s\n",
      "[CV 2/4] END alpha=33, selection=random, tol=0.01;, score=-0.033 total time=   0.0s\n",
      "[CV 2/4] END alpha=33, selection=cyclic, tol=1e-07;, score=-0.035 total time=   0.0s\n",
      "[CV 1/4] END alpha=32, selection=random, tol=1e-07;, score=-0.708 total time=   0.0s\n",
      "[CV 2/4] END alpha=36, selection=cyclic, tol=1e-05;, score=-0.044 total time=   0.0s\n",
      "[CV 3/4] END alpha=30, selection=cyclic, tol=1e-07;, score=-0.183 total time=   0.0s[CV 4/4] END alpha=37, selection=cyclic, tol=1e-07;, score=-1.372 total time=   0.0s\n",
      "\n",
      "[CV 3/4] END alpha=35, selection=cyclic, tol=0.01;, score=-0.181 total time=   0.0s\n",
      "[CV 1/4] END alpha=32, selection=cyclic, tol=1e-05;, score=-0.708 total time=   0.0s\n",
      "[CV 1/4] END alpha=39, selection=random, tol=1e-07;, score=-0.766 total time=   0.0s\n",
      "[CV 3/4] END alpha=38, selection=cyclic, tol=0.01;, score=-0.177 total time=   0.0s\n",
      "[CV 4/4] END alpha=30, selection=random, tol=1e-07;, score=-1.069 total time=   0.0s\n",
      "[CV 1/4] END alpha=40, selection=cyclic, tol=0.01;, score=-0.775 total time=   0.0s\n",
      "[CV 3/4] END alpha=36, selection=cyclic, tol=1e-05;, score=-0.179 total time=   0.0s\n",
      "[CV 2/4] END alpha=39, selection=cyclic, tol=1e-05;, score=-0.053 total time=   0.0s\n",
      "[CV 2/4] END alpha=34, selection=random, tol=0.01;, score=-0.036 total time=   0.0s\n",
      "[CV 1/4] END alpha=36, selection=random, tol=1e-07;, score=-0.741 total time=   0.0s\n",
      "[CV 3/4] END alpha=33, selection=cyclic, tol=1e-07;, score=-0.184 total time=   0.0s[CV 1/4] END alpha=37, selection=cyclic, tol=0.01;, score=-0.749 total time=   0.0s\n",
      "[CV 4/4] END alpha=35, selection=cyclic, tol=0.01;, score=-1.314 total time=   0.0s\n",
      "\n",
      "[CV 2/4] END alpha=32, selection=cyclic, tol=1e-05;, score=-0.032 total time=   0.0s\n",
      "[CV 4/4] END alpha=37, selection=random, tol=1e-07;, score=-1.372 total time=   0.0s\n",
      "[CV 4/4] END alpha=30, selection=cyclic, tol=1e-07;, score=-1.069 total time=   0.0s\n",
      "[CV 1/4] END alpha=30, selection=random, tol=0.01;, score=-0.621 total time=   0.0s\n",
      "[CV 4/4] END alpha=38, selection=cyclic, tol=0.01;, score=-1.402 total time=   0.0s\n",
      "[CV 2/4] END alpha=32, selection=random, tol=1e-07;, score=-0.032 total time=   0.0s\n",
      "[CV 4/4] END alpha=36, selection=cyclic, tol=1e-05;, score=-1.343 total time=   0.0s\n",
      "[CV 3/4] END alpha=34, selection=random, tol=0.01;, score=-0.178 total time=   0.0s\n",
      "[CV 2/4] END alpha=37, selection=cyclic, tol=0.01;, score=-0.047 total time=   0.0s\n",
      "[CV 3/4] END alpha=39, selection=cyclic, tol=1e-05;, score=-0.175 total time=   0.0s\n",
      "[CV 1/4] END alpha=35, selection=random, tol=1e-05;, score=-0.732 total time=   0.0s\n",
      "[CV 3/4] END alpha=32, selection=cyclic, tol=1e-05;, score=-0.185 total time=   0.0s\n",
      "[CV 1/4] END alpha=36, selection=cyclic, tol=1e-07;, score=-0.741 total time=   0.0s\n",
      "[CV 2/4] END alpha=40, selection=cyclic, tol=0.01;, score=-0.053 total time=   0.0s\n",
      "[CV 4/4] END alpha=34, selection=random, tol=0.01;, score=-1.285 total time=   0.0s\n",
      "[CV 2/4] END alpha=30, selection=random, tol=0.01;, score=-0.026 total time=   0.0s\n",
      "[CV 1/4] END alpha=38, selection=random, tol=1e-05;, score=-0.758 total time=   0.0s\n",
      "[CV 3/4] END alpha=32, selection=random, tol=1e-07;, score=-0.185 total time=   0.0s\n",
      "[CV 3/4] END alpha=33, selection=random, tol=0.01;, score=-0.184 total time=   0.0s\n",
      "[CV 2/4] END alpha=36, selection=random, tol=1e-07;, score=-0.044 total time=   0.0s\n",
      "[CV 3/4] END alpha=37, selection=cyclic, tol=0.01;, score=-0.178 total time=   0.0s\n",
      "[CV 1/4] END alpha=37, selection=random, tol=0.01;, score=-0.737 total time=   0.0s\n",
      "[CV 4/4] END alpha=39, selection=cyclic, tol=1e-05;, score=-1.433 total time=   0.0s\n",
      "[CV 4/4] END alpha=32, selection=cyclic, tol=1e-05;, score=-1.230 total time=   0.0s\n",
      "[CV 1/4] END alpha=41, selection=random, tol=1e-07;, score=-0.784 total time=   0.0s\n",
      "[CV 2/4] END alpha=36, selection=cyclic, tol=1e-07;, score=-0.044 total time=   0.0s[CV 2/4] END alpha=35, selection=random, tol=1e-05;, score=-0.041 total time=   0.0s\n",
      "\n",
      "[CV 2/4] END alpha=39, selection=random, tol=1e-07;, score=-0.053 total time=   0.0s\n",
      "[CV 4/4] END alpha=33, selection=cyclic, tol=1e-07;, score=-1.257 total time=   0.0s\n",
      "[CV 2/4] END alpha=38, selection=random, tol=1e-05;, score=-0.050 total time=   0.0s\n",
      "[CV 3/4] END alpha=30, selection=random, tol=0.01;, score=-0.170 total time=   0.0s\n",
      "[CV 4/4] END alpha=32, selection=random, tol=1e-07;, score=-1.230 total time=   0.0s\n",
      "[CV 3/4] END alpha=40, selection=cyclic, tol=0.01;, score=-0.174 total time=   0.0s[CV 4/4] END alpha=37, selection=cyclic, tol=0.01;, score=-1.372 total time=   0.0s\n",
      "\n",
      "[CV 1/4] END alpha=41, selection=cyclic, tol=1e-05;, score=-0.784 total time=   0.0s\n",
      "[CV 1/4] END alpha=39, selection=cyclic, tol=1e-07;, score=-0.766 total time=   0.0s\n",
      "[CV 1/4] END alpha=32, selection=cyclic, tol=1e-07;, score=-0.708 total time=   0.0s\n",
      "[CV 3/4] END alpha=36, selection=cyclic, tol=1e-07;, score=-0.179 total time=   0.0s\n",
      "[CV 2/4] END alpha=41, selection=random, tol=1e-07;, score=-0.054 total time=   0.0s\n",
      "[CV 4/4] END alpha=33, selection=random, tol=0.01;, score=-1.259 total time=   0.0s\n",
      "[CV 3/4] END alpha=35, selection=random, tol=1e-05;, score=-0.181 total time=   0.0s\n",
      "[CV 3/4] END alpha=38, selection=random, tol=1e-05;, score=-0.177 total time=   0.0s\n",
      "[CV 4/4] END alpha=30, selection=random, tol=0.01;, score=-1.071 total time=   0.0s\n",
      "[CV 1/4] END alpha=32, selection=random, tol=0.01;, score=-0.708 total time=   0.0s\n",
      "[CV 3/4] END alpha=36, selection=random, tol=1e-07;, score=-0.179 total time=   0.0s\n",
      "[CV 4/4] END alpha=36, selection=cyclic, tol=1e-07;, score=-1.343 total time=   0.0s\n",
      "[CV 3/4] END alpha=41, selection=random, tol=1e-07;, score=-0.173 total time=   0.0s\n",
      "[CV 2/4] END alpha=32, selection=cyclic, tol=1e-07;, score=-0.032 total time=   0.0s\n",
      "[CV 1/4] END alpha=37, selection=random, tol=1e-05;, score=-0.749 total time=   0.0s\n",
      "[CV 2/4] END alpha=39, selection=cyclic, tol=1e-07;, score=-0.053 total time=   0.0s\n",
      "[CV 3/4] END alpha=39, selection=random, tol=1e-07;, score=-0.175 total time=   0.0s\n",
      "[CV 4/4] END alpha=35, selection=random, tol=1e-05;, score=-1.314 total time=   0.0s\n",
      "[CV 2/4] END alpha=41, selection=cyclic, tol=1e-05;, score=-0.054 total time=   0.0s\n",
      "[CV 4/4] END alpha=38, selection=random, tol=1e-05;, score=-1.402 total time=   0.0s\n",
      "[CV 1/4] END alpha=33, selection=cyclic, tol=0.01;, score=-0.716 total time=   0.0s\n",
      "[CV 2/4] END alpha=37, selection=random, tol=0.01;, score=-0.046 total time=   0.0s\n",
      "[CV 4/4] END alpha=36, selection=random, tol=1e-07;, score=-1.343 total time=   0.0s\n",
      "[CV 4/4] END alpha=41, selection=random, tol=1e-07;, score=-1.496 total time=   0.0s\n",
      "[CV 3/4] END alpha=32, selection=cyclic, tol=1e-07;, score=-0.185 total time=   0.0s\n",
      "[CV 3/4] END alpha=39, selection=cyclic, tol=1e-07;, score=-0.175 total time=   0.0s\n",
      "[CV 2/4] END alpha=37, selection=random, tol=1e-05;, score=-0.047 total time=   0.0s\n",
      "[CV 1/4] END alpha=42, selection=cyclic, tol=0.01;, score=-0.785 total time=   0.0s\n",
      "[CV 4/4] END alpha=40, selection=cyclic, tol=0.01;, score=-1.464 total time=   0.0s\n",
      "[CV 2/4] END alpha=32, selection=random, tol=0.01;, score=-0.034 total time=   0.0s\n",
      "[CV 1/4] END alpha=38, selection=random, tol=1e-07;, score=-0.758 total time=   0.0s\n",
      "[CV 1/4] END alpha=43, selection=cyclic, tol=1e-05;, score=-0.783 total time=   0.0s\n",
      "[CV 4/4] END alpha=39, selection=random, tol=1e-07;, score=-1.433 total time=   0.0s\n",
      "[CV 3/4] END alpha=41, selection=cyclic, tol=1e-05;, score=-0.173 total time=   0.0s\n",
      "[CV 1/4] END alpha=34, selection=cyclic, tol=1e-05;, score=-0.724 total time=   0.0s\n",
      "[CV 4/4] END alpha=32, selection=cyclic, tol=1e-07;, score=-1.230 total time=   0.0s\n",
      "[CV 1/4] END alpha=41, selection=random, tol=0.01;, score=-0.780 total time=   0.0s\n",
      "[CV 4/4] END alpha=39, selection=cyclic, tol=1e-07;, score=-1.433 total time=   0.0s\n",
      "[CV 3/4] END alpha=37, selection=random, tol=0.01;, score=-0.177 total time=   0.0s\n",
      "[CV 1/4] END alpha=43, selection=random, tol=1e-07;, score=-0.783 total time=   0.0s\n",
      "[CV 2/4] END alpha=42, selection=cyclic, tol=0.01;, score=-0.054 total time=   0.0s\n",
      "[CV 3/4] END alpha=32, selection=random, tol=0.01;, score=-0.185 total time=   0.0s\n",
      "[CV 1/4] END alpha=40, selection=random, tol=1e-05;, score=-0.775 total time=   0.0s\n",
      "[CV 2/4] END alpha=43, selection=cyclic, tol=1e-05;, score=-0.054 total time=   0.0s\n",
      "[CV 2/4] END alpha=38, selection=random, tol=1e-07;, score=-0.050 total time=   0.0s[CV 3/4] END alpha=37, selection=random, tol=1e-05;, score=-0.178 total time=   0.0s\n",
      "\n",
      "[CV 2/4] END alpha=33, selection=cyclic, tol=0.01;, score=-0.035 total time=   0.0s\n",
      "[CV 4/4] END alpha=41, selection=cyclic, tol=1e-05;, score=-1.496 total time=   0.0s\n",
      "[CV 2/4] END alpha=41, selection=random, tol=0.01;, score=-0.053 total time=   0.0s\n",
      "[CV 1/4] END alpha=39, selection=cyclic, tol=0.01;, score=-0.767 total time=   0.0s\n",
      "[CV 1/4] END alpha=44, selection=cyclic, tol=0.01;, score=-0.782 total time=   0.0s\n",
      "[CV 3/4] END alpha=43, selection=cyclic, tol=1e-05;, score=-0.170 total time=   0.0s\n",
      "[CV 3/4] END alpha=42, selection=cyclic, tol=0.01;, score=-0.171 total time=   0.0s\n",
      "[CV 4/4] END alpha=37, selection=random, tol=0.01;, score=-1.355 total time=   0.0s\n",
      "[CV 2/4] END alpha=40, selection=random, tol=1e-05;, score=-0.053 total time=   0.0s\n",
      "[CV 4/4] END alpha=32, selection=random, tol=0.01;, score=-1.255 total time=   0.0s\n",
      "[CV 4/4] END alpha=37, selection=random, tol=1e-05;, score=-1.372 total time=   0.0s\n",
      "[CV 2/4] END alpha=34, selection=cyclic, tol=1e-05;, score=-0.038 total time=   0.0s\n",
      "[CV 3/4] END alpha=41, selection=random, tol=0.01;, score=-0.173 total time=   0.0s\n",
      "[CV 1/4] END alpha=39, selection=random, tol=0.01;, score=-0.690 total time=   0.0s\n",
      "[CV 2/4] END alpha=43, selection=random, tol=1e-07;, score=-0.054 total time=   0.0s\n",
      "[CV 2/4] END alpha=44, selection=cyclic, tol=0.01;, score=-0.055 total time=   0.0s\n",
      "[CV 2/4] END alpha=39, selection=cyclic, tol=0.01;, score=-0.053 total time=   0.0s\n",
      "[CV 1/4] END alpha=36, selection=random, tol=0.01;, score=-0.704 total time=   0.0s\n",
      "[CV 4/4] END alpha=43, selection=cyclic, tol=1e-05;, score=-1.562 total time=   0.0s\n",
      "[CV 3/4] END alpha=40, selection=random, tol=1e-05;, score=-0.174 total time=   0.0s\n",
      "[CV 4/4] END alpha=42, selection=cyclic, tol=0.01;, score=-1.528 total time=   0.0s\n",
      "[CV 3/4] END alpha=38, selection=random, tol=1e-07;, score=-0.177 total time=   0.0s\n",
      "[CV 1/4] END alpha=41, selection=cyclic, tol=1e-07;, score=-0.784 total time=   0.0s\n",
      "[CV 1/4] END alpha=38, selection=cyclic, tol=1e-05;, score=-0.758 total time=   0.0s\n",
      "[CV 4/4] END alpha=41, selection=random, tol=0.01;, score=-1.481 total time=   0.0s\n",
      "[CV 3/4] END alpha=34, selection=cyclic, tol=1e-05;, score=-0.182 total time=   0.0s\n",
      "[CV 2/4] END alpha=39, selection=random, tol=0.01;, score=-0.053 total time=   0.0s\n",
      "[CV 3/4] END alpha=44, selection=cyclic, tol=0.01;, score=-0.169 total time=   0.0s\n",
      "[CV 3/4] END alpha=43, selection=random, tol=1e-07;, score=-0.170 total time=   0.0s\n",
      "[CV 3/4] END alpha=39, selection=cyclic, tol=0.01;, score=-0.175 total time=   0.0s\n",
      "[CV 1/4] END alpha=43, selection=cyclic, tol=1e-07;, score=-0.783 total time=   0.0s\n",
      "[CV 2/4] END alpha=36, selection=random, tol=0.01;, score=-0.044 total time=   0.0s\n",
      "[CV 3/4] END alpha=33, selection=cyclic, tol=0.01;, score=-0.184 total time=   0.0s\n",
      "[CV 4/4] END alpha=40, selection=random, tol=1e-05;, score=-1.464 total time=   0.0s\n",
      "[CV 1/4] END alpha=42, selection=random, tol=1e-05;, score=-0.784 total time=   0.0s\n",
      "[CV 2/4] END alpha=41, selection=cyclic, tol=1e-07;, score=-0.054 total time=   0.0s\n",
      "[CV 4/4] END alpha=34, selection=cyclic, tol=1e-05;, score=-1.285 total time=   0.0s\n",
      "[CV 1/4] END alpha=42, selection=cyclic, tol=1e-05;, score=-0.784 total time=   0.0s\n",
      "[CV 2/4] END alpha=38, selection=cyclic, tol=1e-05;, score=-0.050 total time=   0.0s\n",
      "[CV 3/4] END alpha=39, selection=random, tol=0.01;, score=-0.175 total time=   0.0s\n",
      "[CV 4/4] END alpha=44, selection=cyclic, tol=0.01;, score=-1.595 total time=   0.0s\n",
      "[CV 2/4] END alpha=43, selection=cyclic, tol=1e-07;, score=-0.054 total time=   0.0s\n",
      "[CV 3/4] END alpha=36, selection=random, tol=0.01;, score=-0.179 total time=   0.0s\n",
      "[CV 4/4] END alpha=39, selection=cyclic, tol=0.01;, score=-1.432 total time=   0.0s\n",
      "[CV 4/4] END alpha=38, selection=random, tol=1e-07;, score=-1.402 total time=   0.0s\n",
      "[CV 4/4] END alpha=33, selection=cyclic, tol=0.01;, score=-1.257 total time=   0.0s\n",
      "[CV 2/4] END alpha=42, selection=random, tol=1e-05;, score=-0.054 total time=   0.0s\n",
      "[CV 1/4] END alpha=40, selection=random, tol=1e-07;, score=-0.775 total time=   0.0s\n",
      "[CV 1/4] END alpha=34, selection=cyclic, tol=1e-07;, score=-0.724 total time=   0.0s\n",
      "[CV 2/4] END alpha=42, selection=cyclic, tol=1e-05;, score=-0.054 total time=   0.0s\n",
      "[CV 4/4] END alpha=39, selection=random, tol=0.01;, score=-1.428 total time=   0.0s\n",
      "[CV 1/4] END alpha=39, selection=random, tol=1e-05;, score=-0.766 total time=   0.0s\n",
      "[CV 1/4] END alpha=44, selection=random, tol=1e-05;, score=-0.781 total time=   0.0s\n",
      "[CV 4/4] END alpha=36, selection=random, tol=0.01;, score=-1.343 total time=   0.0s\n",
      "[CV 3/4] END alpha=38, selection=cyclic, tol=1e-05;, score=-0.177 total time=   0.0s\n",
      "[CV 3/4] END alpha=43, selection=cyclic, tol=1e-07;, score=-0.170 total time=   0.0s\n",
      "[CV 3/4] END alpha=41, selection=cyclic, tol=1e-07;, score=-0.173 total time=   0.0s\n",
      "[CV 1/4] END alpha=33, selection=random, tol=1e-05;, score=-0.716 total time=   0.0s\n",
      "[CV 3/4] END alpha=42, selection=random, tol=1e-05;, score=-0.172 total time=   0.0s\n",
      "[CV 2/4] END alpha=34, selection=cyclic, tol=1e-07;, score=-0.038 total time=   0.0s\n",
      "[CV 2/4] END alpha=44, selection=random, tol=1e-05;, score=-0.055 total time=   0.0s\n",
      "[CV 3/4] END alpha=42, selection=cyclic, tol=1e-05;, score=-0.172 total time=   0.0s[CV 1/4] END alpha=40, selection=cyclic, tol=1e-05;, score=-0.775 total time=   0.0s\n",
      "\n",
      "[CV 2/4] END alpha=39, selection=random, tol=1e-05;, score=-0.053 total time=   0.0s\n",
      "[CV 2/4] END alpha=40, selection=random, tol=1e-07;, score=-0.053 total time=   0.0s\n",
      "[CV 4/4] END alpha=43, selection=cyclic, tol=1e-07;, score=-1.562 total time=   0.0s\n",
      "[CV 4/4] END alpha=38, selection=cyclic, tol=1e-05;, score=-1.402 total time=   0.0s\n",
      "[CV 4/4] END alpha=43, selection=random, tol=1e-07;, score=-1.562 total time=   0.0s\n",
      "[CV 2/4] END alpha=33, selection=random, tol=1e-05;, score=-0.035 total time=   0.0s\n",
      "[CV 4/4] END alpha=42, selection=random, tol=1e-05;, score=-1.528 total time=   0.0s\n",
      "[CV 3/4] END alpha=39, selection=random, tol=1e-05;, score=-0.175 total time=   0.0s\n",
      "[CV 4/4] END alpha=42, selection=cyclic, tol=1e-05;, score=-1.529 total time=   0.0s[CV 2/4] END alpha=40, selection=cyclic, tol=1e-05;, score=-0.053 total time=   0.0s[CV 1/4] END alpha=38, selection=random, tol=0.01;, score=-0.757 total time=   0.0s\n",
      "\n",
      "\n",
      "[CV 4/4] END alpha=41, selection=cyclic, tol=1e-07;, score=-1.496 total time=   0.0s\n",
      "[CV 3/4] END alpha=40, selection=random, tol=1e-07;, score=-0.174 total time=   0.0s\n",
      "[CV 3/4] END alpha=34, selection=cyclic, tol=1e-07;, score=-0.182 total time=   0.0s\n",
      "[CV 3/4] END alpha=44, selection=random, tol=1e-05;, score=-0.169 total time=   0.0s\n",
      "[CV 1/4] END alpha=43, selection=cyclic, tol=0.01;, score=-0.784 total time=   0.0s\n",
      "[CV 1/4] END alpha=38, selection=cyclic, tol=1e-07;, score=-0.758 total time=   0.0s\n",
      "[CV 1/4] END alpha=43, selection=random, tol=0.01;, score=-0.783 total time=   0.0s[CV 4/4] END alpha=39, selection=random, tol=1e-05;, score=-1.433 total time=   0.0s\n",
      "\n",
      "[CV 1/4] END alpha=42, selection=random, tol=1e-07;, score=-0.784 total time=   0.0s[CV 3/4] END alpha=40, selection=cyclic, tol=1e-05;, score=-0.174 total time=   0.0s[CV 1/4] END alpha=42, selection=cyclic, tol=1e-07;, score=-0.784 total time=   0.0s\n",
      "\n",
      "\n",
      "[CV 3/4] END alpha=33, selection=random, tol=1e-05;, score=-0.184 total time=   0.0s\n",
      "[CV 4/4] END alpha=34, selection=cyclic, tol=1e-07;, score=-1.285 total time=   0.0s\n",
      "[CV 2/4] END alpha=38, selection=random, tol=0.01;, score=-0.050 total time=   0.0s\n",
      "[CV 2/4] END alpha=43, selection=cyclic, tol=0.01;, score=-0.054 total time=   0.0s\n",
      "[CV 4/4] END alpha=40, selection=random, tol=1e-07;, score=-1.464 total time=   0.0s\n",
      "[CV 1/4] END alpha=41, selection=cyclic, tol=0.01;, score=-0.787 total time=   0.0s\n",
      "[CV 2/4] END alpha=42, selection=cyclic, tol=1e-07;, score=-0.054 total time=   0.0s[CV 4/4] END alpha=40, selection=cyclic, tol=1e-05;, score=-1.464 total time=   0.0s\n",
      "\n",
      "[CV 2/4] END alpha=38, selection=cyclic, tol=1e-07;, score=-0.050 total time=   0.0s[CV 2/4] END alpha=42, selection=random, tol=1e-07;, score=-0.054 total time=   0.0s\n",
      "\n",
      "[CV 4/4] END alpha=33, selection=random, tol=1e-05;, score=-1.257 total time=   0.0s\n",
      "[CV 4/4] END alpha=44, selection=random, tol=1e-05;, score=-1.595 total time=   0.0s\n",
      "[CV 2/4] END alpha=43, selection=random, tol=0.01;, score=-0.054 total time=   0.0s\n",
      "[CV 3/4] END alpha=43, selection=cyclic, tol=0.01;, score=-0.170 total time=   0.0s\n",
      "[CV 2/4] END alpha=41, selection=cyclic, tol=0.01;, score=-0.053 total time=   0.0s\n",
      "[CV 1/4] END alpha=46, selection=cyclic, tol=0.01;, score=-0.780 total time=   0.0s\n",
      "[CV 1/4] END alpha=40, selection=cyclic, tol=1e-07;, score=-0.775 total time=   0.0s\n",
      "[CV 3/4] END alpha=42, selection=cyclic, tol=1e-07;, score=-0.172 total time=   0.0s\n",
      "[CV 3/4] END alpha=42, selection=random, tol=1e-07;, score=-0.172 total time=   0.0s\n",
      "[CV 1/4] END alpha=45, selection=cyclic, tol=1e-05;, score=-0.780 total time=   0.0s\n",
      "[CV 3/4] END alpha=38, selection=random, tol=0.01;, score=-0.177 total time=   0.0s\n",
      "[CV 1/4] END alpha=40, selection=random, tol=0.01;, score=-0.775 total time=   0.0s\n",
      "[CV 4/4] END alpha=43, selection=cyclic, tol=0.01;, score=-1.561 total time=   0.0s\n",
      "[CV 2/4] END alpha=40, selection=cyclic, tol=1e-07;, score=-0.053 total time=   0.0s[CV 4/4] END alpha=42, selection=cyclic, tol=1e-07;, score=-1.529 total time=   0.0s\n",
      "\n",
      "[CV 4/4] END alpha=42, selection=random, tol=1e-07;, score=-1.529 total time=   0.0s\n",
      "[CV 2/4] END alpha=46, selection=cyclic, tol=0.01;, score=-0.055 total time=   0.0s\n",
      "[CV 3/4] END alpha=38, selection=cyclic, tol=1e-07;, score=-0.177 total time=   0.0s\n",
      "[CV 3/4] END alpha=41, selection=cyclic, tol=0.01;, score=-0.173 total time=   0.0s\n",
      "[CV 1/4] END alpha=44, selection=random, tol=1e-07;, score=-0.781 total time=   0.0s\n",
      "[CV 1/4] END alpha=47, selection=random, tol=1e-07;, score=-0.777 total time=   0.0s\n",
      "[CV 1/4] END alpha=43, selection=random, tol=1e-05;, score=-0.783 total time=   0.0s\n",
      "[CV 3/4] END alpha=40, selection=cyclic, tol=1e-07;, score=-0.174 total time=   0.0s\n",
      "[CV 2/4] END alpha=40, selection=random, tol=0.01;, score=-0.054 total time=   0.0s\n",
      "[CV 4/4] END alpha=38, selection=random, tol=0.01;, score=-1.330 total time=   0.0s\n",
      "[CV 1/4] END alpha=42, selection=random, tol=0.01;, score=-0.784 total time=   0.0s\n",
      "[CV 3/4] END alpha=43, selection=random, tol=0.01;, score=-0.170 total time=   0.0s\n",
      "[CV 3/4] END alpha=46, selection=cyclic, tol=0.01;, score=-0.167 total time=   0.0s\n",
      "[CV 4/4] END alpha=38, selection=cyclic, tol=1e-07;, score=-1.402 total time=   0.0s\n",
      "[CV 2/4] END alpha=45, selection=cyclic, tol=1e-05;, score=-0.055 total time=   0.0s\n",
      "[CV 2/4] END alpha=43, selection=random, tol=1e-05;, score=-0.054 total time=   0.0s\n",
      "[CV 4/4] END alpha=40, selection=cyclic, tol=1e-07;, score=-1.464 total time=   0.0s\n",
      "[CV 4/4] END alpha=41, selection=cyclic, tol=0.01;, score=-1.496 total time=   0.0s\n",
      "[CV 2/4] END alpha=44, selection=random, tol=1e-07;, score=-0.055 total time=   0.0s\n",
      "[CV 2/4] END alpha=42, selection=random, tol=0.01;, score=-0.050 total time=   0.0s\n",
      "[CV 3/4] END alpha=40, selection=random, tol=0.01;, score=-0.174 total time=   0.0s\n",
      "[CV 4/4] END alpha=43, selection=random, tol=0.01;, score=-1.530 total time=   0.0s\n",
      "[CV 4/4] END alpha=46, selection=cyclic, tol=0.01;, score=-1.664 total time=   0.0s\n",
      "[CV 3/4] END alpha=45, selection=cyclic, tol=1e-05;, score=-0.168 total time=   0.0s\n",
      "[CV 3/4] END alpha=43, selection=random, tol=1e-05;, score=-0.170 total time=   0.0s\n",
      "[CV 1/4] END alpha=41, selection=random, tol=1e-05;, score=-0.784 total time=   0.0s\n",
      "[CV 1/4] END alpha=49, selection=cyclic, tol=1e-05;, score=-0.775 total time=   0.0s[CV 3/4] END alpha=44, selection=random, tol=1e-07;, score=-0.169 total time=   0.0s\n",
      "\n",
      "[CV 3/4] END alpha=42, selection=random, tol=0.01;, score=-0.170 total time=   0.0s\n",
      "[CV 1/4] END alpha=46, selection=random, tol=1e-05;, score=-0.779 total time=   0.0s\n",
      "[CV 1/4] END alpha=44, selection=cyclic, tol=1e-05;, score=-0.781 total time=   0.0s\n",
      "[CV 4/4] END alpha=43, selection=random, tol=1e-05;, score=-1.562 total time=   0.0s\n",
      "[CV 2/4] END alpha=41, selection=random, tol=1e-05;, score=-0.054 total time=   0.0s\n",
      "[CV 4/4] END alpha=45, selection=cyclic, tol=1e-05;, score=-1.630 total time=   0.0s\n",
      "[CV 2/4] END alpha=47, selection=random, tol=1e-07;, score=-0.056 total time=   0.0s\n",
      "[CV 2/4] END alpha=46, selection=random, tol=1e-05;, score=-0.056 total time=   0.0s\n",
      "[CV 4/4] END alpha=44, selection=random, tol=1e-07;, score=-1.595 total time=   0.0s\n",
      "[CV 4/4] END alpha=42, selection=random, tol=0.01;, score=-1.431 total time=   0.0s\n",
      "[CV 2/4] END alpha=44, selection=cyclic, tol=1e-05;, score=-0.055 total time=   0.0s\n",
      "[CV 2/4] END alpha=49, selection=cyclic, tol=1e-05;, score=-0.057 total time=   0.0s\n",
      "[CV 3/4] END alpha=41, selection=random, tol=1e-05;, score=-0.173 total time=   0.0s\n",
      "[CV 1/4] END alpha=45, selection=cyclic, tol=1e-07;, score=-0.780 total time=   0.0s\n",
      "[CV 3/4] END alpha=46, selection=random, tol=1e-05;, score=-0.167 total time=   0.0s\n",
      "[CV 3/4] END alpha=47, selection=random, tol=1e-07;, score=-0.166 total time=   0.0s\n",
      "[CV 4/4] END alpha=40, selection=random, tol=0.01;, score=-1.375 total time=   0.0s\n",
      "[CV 3/4] END alpha=44, selection=cyclic, tol=1e-05;, score=-0.169 total time=   0.0s\n",
      "[CV 3/4] END alpha=49, selection=cyclic, tol=1e-05;, score=-0.164 total time=   0.0s\n",
      "[CV 1/4] END alpha=44, selection=random, tol=0.01;, score=-0.780 total time=   0.0s\n",
      "[CV 2/4] END alpha=45, selection=cyclic, tol=1e-07;, score=-0.055 total time=   0.0s\n",
      "[CV 4/4] END alpha=41, selection=random, tol=1e-05;, score=-1.496 total time=   0.0s\n",
      "[CV 4/4] END alpha=46, selection=random, tol=1e-05;, score=-1.664 total time=   0.0s\n",
      "[CV 2/4] END alpha=44, selection=random, tol=0.01;, score=-0.053 total time=   0.0s\n",
      "[CV 4/4] END alpha=49, selection=cyclic, tol=1e-05;, score=-1.772 total time=   0.0s\n",
      "[CV 1/4] END alpha=50, selection=cyclic, tol=0.01;, score=-0.774 total time=   0.0s\n",
      "[CV 4/4] END alpha=44, selection=cyclic, tol=1e-05;, score=-1.595 total time=   0.0s\n",
      "[CV 3/4] END alpha=45, selection=cyclic, tol=1e-07;, score=-0.168 total time=   0.0s\n",
      "[CV 4/4] END alpha=47, selection=random, tol=1e-07;, score=-1.700 total time=   0.0s\n",
      "[CV 1/4] END alpha=46, selection=random, tol=1e-07;, score=-0.779 total time=   0.0s\n",
      "[CV 2/4] END alpha=50, selection=cyclic, tol=0.01;, score=-0.057 total time=   0.0s\n",
      "[CV 1/4] END alpha=49, selection=cyclic, tol=1e-07;, score=-0.775 total time=   0.0s\n",
      "[CV 3/4] END alpha=44, selection=random, tol=0.01;, score=-0.169 total time=   0.0s\n",
      "[CV 1/4] END alpha=51, selection=random, tol=1e-07;, score=-0.772 total time=   0.0s\n",
      "[CV 1/4] END alpha=44, selection=cyclic, tol=1e-07;, score=-0.781 total time=   0.0s\n",
      "[CV 4/4] END alpha=45, selection=cyclic, tol=1e-07;, score=-1.630 total time=   0.0s\n",
      "[CV 2/4] END alpha=46, selection=random, tol=1e-07;, score=-0.056 total time=   0.0s\n",
      "[CV 1/4] END alpha=47, selection=random, tol=0.01;, score=-0.767 total time=   0.0s\n",
      "[CV 3/4] END alpha=50, selection=cyclic, tol=0.01;, score=-0.163 total time=   0.0s\n",
      "[CV 2/4] END alpha=49, selection=cyclic, tol=1e-07;, score=-0.057 total time=   0.0s\n",
      "[CV 4/4] END alpha=44, selection=random, tol=0.01;, score=-1.595 total time=   0.0s\n",
      "[CV 2/4] END alpha=51, selection=random, tol=1e-07;, score=-0.058 total time=   0.0s\n",
      "[CV 2/4] END alpha=44, selection=cyclic, tol=1e-07;, score=-0.055 total time=   0.0s\n",
      "[CV 1/4] END alpha=45, selection=cyclic, tol=0.01;, score=-0.781 total time=   0.0s\n",
      "[CV 3/4] END alpha=49, selection=cyclic, tol=1e-07;, score=-0.164 total time=   0.0s\n",
      "[CV 4/4] END alpha=50, selection=cyclic, tol=0.01;, score=-1.809 total time=   0.0s\n",
      "[CV 3/4] END alpha=46, selection=random, tol=1e-07;, score=-0.167 total time=   0.0s\n",
      "[CV 1/4] END alpha=53, selection=cyclic, tol=1e-05;, score=-0.769 total time=   0.0s\n",
      "[CV 2/4] END alpha=47, selection=random, tol=0.01;, score=-0.057 total time=   0.0s\n",
      "[CV 3/4] END alpha=51, selection=random, tol=1e-07;, score=-0.162 total time=   0.0s\n",
      "[CV 3/4] END alpha=44, selection=cyclic, tol=1e-07;, score=-0.169 total time=   0.0s\n",
      "[CV 2/4] END alpha=45, selection=cyclic, tol=0.01;, score=-0.055 total time=   0.0s\n",
      "[CV 4/4] END alpha=49, selection=cyclic, tol=1e-07;, score=-1.772 total time=   0.0s\n",
      "[CV 1/4] END alpha=50, selection=random, tol=1e-05;, score=-0.773 total time=   0.0s\n",
      "[CV 4/4] END alpha=46, selection=random, tol=1e-07;, score=-1.664 total time=   0.0s\n",
      "[CV 2/4] END alpha=53, selection=cyclic, tol=1e-05;, score=-0.058 total time=   0.0s\n",
      "[CV 4/4] END alpha=51, selection=random, tol=1e-07;, score=-1.838 total time=   0.0s\n",
      "[CV 4/4] END alpha=44, selection=cyclic, tol=1e-07;, score=-1.595 total time=   0.0s\n",
      "[CV 1/4] END alpha=55, selection=random, tol=1e-07;, score=-0.766 total time=   0.0s\n",
      "[CV 1/4] END alpha=54, selection=cyclic, tol=0.01;, score=-0.769 total time=   0.0s\n",
      "[CV 3/4] END alpha=45, selection=cyclic, tol=0.01;, score=-0.168 total time=   0.0s\n",
      "[CV 1/4] END alpha=49, selection=cyclic, tol=0.01;, score=-0.776 total time=   0.0s\n",
      "[CV 2/4] END alpha=50, selection=random, tol=1e-05;, score=-0.057 total time=   0.0s\n",
      "[CV 1/4] END alpha=46, selection=random, tol=0.01;, score=-0.776 total time=   0.0s\n",
      "[CV 3/4] END alpha=53, selection=cyclic, tol=1e-05;, score=-0.161 total time=   0.0s\n",
      "[CV 1/4] END alpha=51, selection=random, tol=0.01;, score=-0.770 total time=   0.0s\n",
      "[CV 2/4] END alpha=55, selection=random, tol=1e-07;, score=-0.059 total time=   0.0s[CV 2/4] END alpha=54, selection=cyclic, tol=0.01;, score=-0.059 total time=   0.0s\n",
      "\n",
      "[CV 4/4] END alpha=45, selection=cyclic, tol=0.01;, score=-1.629 total time=   0.0s\n",
      "[CV 3/4] END alpha=47, selection=random, tol=0.01;, score=-0.166 total time=   0.0s\n",
      "[CV 3/4] END alpha=50, selection=random, tol=1e-05;, score=-0.163 total time=   0.0s\n",
      "[CV 1/4] END alpha=57, selection=cyclic, tol=1e-05;, score=-0.764 total time=   0.0s\n",
      "[CV 4/4] END alpha=53, selection=cyclic, tol=1e-05;, score=-1.868 total time=   0.0s\n",
      "[CV 2/4] END alpha=46, selection=random, tol=0.01;, score=-0.056 total time=   0.0s\n",
      "[CV 2/4] END alpha=49, selection=cyclic, tol=0.01;, score=-0.057 total time=   0.0s\n",
      "[CV 2/4] END alpha=51, selection=random, tol=0.01;, score=-0.058 total time=   0.0s\n",
      "[CV 3/4] END alpha=55, selection=random, tol=1e-07;, score=-0.159 total time=   0.0s[CV 3/4] END alpha=54, selection=cyclic, tol=0.01;, score=-0.160 total time=   0.0s\n",
      "\n",
      "[CV 1/4] END alpha=45, selection=random, tol=1e-05;, score=-0.780 total time=   0.0s\n",
      "[CV 4/4] END alpha=47, selection=random, tol=0.01;, score=-1.685 total time=   0.0s\n",
      "[CV 2/4] END alpha=57, selection=cyclic, tol=1e-05;, score=-0.060 total time=   0.0s\n",
      "[CV 4/4] END alpha=50, selection=random, tol=1e-05;, score=-1.810 total time=   0.0s\n",
      "[CV 1/4] END alpha=53, selection=cyclic, tol=1e-07;, score=-0.769 total time=   0.0s\n",
      "[CV 1/4] END alpha=58, selection=cyclic, tol=0.01;, score=-0.763 total time=   0.0s\n",
      "[CV 3/4] END alpha=46, selection=random, tol=0.01;, score=-0.167 total time=   0.0s\n",
      "[CV 3/4] END alpha=49, selection=cyclic, tol=0.01;, score=-0.164 total time=   0.0s\n",
      "[CV 3/4] END alpha=51, selection=random, tol=0.01;, score=-0.163 total time=   0.0s\n",
      "[CV 4/4] END alpha=55, selection=random, tol=1e-07;, score=-1.898 total time=   0.0s[CV 4/4] END alpha=54, selection=cyclic, tol=0.01;, score=-1.883 total time=   0.0s\n",
      "\n",
      "[CV 1/4] END alpha=48, selection=cyclic, tol=1e-05;, score=-0.776 total time=   0.0s\n",
      "[CV 2/4] END alpha=45, selection=random, tol=1e-05;, score=-0.055 total time=   0.0s\n",
      "[CV 3/4] END alpha=57, selection=cyclic, tol=1e-05;, score=-0.158 total time=   0.0s\n",
      "[CV 2/4] END alpha=53, selection=cyclic, tol=1e-07;, score=-0.058 total time=   0.0s\n",
      "[CV 1/4] END alpha=50, selection=random, tol=1e-07;, score=-0.773 total time=   0.0s\n",
      "[CV 4/4] END alpha=46, selection=random, tol=0.01;, score=-1.544 total time=   0.0s\n",
      "[CV 2/4] END alpha=58, selection=cyclic, tol=0.01;, score=-0.060 total time=   0.0s\n",
      "[CV 4/4] END alpha=49, selection=cyclic, tol=0.01;, score=-1.772 total time=   0.0s\n",
      "[CV 4/4] END alpha=51, selection=random, tol=0.01;, score=-1.837 total time=   0.0s\n",
      "[CV 2/4] END alpha=48, selection=cyclic, tol=1e-05;, score=-0.056 total time=   0.0s\n",
      "[CV 1/4] END alpha=55, selection=random, tol=0.01;, score=-0.756 total time=   0.0s[CV 1/4] END alpha=54, selection=random, tol=1e-05;, score=-0.768 total time=   0.0s\n",
      "\n",
      "[CV 3/4] END alpha=45, selection=random, tol=1e-05;, score=-0.168 total time=   0.0s\n",
      "[CV 4/4] END alpha=57, selection=cyclic, tol=1e-05;, score=-1.929 total time=   0.0s\n",
      "[CV 3/4] END alpha=53, selection=cyclic, tol=1e-07;, score=-0.161 total time=   0.0s\n",
      "[CV 1/4] END alpha=47, selection=cyclic, tol=1e-05;, score=-0.777 total time=   0.0s\n",
      "[CV 2/4] END alpha=50, selection=random, tol=1e-07;, score=-0.057 total time=   0.0s\n",
      "[CV 1/4] END alpha=49, selection=random, tol=1e-05;, score=-0.775 total time=   0.0s\n",
      "[CV 3/4] END alpha=58, selection=cyclic, tol=0.01;, score=-0.157 total time=   0.0s\n",
      "[CV 3/4] END alpha=48, selection=cyclic, tol=1e-05;, score=-0.165 total time=   0.0s\n",
      "[CV 1/4] END alpha=52, selection=cyclic, tol=1e-05;, score=-0.770 total time=   0.0s\n",
      "[CV 2/4] END alpha=55, selection=random, tol=0.01;, score=-0.059 total time=   0.0s[CV 2/4] END alpha=54, selection=random, tol=1e-05;, score=-0.059 total time=   0.0s\n",
      "\n",
      "[CV 4/4] END alpha=45, selection=random, tol=1e-05;, score=-1.630 total time=   0.0s\n",
      "[CV 1/4] END alpha=59, selection=random, tol=1e-07;, score=-0.761 total time=   0.0s\n",
      "[CV 1/4] END alpha=57, selection=cyclic, tol=1e-07;, score=-0.764 total time=   0.0s\n",
      "[CV 3/4] END alpha=50, selection=random, tol=1e-07;, score=-0.163 total time=   0.0s\n",
      "[CV 4/4] END alpha=53, selection=cyclic, tol=1e-07;, score=-1.868 total time=   0.0s\n",
      "[CV 2/4] END alpha=49, selection=random, tol=1e-05;, score=-0.057 total time=   0.0s\n",
      "[CV 4/4] END alpha=48, selection=cyclic, tol=1e-05;, score=-1.736 total time=   0.0s\n",
      "[CV 4/4] END alpha=58, selection=cyclic, tol=0.01;, score=-1.944 total time=   0.0s\n",
      "[CV 3/4] END alpha=54, selection=random, tol=1e-05;, score=-0.160 total time=   0.0s\n",
      "[CV 2/4] END alpha=52, selection=cyclic, tol=1e-05;, score=-0.058 total time=   0.0s\n",
      "[CV 3/4] END alpha=55, selection=random, tol=0.01;, score=-0.159 total time=   0.0s[CV 1/4] END alpha=45, selection=random, tol=1e-07;, score=-0.780 total time=   0.0s\n",
      "\n",
      "[CV 2/4] END alpha=47, selection=cyclic, tol=1e-05;, score=-0.056 total time=   0.0s\n",
      "[CV 2/4] END alpha=57, selection=cyclic, tol=1e-07;, score=-0.060 total time=   0.0s\n",
      "[CV 4/4] END alpha=50, selection=random, tol=1e-07;, score=-1.810 total time=   0.0s\n",
      "[CV 2/4] END alpha=59, selection=random, tol=1e-07;, score=-0.061 total time=   0.0s[CV 1/4] END alpha=53, selection=cyclic, tol=0.01;, score=-0.770 total time=   0.0s\n",
      "\n",
      "[CV 1/4] END alpha=48, selection=cyclic, tol=1e-07;, score=-0.776 total time=   0.0s\n",
      "[CV 3/4] END alpha=49, selection=random, tol=1e-05;, score=-0.164 total time=   0.0s\n",
      "[CV 4/4] END alpha=54, selection=random, tol=1e-05;, score=-1.883 total time=   0.0s\n",
      "[CV 4/4] END alpha=55, selection=random, tol=0.01;, score=-1.898 total time=   0.0s\n",
      "[CV 2/4] END alpha=45, selection=random, tol=1e-07;, score=-0.055 total time=   0.0s\n",
      "[CV 3/4] END alpha=47, selection=cyclic, tol=1e-05;, score=-0.166 total time=   0.0s\n",
      "[CV 1/4] END alpha=61, selection=cyclic, tol=1e-05;, score=-0.758 total time=   0.0s\n",
      "[CV 1/4] END alpha=50, selection=random, tol=0.01;, score=-0.772 total time=   0.0s\n",
      "[CV 3/4] END alpha=57, selection=cyclic, tol=1e-07;, score=-0.158 total time=   0.0s\n",
      "[CV 3/4] END alpha=52, selection=cyclic, tol=1e-05;, score=-0.162 total time=   0.0s\n",
      "[CV 2/4] END alpha=53, selection=cyclic, tol=0.01;, score=-0.058 total time=   0.0s\n",
      "[CV 2/4] END alpha=48, selection=cyclic, tol=1e-07;, score=-0.056 total time=   0.0s\n",
      "[CV 4/4] END alpha=49, selection=random, tol=1e-05;, score=-1.772 total time=   0.0s\n",
      "[CV 1/4] END alpha=58, selection=random, tol=1e-05;, score=-0.762 total time=   0.0s[CV 3/4] END alpha=59, selection=random, tol=1e-07;, score=-0.157 total time=   0.0s\n",
      "\n",
      "[CV 1/4] END alpha=56, selection=cyclic, tol=1e-05;, score=-0.765 total time=   0.0s\n",
      "[CV 1/4] END alpha=54, selection=random, tol=1e-07;, score=-0.768 total time=   0.0s\n",
      "[CV 3/4] END alpha=45, selection=random, tol=1e-07;, score=-0.168 total time=   0.0s\n",
      "[CV 4/4] END alpha=47, selection=cyclic, tol=1e-05;, score=-1.700 total time=   0.0s\n",
      "[CV 4/4] END alpha=57, selection=cyclic, tol=1e-07;, score=-1.929 total time=   0.0s\n",
      "[CV 3/4] END alpha=48, selection=cyclic, tol=1e-07;, score=-0.165 total time=   0.0s\n",
      "[CV 2/4] END alpha=50, selection=random, tol=0.01;, score=-0.057 total time=   0.0s\n",
      "[CV 1/4] END alpha=49, selection=random, tol=1e-07;, score=-0.775 total time=   0.0s\n",
      "[CV 2/4] END alpha=58, selection=random, tol=1e-05;, score=-0.060 total time=   0.0s\n",
      "[CV 3/4] END alpha=53, selection=cyclic, tol=0.01;, score=-0.161 total time=   0.0s\n",
      "[CV 2/4] END alpha=56, selection=cyclic, tol=1e-05;, score=-0.060 total time=   0.0s[CV 2/4] END alpha=54, selection=random, tol=1e-07;, score=-0.059 total time=   0.0s\n",
      "\n",
      "[CV 4/4] END alpha=59, selection=random, tol=1e-07;, score=-1.960 total time=   0.0s\n",
      "[CV 4/4] END alpha=45, selection=random, tol=1e-07;, score=-1.630 total time=   0.0s\n",
      "[CV 1/4] END alpha=47, selection=cyclic, tol=1e-07;, score=-0.777 total time=   0.0s\n",
      "[CV 4/4] END alpha=48, selection=cyclic, tol=1e-07;, score=-1.736 total time=   0.0s\n",
      "[CV 1/4] END alpha=57, selection=cyclic, tol=0.01;, score=-0.765 total time=   0.0s\n",
      "[CV 3/4] END alpha=50, selection=random, tol=0.01;, score=-0.162 total time=   0.0s\n",
      "[CV 4/4] END alpha=52, selection=cyclic, tol=1e-05;, score=-1.853 total time=   0.0s[CV 2/4] END alpha=49, selection=random, tol=1e-07;, score=-0.057 total time=   0.0s\n",
      "\n",
      "[CV 3/4] END alpha=54, selection=random, tol=1e-07;, score=-0.160 total time=   0.0s[CV 3/4] END alpha=56, selection=cyclic, tol=1e-05;, score=-0.159 total time=   0.0s\n",
      "\n",
      "[CV 1/4] END alpha=45, selection=random, tol=0.01;, score=-0.781 total time=   0.0s[CV 1/4] END alpha=63, selection=random, tol=1e-07;, score=-0.756 total time=   0.0s\n",
      "\n",
      "[CV 3/4] END alpha=58, selection=random, tol=1e-05;, score=-0.157 total time=   0.0s\n",
      "[CV 4/4] END alpha=53, selection=cyclic, tol=0.01;, score=-1.868 total time=   0.0s\n",
      "[CV 2/4] END alpha=47, selection=cyclic, tol=1e-07;, score=-0.056 total time=   0.0s\n",
      "[CV 1/4] END alpha=59, selection=random, tol=0.01;, score=-0.752 total time=   0.0s\n",
      "[CV 1/4] END alpha=62, selection=cyclic, tol=0.01;, score=-0.758 total time=   0.0s\n",
      "[CV 1/4] END alpha=48, selection=cyclic, tol=0.01;, score=-0.777 total time=   0.0s\n",
      "[CV 2/4] END alpha=57, selection=cyclic, tol=0.01;, score=-0.060 total time=   0.0s\n",
      "[CV 2/4] END alpha=61, selection=cyclic, tol=1e-05;, score=-0.062 total time=   0.0s\n",
      "[CV 3/4] END alpha=49, selection=random, tol=1e-07;, score=-0.164 total time=   0.0s\n",
      "[CV 4/4] END alpha=50, selection=random, tol=0.01;, score=-1.818 total time=   0.0s\n",
      "[CV 4/4] END alpha=54, selection=random, tol=1e-07;, score=-1.883 total time=   0.0s[CV 4/4] END alpha=56, selection=cyclic, tol=1e-05;, score=-1.914 total time=   0.0s\n",
      "\n",
      "[CV 2/4] END alpha=45, selection=random, tol=0.01;, score=-0.056 total time=   0.0s\n",
      "[CV 1/4] END alpha=65, selection=cyclic, tol=1e-05;, score=-0.753 total time=   0.0s\n",
      "[CV 2/4] END alpha=59, selection=random, tol=0.01;, score=-0.061 total time=   0.0s[CV 1/4] END alpha=51, selection=cyclic, tol=1e-05;, score=-0.772 total time=   0.0s\n",
      "[CV 2/4] END alpha=62, selection=cyclic, tol=0.01;, score=-0.062 total time=   0.0s\n",
      "[CV 3/4] END alpha=57, selection=cyclic, tol=0.01;, score=-0.158 total time=   0.0s\n",
      "[CV 2/4] END alpha=48, selection=cyclic, tol=0.01;, score=-0.056 total time=   0.0s\n",
      "[CV 3/4] END alpha=61, selection=cyclic, tol=1e-05;, score=-0.157 total time=   0.0s\n",
      "[CV 1/4] END alpha=56, selection=cyclic, tol=1e-07;, score=-0.765 total time=   0.0s[CV 1/4] END alpha=54, selection=random, tol=0.01;, score=-0.767 total time=   0.0s\n",
      "\n",
      "[CV 1/4] END alpha=52, selection=cyclic, tol=1e-07;, score=-0.770 total time=   0.0s\n",
      "\n",
      "[CV 4/4] END alpha=49, selection=random, tol=1e-07;, score=-1.772 total time=   0.0s\n",
      "[CV 3/4] END alpha=45, selection=random, tol=0.01;, score=-0.168 total time=   0.0s\n",
      "[CV 1/4] END alpha=53, selection=random, tol=1e-05;, score=-0.769 total time=   0.0s\n",
      "[CV 4/4] END alpha=58, selection=random, tol=1e-05;, score=-1.944 total time=   0.0s\n",
      "[CV 2/4] END alpha=63, selection=random, tol=1e-07;, score=-0.062 total time=   0.0s\n",
      "[CV 2/4] END alpha=54, selection=random, tol=0.01;, score=-0.059 total time=   0.0s[CV 4/4] END alpha=57, selection=cyclic, tol=0.01;, score=-1.929 total time=   0.0s\n",
      "\n",
      "[CV 4/4] END alpha=45, selection=random, tol=0.01;, score=-1.628 total time=   0.0s\n",
      "[CV 1/4] END alpha=49, selection=random, tol=0.01;, score=-0.775 total time=   0.0s\n",
      "[CV 2/4] END alpha=56, selection=cyclic, tol=1e-07;, score=-0.060 total time=   0.0s\n",
      "[CV 2/4] END alpha=65, selection=cyclic, tol=1e-05;, score=-0.063 total time=   0.0s[CV 3/4] END alpha=62, selection=cyclic, tol=0.01;, score=-0.157 total time=   0.0s\n",
      "\n",
      "[CV 1/4] END alpha=58, selection=random, tol=1e-07;, score=-0.762 total time=   0.0s\n",
      "[CV 1/4] END alpha=57, selection=random, tol=1e-05;, score=-0.764 total time=   0.0s\n",
      "[CV 3/4] END alpha=59, selection=random, tol=0.01;, score=-0.157 total time=   0.0s[CV 3/4] END alpha=54, selection=random, tol=0.01;, score=-0.160 total time=   0.0s\n",
      "\n",
      "[CV 1/4] END alpha=46, selection=cyclic, tol=1e-05;, score=-0.779 total time=   0.0s\n",
      "[CV 2/4] END alpha=49, selection=random, tol=0.01;, score=-0.057 total time=   0.0s\n",
      "[CV 3/4] END alpha=47, selection=cyclic, tol=1e-07;, score=-0.166 total time=   0.0s\n",
      "[CV 3/4] END alpha=63, selection=random, tol=1e-07;, score=-0.157 total time=   0.0s\n",
      "[CV 2/4] END alpha=53, selection=random, tol=1e-05;, score=-0.058 total time=   0.0s[CV 3/4] END alpha=56, selection=cyclic, tol=1e-07;, score=-0.159 total time=   0.0s\n",
      "[CV 2/4] END alpha=51, selection=cyclic, tol=1e-05;, score=-0.058 total time=   0.0s\n",
      "[CV 4/4] END alpha=62, selection=cyclic, tol=0.01;, score=-2.005 total time=   0.0s\n",
      "[CV 2/4] END alpha=52, selection=cyclic, tol=1e-07;, score=-0.058 total time=   0.0s\n",
      "[CV 3/4] END alpha=48, selection=cyclic, tol=0.01;, score=-0.165 total time=   0.0s\n",
      "[CV 4/4] END alpha=61, selection=cyclic, tol=1e-05;, score=-1.990 total time=   0.0s[CV 4/4] END alpha=54, selection=random, tol=0.01;, score=-1.883 total time=   0.0s\n",
      "[CV 2/4] END alpha=57, selection=random, tol=1e-05;, score=-0.060 total time=   0.0s\n",
      "[CV 4/4] END alpha=59, selection=random, tol=0.01;, score=-1.960 total time=   0.0s\n",
      "[CV 2/4] END alpha=46, selection=cyclic, tol=1e-05;, score=-0.056 total time=   0.0s\n",
      "[CV 3/4] END alpha=51, selection=cyclic, tol=1e-05;, score=-0.162 total time=   0.0s\n",
      "[CV 3/4] END alpha=49, selection=random, tol=0.01;, score=-0.165 total time=   0.0s\n",
      "[CV 4/4] END alpha=47, selection=cyclic, tol=1e-07;, score=-1.700 total time=   0.0s[CV 1/4] END alpha=62, selection=random, tol=1e-05;, score=-0.757 total time=   0.0s[CV 4/4] END alpha=56, selection=cyclic, tol=1e-07;, score=-1.914 total time=   0.0s\n",
      "\n",
      "\n",
      "[CV 3/4] END alpha=52, selection=cyclic, tol=1e-07;, score=-0.162 total time=   0.0s\n",
      "[CV 3/4] END alpha=65, selection=cyclic, tol=1e-05;, score=-0.157 total time=   0.0s\n",
      "[CV 1/4] END alpha=55, selection=cyclic, tol=1e-05;, score=-0.766 total time=   0.0s\n",
      "[CV 2/4] END alpha=58, selection=random, tol=1e-07;, score=-0.060 total time=   0.0s\n",
      "[CV 3/4] END alpha=57, selection=random, tol=1e-05;, score=-0.158 total time=   0.0s\n",
      "[CV 1/4] END alpha=60, selection=cyclic, tol=1e-05;, score=-0.760 total time=   0.0s[CV 3/4] END alpha=46, selection=cyclic, tol=1e-05;, score=-0.167 total time=   0.0s\n",
      "\n",
      "[CV 4/4] END alpha=49, selection=random, tol=0.01;, score=-1.692 total time=   0.0s\n",
      "\n",
      "[CV 4/4] END alpha=63, selection=random, tol=1e-07;, score=-2.019 total time=   0.0s\n",
      "[CV 1/4] END alpha=56, selection=cyclic, tol=0.01;, score=-0.766 total time=   0.0s\n",
      "[CV 4/4] END alpha=52, selection=cyclic, tol=1e-07;, score=-1.853 total time=   0.0s\n",
      "[CV 4/4] END alpha=65, selection=cyclic, tol=1e-05;, score=-2.048 total time=   0.0s\n",
      "[CV 2/4] END alpha=62, selection=random, tol=1e-05;, score=-0.062 total time=   0.0s\n",
      "\n",
      "[CV 3/4] END alpha=58, selection=random, tol=1e-07;, score=-0.157 total time=   0.0s\n",
      "[CV 4/4] END alpha=57, selection=random, tol=1e-05;, score=-1.929 total time=   0.0s\n",
      "[CV 2/4] END alpha=55, selection=cyclic, tol=1e-05;, score=-0.059 total time=   0.0s\n",
      "[CV 1/4] END alpha=50, selection=cyclic, tol=1e-05;, score=-0.773 total time=   0.0s\n",
      "[CV 2/4] END alpha=60, selection=cyclic, tol=1e-05;, score=-0.061 total time=   0.0s[CV 1/4] END alpha=61, selection=cyclic, tol=1e-07;, score=-0.758 total time=   0.0s\n",
      "\n",
      "[CV 2/4] END alpha=56, selection=cyclic, tol=0.01;, score=-0.059 total time=   0.0s\n",
      "[CV 1/4] END alpha=63, selection=random, tol=0.01;, score=-0.756 total time=   0.0s\n",
      "[CV 1/4] END alpha=52, selection=cyclic, tol=0.01;, score=-0.771 total time=   0.0s\n",
      "[CV 4/4] END alpha=51, selection=cyclic, tol=1e-05;, score=-1.838 total time=   0.0s\n",
      "[CV 1/4] END alpha=47, selection=cyclic, tol=0.01;, score=-0.778 total time=   0.0s\n",
      "[CV 1/4] END alpha=65, selection=cyclic, tol=1e-07;, score=-0.753 total time=   0.0s\n",
      "[CV 3/4] END alpha=62, selection=random, tol=1e-05;, score=-0.157 total time=   0.0s\n",
      "[CV 1/4] END alpha=57, selection=random, tol=1e-07;, score=-0.764 total time=   0.0s\n",
      "[CV 3/4] END alpha=55, selection=cyclic, tol=1e-05;, score=-0.159 total time=   0.0s\n",
      "[CV 3/4] END alpha=60, selection=cyclic, tol=1e-05;, score=-0.157 total time=   0.0s\n",
      "[CV 2/4] END alpha=50, selection=cyclic, tol=1e-05;, score=-0.057 total time=   0.0s\n",
      "[CV 3/4] END alpha=56, selection=cyclic, tol=0.01;, score=-0.159 total time=   0.0s\n",
      "[CV 1/4] END alpha=51, selection=cyclic, tol=1e-07;, score=-0.772 total time=   0.0s\n",
      "[CV 2/4] END alpha=52, selection=cyclic, tol=0.01;, score=-0.058 total time=   0.0s\n",
      "[CV 4/4] END alpha=46, selection=cyclic, tol=1e-05;, score=-1.664 total time=   0.0s\n",
      "[CV 2/4] END alpha=47, selection=cyclic, tol=0.01;, score=-0.056 total time=   0.0s[CV 4/4] END alpha=55, selection=cyclic, tol=1e-05;, score=-1.898 total time=   0.0s\n",
      "[CV 2/4] END alpha=57, selection=random, tol=1e-07;, score=-0.060 total time=   0.0s\n",
      "[CV 4/4] END alpha=62, selection=random, tol=1e-05;, score=-2.005 total time=   0.0s\n",
      "[CV 4/4] END alpha=60, selection=cyclic, tol=1e-05;, score=-1.975 total time=   0.0s\n",
      "[CV 3/4] END alpha=50, selection=cyclic, tol=1e-05;, score=-0.163 total time=   0.0s\n",
      "[CV 2/4] END alpha=51, selection=cyclic, tol=1e-07;, score=-0.058 total time=   0.0s\n",
      "[CV 4/4] END alpha=48, selection=cyclic, tol=0.01;, score=-1.736 total time=   0.0s\n",
      "[CV 4/4] END alpha=56, selection=cyclic, tol=0.01;, score=-1.914 total time=   0.0s\n",
      "[CV 2/4] END alpha=63, selection=random, tol=0.01;, score=-0.063 total time=   0.0s\n",
      "[CV 3/4] END alpha=52, selection=cyclic, tol=0.01;, score=-0.161 total time=   0.0s\n",
      "[CV 3/4] END alpha=53, selection=random, tol=1e-05;, score=-0.161 total time=   0.0s\n",
      "[CV 2/4] END alpha=61, selection=cyclic, tol=1e-07;, score=-0.062 total time=   0.0s\n",
      "[CV 1/4] END alpha=55, selection=cyclic, tol=1e-07;, score=-0.766 total time=   0.0s\n",
      "[CV 3/4] END alpha=57, selection=random, tol=1e-07;, score=-0.158 total time=   0.0s\n",
      "[CV 1/4] END alpha=60, selection=cyclic, tol=1e-07;, score=-0.760 total time=   0.0s\n",
      "[CV 1/4] END alpha=62, selection=random, tol=1e-07;, score=-0.757 total time=   0.0s\n",
      "[CV 3/4] END alpha=51, selection=cyclic, tol=1e-07;, score=-0.162 total time=   0.0s\n",
      "[CV 4/4] END alpha=50, selection=cyclic, tol=1e-05;, score=-1.810 total time=   0.0s\n",
      "[CV 1/4] END alpha=56, selection=random, tol=1e-05;, score=-0.765 total time=   0.0s\n",
      "[CV 1/4] END alpha=46, selection=cyclic, tol=1e-07;, score=-0.779 total time=   0.0s\n",
      "\n",
      "[CV 1/4] END alpha=48, selection=random, tol=1e-05;, score=-0.776 total time=   0.0s\n",
      "[CV 2/4] END alpha=65, selection=cyclic, tol=1e-07;, score=-0.063 total time=   0.0s\n",
      "[CV 4/4] END alpha=58, selection=random, tol=1e-07;, score=-1.944 total time=   0.0s\n",
      "[CV 3/4] END alpha=63, selection=random, tol=0.01;, score=-0.156 total time=   0.0s\n",
      "[CV 2/4] END alpha=55, selection=cyclic, tol=1e-07;, score=-0.059 total time=   0.0s\n",
      "[CV 4/4] END alpha=57, selection=random, tol=1e-07;, score=-1.929 total time=   0.0s[CV 4/4] END alpha=51, selection=cyclic, tol=1e-07;, score=-1.838 total time=   0.0s\n",
      "\n",
      "[CV 2/4] END alpha=62, selection=random, tol=1e-07;, score=-0.062 total time=   0.0s\n",
      "[CV 4/4] END alpha=53, selection=random, tol=1e-05;, score=-1.868 total time=   0.0s\n",
      "[CV 2/4] END alpha=56, selection=random, tol=1e-05;, score=-0.060 total time=   0.0s\n",
      "[CV 1/4] END alpha=50, selection=cyclic, tol=1e-07;, score=-0.773 total time=   0.0s\n",
      "[CV 2/4] END alpha=60, selection=cyclic, tol=1e-07;, score=-0.061 total time=   0.0s\n",
      "[CV 3/4] END alpha=47, selection=cyclic, tol=0.01;, score=-0.166 total time=   0.0s\n",
      "[CV 2/4] END alpha=48, selection=random, tol=1e-05;, score=-0.056 total time=   0.0s\n",
      "[CV 1/4] END alpha=58, selection=random, tol=0.01;, score=-0.781 total time=   0.0s\n",
      "[CV 3/4] END alpha=65, selection=cyclic, tol=1e-07;, score=-0.157 total time=   0.0s\n",
      "[CV 4/4] END alpha=52, selection=cyclic, tol=0.01;, score=-1.853 total time=   0.0s\n",
      "[CV 3/4] END alpha=61, selection=cyclic, tol=1e-07;, score=-0.157 total time=   0.0s\n",
      "[CV 3/4] END alpha=55, selection=cyclic, tol=1e-07;, score=-0.159 total time=   0.0s\n",
      "[CV 4/4] END alpha=63, selection=random, tol=0.01;, score=-2.019 total time=   0.0s\n",
      "[CV 1/4] END alpha=57, selection=random, tol=0.01;, score=-0.763 total time=   0.0s\n",
      "[CV 3/4] END alpha=62, selection=random, tol=1e-07;, score=-0.157 total time=   0.0s\n",
      "[CV 3/4] END alpha=56, selection=random, tol=1e-05;, score=-0.159 total time=   0.0s\n",
      "[CV 2/4] END alpha=50, selection=cyclic, tol=1e-07;, score=-0.057 total time=   0.0s\n",
      "[CV 4/4] END alpha=47, selection=cyclic, tol=0.01;, score=-1.700 total time=   0.0s\n",
      "[CV 4/4] END alpha=55, selection=cyclic, tol=1e-07;, score=-1.898 total time=   0.0s\n",
      "[CV 2/4] END alpha=46, selection=cyclic, tol=1e-07;, score=-0.056 total time=   0.0s\n",
      "[CV 1/4] END alpha=52, selection=random, tol=1e-05;, score=-0.770 total time=   0.0s\n",
      "[CV 2/4] END alpha=58, selection=random, tol=0.01;, score=-0.060 total time=   0.0s[CV 1/4] END alpha=51, selection=cyclic, tol=0.01;, score=-0.773 total time=   0.0s\n",
      "\n",
      "[CV 1/4] END alpha=53, selection=random, tol=1e-07;, score=-0.769 total time=   0.0s\n",
      "[CV 4/4] END alpha=56, selection=random, tol=1e-05;, score=-1.914 total time=   0.0s\n",
      "[CV 4/4] END alpha=62, selection=random, tol=1e-07;, score=-2.005 total time=   0.0s\n",
      "[CV 4/4] END alpha=65, selection=cyclic, tol=1e-07;, score=-2.048 total time=   0.0s\n",
      "[CV 3/4] END alpha=50, selection=cyclic, tol=1e-07;, score=-0.163 total time=   0.0s\n",
      "[CV 3/4] END alpha=60, selection=cyclic, tol=1e-07;, score=-0.157 total time=   0.0s\n",
      "[CV 1/4] END alpha=64, selection=cyclic, tol=1e-05;, score=-0.754 total time=   0.0s\n",
      "[CV 3/4] END alpha=46, selection=cyclic, tol=1e-07;, score=-0.167 total time=   0.0s\n",
      "[CV 1/4] END alpha=47, selection=random, tol=1e-05;, score=-0.777 total time=   0.0s\n",
      "[CV 1/4] END alpha=55, selection=cyclic, tol=0.01;, score=-0.767 total time=   0.0s\n",
      "[CV 2/4] END alpha=57, selection=random, tol=0.01;, score=-0.060 total time=   0.0s\n",
      "[CV 4/4] END alpha=61, selection=cyclic, tol=1e-07;, score=-1.990 total time=   0.0s\n",
      "[CV 1/4] END alpha=56, selection=random, tol=1e-07;, score=-0.765 total time=   0.0s\n",
      "[CV 1/4] END alpha=62, selection=random, tol=0.01;, score=-0.758 total time=   0.0s\n",
      "[CV 4/4] END alpha=60, selection=cyclic, tol=1e-07;, score=-1.975 total time=   0.0s\n",
      "[CV 3/4] END alpha=58, selection=random, tol=0.01;, score=-0.159 total time=   0.0s\n",
      "[CV 4/4] END alpha=50, selection=cyclic, tol=1e-07;, score=-1.810 total time=   0.0s\n",
      "[CV 2/4] END alpha=55, selection=cyclic, tol=0.01;, score=-0.059 total time=   0.0s\n",
      "[CV 2/4] END alpha=64, selection=cyclic, tol=1e-05;, score=-0.063 total time=   0.0s\n",
      "[CV 2/4] END alpha=47, selection=random, tol=1e-05;, score=-0.056 total time=   0.0s\n",
      "[CV 2/4] END alpha=52, selection=random, tol=1e-05;, score=-0.058 total time=   0.0s\n",
      "[CV 3/4] END alpha=48, selection=random, tol=1e-05;, score=-0.165 total time=   0.0s\n",
      "[CV 3/4] END alpha=57, selection=random, tol=0.01;, score=-0.155 total time=   0.0s\n",
      "[CV 2/4] END alpha=56, selection=random, tol=1e-07;, score=-0.060 total time=   0.0s\n",
      "[CV 2/4] END alpha=62, selection=random, tol=0.01;, score=-0.060 total time=   0.0s\n",
      "[CV 1/4] END alpha=60, selection=cyclic, tol=0.01;, score=-0.761 total time=   0.0s\n",
      "[CV 4/4] END alpha=58, selection=random, tol=0.01;, score=-1.869 total time=   0.0s\n",
      "[CV 1/4] END alpha=65, selection=cyclic, tol=0.01;, score=-0.754 total time=   0.0s\n",
      "[CV 2/4] END alpha=53, selection=random, tol=1e-07;, score=-0.058 total time=   0.0s\n",
      "[CV 3/4] END alpha=64, selection=cyclic, tol=1e-05;, score=-0.157 total time=   0.0s\n",
      "[CV 3/4] END alpha=55, selection=cyclic, tol=0.01;, score=-0.159 total time=   0.0s\n",
      "[CV 3/4] END alpha=52, selection=random, tol=1e-05;, score=-0.162 total time=   0.0s\n",
      "[CV 2/4] END alpha=51, selection=cyclic, tol=0.01;, score=-0.057 total time=   0.0s\n",
      "[CV 4/4] END alpha=46, selection=cyclic, tol=1e-07;, score=-1.664 total time=   0.0s\n",
      "[CV 3/4] END alpha=47, selection=random, tol=1e-05;, score=-0.166 total time=   0.0s\n",
      "[CV 4/4] END alpha=57, selection=random, tol=0.01;, score=-1.930 total time=   0.0s\n",
      "[CV 1/4] END alpha=61, selection=cyclic, tol=0.01;, score=-0.759 total time=   0.0s\n",
      "[CV 3/4] END alpha=56, selection=random, tol=1e-07;, score=-0.159 total time=   0.0s\n",
      "[CV 2/4] END alpha=60, selection=cyclic, tol=0.01;, score=-0.061 total time=   0.0s\n",
      "[CV 4/4] END alpha=55, selection=cyclic, tol=0.01;, score=-1.898 total time=   0.0s\n",
      "[CV 4/4] END alpha=64, selection=cyclic, tol=1e-05;, score=-2.033 total time=   0.0s\n",
      "[CV 2/4] END alpha=65, selection=cyclic, tol=0.01;, score=-0.063 total time=   0.0s\n",
      "[CV 3/4] END alpha=51, selection=cyclic, tol=0.01;, score=-0.162 total time=   0.0s\n",
      "[CV 4/4] END alpha=52, selection=random, tol=1e-05;, score=-1.853 total time=   0.0s\n",
      "[CV 2/4] END alpha=61, selection=cyclic, tol=0.01;, score=-0.061 total time=   0.0s\n",
      "[CV 4/4] END alpha=56, selection=random, tol=1e-07;, score=-1.914 total time=   0.0s\n",
      "[CV 1/4] END alpha=59, selection=cyclic, tol=1e-05;, score=-0.761 total time=   0.0s\n",
      "[CV 3/4] END alpha=60, selection=cyclic, tol=0.01;, score=-0.157 total time=   0.0s\n",
      "[CV 3/4] END alpha=62, selection=random, tol=0.01;, score=-0.157 total time=   0.0s\n",
      "[CV 1/4] END alpha=55, selection=random, tol=1e-05;, score=-0.766 total time=   0.0s\n",
      "[CV 1/4] END alpha=58, selection=cyclic, tol=1e-05;, score=-0.762 total time=   0.0s\n",
      "[CV 4/4] END alpha=51, selection=cyclic, tol=0.01;, score=-1.838 total time=   0.0s\n",
      "[CV 3/4] END alpha=65, selection=cyclic, tol=0.01;, score=-0.158 total time=   0.0s\n",
      "[CV 1/4] END alpha=52, selection=random, tol=1e-07;, score=-0.770 total time=   0.0s\n",
      "[CV 4/4] END alpha=48, selection=random, tol=1e-05;, score=-1.736 total time=   0.0s\n",
      "[CV 1/4] END alpha=56, selection=random, tol=0.01;, score=-0.762 total time=   0.0s\n",
      "[CV 4/4] END alpha=60, selection=cyclic, tol=0.01;, score=-1.975 total time=   0.0s\n",
      "[CV 4/4] END alpha=47, selection=random, tol=1e-05;, score=-1.700 total time=   0.0s\n",
      "[CV 1/4] END alpha=51, selection=random, tol=1e-05;, score=-0.772 total time=   0.0s\n",
      "[CV 2/4] END alpha=58, selection=cyclic, tol=1e-05;, score=-0.060 total time=   0.0s\n",
      "[CV 1/4] END alpha=64, selection=cyclic, tol=1e-07;, score=-0.754 total time=   0.0s\n",
      "[CV 2/4] END alpha=56, selection=random, tol=0.01;, score=-0.060 total time=   0.0s\n",
      "[CV 4/4] END alpha=65, selection=cyclic, tol=0.01;, score=-2.048 total time=   0.0s\n",
      "[CV 1/4] END alpha=48, selection=random, tol=1e-07;, score=-0.776 total time=   0.0s\n",
      "[CV 4/4] END alpha=62, selection=random, tol=0.01;, score=-2.005 total time=   0.0s\n",
      "[CV 1/4] END alpha=60, selection=random, tol=1e-05;, score=-0.760 total time=   0.0s\n",
      "[CV 2/4] END alpha=59, selection=cyclic, tol=1e-05;, score=-0.061 total time=   0.0s\n",
      "[CV 2/4] END alpha=51, selection=random, tol=1e-05;, score=-0.058 total time=   0.0s\n",
      "[CV 2/4] END alpha=52, selection=random, tol=1e-07;, score=-0.058 total time=   0.0s\n",
      "[CV 3/4] END alpha=61, selection=cyclic, tol=0.01;, score=-0.157 total time=   0.0s\n",
      "[CV 3/4] END alpha=58, selection=cyclic, tol=1e-05;, score=-0.157 total time=   0.0s\n",
      "[CV 2/4] END alpha=55, selection=random, tol=1e-05;, score=-0.059 total time=   0.0s\n",
      "[CV 2/4] END alpha=64, selection=cyclic, tol=1e-07;, score=-0.063 total time=   0.0s\n",
      "[CV 3/4] END alpha=53, selection=random, tol=1e-07;, score=-0.161 total time=   0.0s\n",
      "[CV 3/4] END alpha=56, selection=random, tol=0.01;, score=-0.159 total time=   0.0s\n",
      "[CV 2/4] END alpha=48, selection=random, tol=1e-07;, score=-0.056 total time=   0.0s\n",
      "[CV 3/4] END alpha=59, selection=cyclic, tol=1e-05;, score=-0.157 total time=   0.0s\n",
      "[CV 1/4] END alpha=65, selection=random, tol=1e-05;, score=-0.753 total time=   0.0s\n",
      "[CV 3/4] END alpha=52, selection=random, tol=1e-07;, score=-0.162 total time=   0.0s\n",
      "[CV 3/4] END alpha=55, selection=random, tol=1e-05;, score=-0.159 total time=   0.0s\n",
      "[CV 4/4] END alpha=58, selection=cyclic, tol=1e-05;, score=-1.944 total time=   0.0s\n",
      "[CV 1/4] END alpha=63, selection=cyclic, tol=1e-05;, score=-0.756 total time=   0.0s\n",
      "[CV 3/4] END alpha=64, selection=cyclic, tol=1e-07;, score=-0.157 total time=   0.0s\n",
      "[CV 4/4] END alpha=56, selection=random, tol=0.01;, score=-1.913 total time=   0.0s\n",
      "[CV 4/4] END alpha=59, selection=cyclic, tol=1e-05;, score=-1.960 total time=   0.0s\n",
      "[CV 2/4] END alpha=65, selection=random, tol=1e-05;, score=-0.063 total time=   0.0s\n",
      "[CV 1/4] END alpha=59, selection=cyclic, tol=1e-07;, score=-0.761 total time=   0.0s\n",
      "[CV 2/4] END alpha=60, selection=random, tol=1e-05;, score=-0.061 total time=   0.0s\n",
      "[CV 4/4] END alpha=52, selection=random, tol=1e-07;, score=-1.853 total time=   0.0s\n",
      "[CV 3/4] END alpha=51, selection=random, tol=1e-05;, score=-0.162 total time=   0.0s\n",
      "[CV 4/4] END alpha=61, selection=cyclic, tol=0.01;, score=-1.990 total time=   0.0s\n",
      "[CV 1/4] END alpha=58, selection=cyclic, tol=1e-07;, score=-0.762 total time=   0.0s\n",
      "[CV 4/4] END alpha=64, selection=cyclic, tol=1e-07;, score=-2.033 total time=   0.0s[CV 4/4] END alpha=55, selection=random, tol=1e-05;, score=-1.898 total time=   0.0s\n",
      "\n",
      "[CV 3/4] END alpha=60, selection=random, tol=1e-05;, score=-0.157 total time=   0.0s\n",
      "[CV 2/4] END alpha=59, selection=cyclic, tol=1e-07;, score=-0.061 total time=   0.0s\n",
      "[CV 4/4] END alpha=51, selection=random, tol=1e-05;, score=-1.838 total time=   0.0s[CV 3/4] END alpha=65, selection=random, tol=1e-05;, score=-0.157 total time=   0.0s\n",
      "[CV 1/4] END alpha=52, selection=random, tol=0.01;, score=-0.774 total time=   0.0s\n",
      "[CV 4/4] END alpha=53, selection=random, tol=1e-07;, score=-1.868 total time=   0.0s\n",
      "[CV 1/4] END alpha=61, selection=random, tol=1e-05;, score=-0.758 total time=   0.0s\n",
      "[CV 2/4] END alpha=63, selection=cyclic, tol=1e-05;, score=-0.062 total time=   0.0s\n",
      "[CV 3/4] END alpha=48, selection=random, tol=1e-07;, score=-0.165 total time=   0.0s\n",
      "[CV 1/4] END alpha=64, selection=cyclic, tol=0.01;, score=-0.755 total time=   0.0s\n",
      "[CV 2/4] END alpha=58, selection=cyclic, tol=1e-07;, score=-0.060 total time=   0.0s\n",
      "[CV 3/4] END alpha=59, selection=cyclic, tol=1e-07;, score=-0.157 total time=   0.0s\n",
      "\n",
      "[CV 4/4] END alpha=65, selection=random, tol=1e-05;, score=-2.048 total time=   0.0s[CV 3/4] END alpha=63, selection=cyclic, tol=1e-05;, score=-0.157 total time=   0.0s\n",
      "\n",
      "[CV 4/4] END alpha=48, selection=random, tol=1e-07;, score=-1.736 total time=   0.0s\n",
      "[CV 2/4] END alpha=64, selection=cyclic, tol=0.01;, score=-0.063 total time=   0.0s\n",
      "[CV 1/4] END alpha=53, selection=random, tol=0.01;, score=-0.770 total time=   0.0s\n",
      "[CV 2/4] END alpha=61, selection=random, tol=1e-05;, score=-0.062 total time=   0.0s\n",
      "[CV 4/4] END alpha=60, selection=random, tol=1e-05;, score=-1.975 total time=   0.0s\n",
      "[CV 4/4] END alpha=63, selection=cyclic, tol=1e-05;, score=-2.019 total time=   0.0s\n",
      "[CV 1/4] END alpha=48, selection=random, tol=0.01;, score=-0.777 total time=   0.0s\n",
      "[CV 3/4] END alpha=64, selection=cyclic, tol=0.01;, score=-0.157 total time=   0.0s[CV 4/4] END alpha=59, selection=cyclic, tol=1e-07;, score=-1.960 total time=   0.0s\n",
      "\n",
      "[CV 1/4] END alpha=66, selection=cyclic, tol=0.01;, score=-0.752 total time=   0.0s\n",
      "[CV 3/4] END alpha=58, selection=cyclic, tol=1e-07;, score=-0.157 total time=   0.0s\n",
      "[CV 2/4] END alpha=52, selection=random, tol=0.01;, score=-0.058 total time=   0.0s\n",
      "[CV 1/4] END alpha=65, selection=random, tol=1e-07;, score=-0.753 total time=   0.0s\n",
      "[CV 1/4] END alpha=67, selection=random, tol=1e-07;, score=-0.750 total time=   0.0s\n",
      "[CV 2/4] END alpha=53, selection=random, tol=0.01;, score=-0.058 total time=   0.0s\n",
      "[CV 2/4] END alpha=48, selection=random, tol=0.01;, score=-0.056 total time=   0.0s\n",
      "[CV 1/4] END alpha=63, selection=cyclic, tol=1e-07;, score=-0.756 total time=   0.0s\n",
      "[CV 1/4] END alpha=69, selection=cyclic, tol=1e-05;, score=-0.748 total time=   0.0s\n",
      "[CV 2/4] END alpha=67, selection=random, tol=1e-07;, score=-0.064 total time=   0.0s\n",
      "[CV 3/4] END alpha=53, selection=random, tol=0.01;, score=-0.162 total time=   0.0s\n",
      "[CV 3/4] END alpha=48, selection=random, tol=0.01;, score=-0.165 total time=   0.0s\n",
      "[CV 1/4] END alpha=59, selection=cyclic, tol=0.01;, score=-0.762 total time=   0.0s[CV 2/4] END alpha=63, selection=cyclic, tol=1e-07;, score=-0.062 total time=   0.0s\n",
      "\n",
      "[CV 3/4] END alpha=52, selection=random, tol=0.01;, score=-0.161 total time=   0.0s\n",
      "[CV 3/4] END alpha=61, selection=random, tol=1e-05;, score=-0.157 total time=   0.0s\n",
      "[CV 4/4] END alpha=58, selection=cyclic, tol=1e-07;, score=-1.944 total time=   0.0s\n",
      "[CV 2/4] END alpha=66, selection=cyclic, tol=0.01;, score=-0.063 total time=   0.0s\n",
      "[CV 2/4] END alpha=65, selection=random, tol=1e-07;, score=-0.063 total time=   0.0s\n",
      "[CV 2/4] END alpha=69, selection=cyclic, tol=1e-05;, score=-0.065 total time=   0.0s\n",
      "[CV 1/4] END alpha=60, selection=random, tol=1e-07;, score=-0.760 total time=   0.0s\n",
      "[CV 4/4] END alpha=64, selection=cyclic, tol=0.01;, score=-2.033 total time=   0.0s\n",
      "[CV 4/4] END alpha=52, selection=random, tol=0.01;, score=-1.853 total time=   0.0s\n",
      "[CV 4/4] END alpha=48, selection=random, tol=0.01;, score=-1.739 total time=   0.0s\n",
      "[CV 3/4] END alpha=63, selection=cyclic, tol=1e-07;, score=-0.157 total time=   0.0s\n",
      "[CV 2/4] END alpha=59, selection=cyclic, tol=0.01;, score=-0.061 total time=   0.0s\n",
      "[CV 4/4] END alpha=53, selection=random, tol=0.01;, score=-1.870 total time=   0.0s\n",
      "[CV 3/4] END alpha=66, selection=cyclic, tol=0.01;, score=-0.158 total time=   0.0s\n",
      "[CV 1/4] END alpha=70, selection=cyclic, tol=0.01;, score=-0.747 total time=   0.0s\n",
      "[CV 4/4] END alpha=61, selection=random, tol=1e-05;, score=-1.990 total time=   0.0s\n",
      "[CV 3/4] END alpha=59, selection=cyclic, tol=0.01;, score=-0.157 total time=   0.0s\n",
      "[CV 1/4] END alpha=54, selection=cyclic, tol=1e-05;, score=-0.768 total time=   0.0s\n",
      "[CV 1/4] END alpha=64, selection=random, tol=1e-05;, score=-0.754 total time=   0.0s\n",
      "[CV 4/4] END alpha=63, selection=cyclic, tol=1e-07;, score=-2.019 total time=   0.0s\n",
      "[CV 3/4] END alpha=69, selection=cyclic, tol=1e-05;, score=-0.158 total time=   0.0s\n",
      "[CV 2/4] END alpha=60, selection=random, tol=1e-07;, score=-0.061 total time=   0.0s\n",
      "[CV 4/4] END alpha=59, selection=cyclic, tol=0.01;, score=-1.960 total time=   0.0s\n",
      "[CV 2/4] END alpha=70, selection=cyclic, tol=0.01;, score=-0.065 total time=   0.0s\n",
      "[CV 3/4] END alpha=67, selection=random, tol=1e-07;, score=-0.158 total time=   0.0s\n",
      "[CV 3/4] END alpha=65, selection=random, tol=1e-07;, score=-0.157 total time=   0.0s\n",
      "[CV 1/4] END alpha=63, selection=cyclic, tol=0.01;, score=-0.757 total time=   0.0s\n",
      "[CV 4/4] END alpha=69, selection=cyclic, tol=1e-05;, score=-2.106 total time=   0.0s\n",
      "[CV 3/4] END alpha=60, selection=random, tol=1e-07;, score=-0.157 total time=   0.0s\n",
      "[CV 1/4] END alpha=59, selection=random, tol=1e-05;, score=-0.761 total time=   0.0s\n",
      "[CV 2/4] END alpha=64, selection=random, tol=1e-05;, score=-0.063 total time=   0.0s\n",
      "[CV 1/4] END alpha=71, selection=random, tol=1e-07;, score=-0.745 total time=   0.0s\n",
      "[CV 4/4] END alpha=66, selection=cyclic, tol=0.01;, score=-2.062 total time=   0.0s\n",
      "[CV 4/4] END alpha=60, selection=random, tol=1e-07;, score=-1.975 total time=   0.0s\n",
      "[CV 2/4] END alpha=59, selection=random, tol=1e-05;, score=-0.061 total time=   0.0s\n",
      "[CV 2/4] END alpha=63, selection=cyclic, tol=0.01;, score=-0.062 total time=   0.0s\n",
      "[CV 3/4] END alpha=70, selection=cyclic, tol=0.01;, score=-0.158 total time=   0.0s\n",
      "[CV 3/4] END alpha=64, selection=random, tol=1e-05;, score=-0.157 total time=   0.0s\n",
      "[CV 2/4] END alpha=71, selection=random, tol=1e-07;, score=-0.066 total time=   0.0s\n",
      "[CV 1/4] END alpha=69, selection=cyclic, tol=1e-07;, score=-0.748 total time=   0.0s\n",
      "[CV 1/4] END alpha=66, selection=random, tol=1e-05;, score=-0.752 total time=   0.0s\n",
      "[CV 4/4] END alpha=65, selection=random, tol=1e-07;, score=-2.048 total time=   0.0s\n",
      "[CV 1/4] END alpha=61, selection=random, tol=1e-07;, score=-0.758 total time=   0.0s\n",
      "[CV 1/4] END alpha=60, selection=random, tol=0.01;, score=-0.761 total time=   0.0s\n",
      "[CV 2/4] END alpha=54, selection=cyclic, tol=1e-05;, score=-0.059 total time=   0.0s\n",
      "[CV 3/4] END alpha=63, selection=cyclic, tol=0.01;, score=-0.157 total time=   0.0s\n",
      "[CV 4/4] END alpha=64, selection=random, tol=1e-05;, score=-2.033 total time=   0.0s\n",
      "[CV 4/4] END alpha=70, selection=cyclic, tol=0.01;, score=-2.121 total time=   0.0s\n",
      "[CV 4/4] END alpha=67, selection=random, tol=1e-07;, score=-2.077 total time=   0.0s\n",
      "[CV 2/4] END alpha=61, selection=random, tol=1e-07;, score=-0.062 total time=   0.0s\n",
      "[CV 1/4] END alpha=64, selection=random, tol=1e-07;, score=-0.754 total time=   0.0s\n",
      "[CV 2/4] END alpha=66, selection=random, tol=1e-05;, score=-0.064 total time=   0.0s\n",
      "[CV 1/4] END alpha=70, selection=random, tol=1e-05;, score=-0.746 total time=   0.0s\n",
      "[CV 1/4] END alpha=65, selection=random, tol=0.01;, score=-0.743 total time=   0.0s\n",
      "[CV 3/4] END alpha=59, selection=random, tol=1e-05;, score=-0.157 total time=   0.0s\n",
      "[CV 2/4] END alpha=60, selection=random, tol=0.01;, score=-0.058 total time=   0.0s\n",
      "[CV 4/4] END alpha=63, selection=cyclic, tol=0.01;, score=-2.019 total time=   0.0s\n",
      "[CV 3/4] END alpha=54, selection=cyclic, tol=1e-05;, score=-0.160 total time=   0.0s\n",
      "[CV 3/4] END alpha=71, selection=random, tol=1e-07;, score=-0.158 total time=   0.0s\n",
      "[CV 2/4] END alpha=69, selection=cyclic, tol=1e-07;, score=-0.065 total time=   0.0s\n",
      "[CV 4/4] END alpha=59, selection=random, tol=1e-05;, score=-1.959 total time=   0.0s\n",
      "[CV 2/4] END alpha=70, selection=random, tol=1e-05;, score=-0.065 total time=   0.0s\n",
      "[CV 2/4] END alpha=64, selection=random, tol=1e-07;, score=-0.063 total time=   0.0s\n",
      "[CV 3/4] END alpha=60, selection=random, tol=0.01;, score=-0.157 total time=   0.0s\n",
      "[CV 2/4] END alpha=65, selection=random, tol=0.01;, score=-0.060 total time=   0.0s\n",
      "[CV 1/4] END alpha=67, selection=random, tol=0.01;, score=-0.767 total time=   0.0s\n",
      "[CV 1/4] END alpha=63, selection=random, tol=1e-05;, score=-0.756 total time=   0.0s\n",
      "[CV 4/4] END alpha=60, selection=random, tol=0.01;, score=-1.975 total time=   0.0s\n",
      "[CV 3/4] END alpha=70, selection=random, tol=1e-05;, score=-0.158 total time=   0.0s\n",
      "[CV 3/4] END alpha=61, selection=random, tol=1e-07;, score=-0.157 total time=   0.0s\n",
      "[CV 3/4] END alpha=69, selection=cyclic, tol=1e-07;, score=-0.158 total time=   0.0s\n",
      "[CV 3/4] END alpha=66, selection=random, tol=1e-05;, score=-0.158 total time=   0.0s\n",
      "[CV 1/4] END alpha=73, selection=cyclic, tol=1e-05;, score=-0.743 total time=   0.0s\n",
      "[CV 3/4] END alpha=64, selection=random, tol=1e-07;, score=-0.157 total time=   0.0s\n",
      "[CV 4/4] END alpha=70, selection=random, tol=1e-05;, score=-2.121 total time=   0.0s\n",
      "[CV 4/4] END alpha=54, selection=cyclic, tol=1e-05;, score=-1.883 total time=   0.0s\n",
      "[CV 2/4] END alpha=63, selection=random, tol=1e-05;, score=-0.062 total time=   0.0s\n",
      "[CV 4/4] END alpha=61, selection=random, tol=1e-07;, score=-1.990 total time=   0.0s\n",
      "[CV 3/4] END alpha=65, selection=random, tol=0.01;, score=-0.158 total time=   0.0s\n",
      "[CV 4/4] END alpha=69, selection=cyclic, tol=1e-07;, score=-2.106 total time=   0.0s\n",
      "[CV 4/4] END alpha=71, selection=random, tol=1e-07;, score=-2.136 total time=   0.0s\n",
      "[CV 2/4] END alpha=73, selection=cyclic, tol=1e-05;, score=-0.066 total time=   0.0s\n",
      "[CV 4/4] END alpha=64, selection=random, tol=1e-07;, score=-2.033 total time=   0.0s\n",
      "[CV 1/4] END alpha=54, selection=cyclic, tol=1e-07;, score=-0.768 total time=   0.0s\n",
      "[CV 1/4] END alpha=70, selection=random, tol=1e-07;, score=-0.746 total time=   0.0s\n",
      "[CV 3/4] END alpha=63, selection=random, tol=1e-05;, score=-0.157 total time=   0.0s[CV 4/4] END alpha=66, selection=random, tol=1e-05;, score=-2.062 total time=   0.0s\n",
      "\n",
      "[CV 1/4] END alpha=69, selection=cyclic, tol=0.01;, score=-0.748 total time=   0.0s\n",
      "[CV 4/4] END alpha=65, selection=random, tol=0.01;, score=-2.048 total time=   0.0s\n",
      "[CV 2/4] END alpha=54, selection=cyclic, tol=1e-07;, score=-0.059 total time=   0.0s\n",
      "[CV 1/4] END alpha=71, selection=random, tol=0.01;, score=-0.744 total time=   0.0s\n",
      "[CV 3/4] END alpha=73, selection=cyclic, tol=1e-05;, score=-0.158 total time=   0.0s\n",
      "[CV 1/4] END alpha=64, selection=random, tol=0.01;, score=-0.752 total time=   0.0s\n",
      "[CV 2/4] END alpha=70, selection=random, tol=1e-07;, score=-0.065 total time=   0.0s\n",
      "[CV 2/4] END alpha=69, selection=cyclic, tol=0.01;, score=-0.065 total time=   0.0s\n",
      "[CV 4/4] END alpha=63, selection=random, tol=1e-05;, score=-2.019 total time=   0.0s\n",
      "[CV 4/4] END alpha=73, selection=cyclic, tol=1e-05;, score=-2.165 total time=   0.0s\n",
      "[CV 1/4] END alpha=66, selection=cyclic, tol=1e-05;, score=-0.752 total time=   0.0s\n",
      "[CV 1/4] END alpha=61, selection=random, tol=0.01;, score=-0.758 total time=   0.0s\n",
      "[CV 3/4] END alpha=70, selection=random, tol=1e-07;, score=-0.158 total time=   0.0s\n",
      "[CV 1/4] END alpha=74, selection=cyclic, tol=0.01;, score=-0.742 total time=   0.0s\n",
      "[CV 3/4] END alpha=69, selection=cyclic, tol=0.01;, score=-0.158 total time=   0.0s\n",
      "[CV 1/4] END alpha=66, selection=random, tol=1e-07;, score=-0.752 total time=   0.0s\n",
      "[CV 2/4] END alpha=64, selection=random, tol=0.01;, score=-0.063 total time=   0.0s\n",
      "[CV 2/4] END alpha=74, selection=cyclic, tol=0.01;, score=-0.067 total time=   0.0s\n",
      "[CV 2/4] END alpha=71, selection=random, tol=0.01;, score=-0.066 total time=   0.0s\n",
      "[CV 1/4] END alpha=73, selection=cyclic, tol=1e-07;, score=-0.743 total time=   0.0s\n",
      "[CV 4/4] END alpha=70, selection=random, tol=1e-07;, score=-2.121 total time=   0.0s\n",
      "[CV 3/4] END alpha=54, selection=cyclic, tol=1e-07;, score=-0.160 total time=   0.0s\n",
      "[CV 2/4] END alpha=67, selection=random, tol=0.01;, score=-0.065 total time=   0.0s\n",
      "[CV 4/4] END alpha=69, selection=cyclic, tol=0.01;, score=-2.106 total time=   0.0s\n",
      "[CV 3/4] END alpha=64, selection=random, tol=0.01;, score=-0.159 total time=   0.0s\n",
      "[CV 3/4] END alpha=74, selection=cyclic, tol=0.01;, score=-0.159 total time=   0.0s\n",
      "[CV 2/4] END alpha=73, selection=cyclic, tol=1e-07;, score=-0.066 total time=   0.0s\n",
      "[CV 1/4] END alpha=70, selection=random, tol=0.01;, score=-0.739 total time=   0.0s\n",
      "[CV 1/4] END alpha=77, selection=cyclic, tol=1e-05;, score=-0.737 total time=   0.0s\n",
      "[CV 4/4] END alpha=74, selection=cyclic, tol=0.01;, score=-2.180 total time=   0.0s\n",
      "[CV 1/4] END alpha=69, selection=random, tol=1e-05;, score=-0.748 total time=   0.0s\n",
      "[CV 4/4] END alpha=64, selection=random, tol=0.01;, score=-2.033 total time=   0.0s\n",
      "[CV 2/4] END alpha=61, selection=random, tol=0.01;, score=-0.062 total time=   0.0s\n",
      "[CV 2/4] END alpha=70, selection=random, tol=0.01;, score=-0.065 total time=   0.0s\n",
      "[CV 3/4] END alpha=73, selection=cyclic, tol=1e-07;, score=-0.158 total time=   0.0s\n",
      "[CV 1/4] END alpha=74, selection=random, tol=1e-05;, score=-0.741 total time=   0.0s\n",
      "[CV 2/4] END alpha=77, selection=cyclic, tol=1e-05;, score=-0.068 total time=   0.0s\n",
      "[CV 1/4] END alpha=75, selection=random, tol=1e-07;, score=-0.740 total time=   0.0s\n",
      "[CV 3/4] END alpha=67, selection=random, tol=0.01;, score=-0.159 total time=   0.0s\n",
      "[CV 2/4] END alpha=69, selection=random, tol=1e-05;, score=-0.065 total time=   0.0s\n",
      "[CV 3/4] END alpha=61, selection=random, tol=0.01;, score=-0.155 total time=   0.0s\n",
      "[CV 2/4] END alpha=74, selection=random, tol=1e-05;, score=-0.067 total time=   0.0s\n",
      "[CV 4/4] END alpha=54, selection=cyclic, tol=1e-07;, score=-1.883 total time=   0.0s\n",
      "[CV 3/4] END alpha=70, selection=random, tol=0.01;, score=-0.156 total time=   0.0s\n",
      "[CV 2/4] END alpha=66, selection=cyclic, tol=1e-05;, score=-0.064 total time=   0.0s\n",
      "[CV 2/4] END alpha=75, selection=random, tol=1e-07;, score=-0.067 total time=   0.0s\n",
      "[CV 1/4] END alpha=78, selection=cyclic, tol=0.01;, score=-0.737 total time=   0.0s\n",
      "[CV 4/4] END alpha=73, selection=cyclic, tol=1e-07;, score=-2.165 total time=   0.0s\n",
      "[CV 3/4] END alpha=77, selection=cyclic, tol=1e-05;, score=-0.159 total time=   0.0s\n",
      "[CV 3/4] END alpha=69, selection=random, tol=1e-05;, score=-0.158 total time=   0.0s\n",
      "[CV 3/4] END alpha=74, selection=random, tol=1e-05;, score=-0.159 total time=   0.0s\n",
      "[CV 3/4] END alpha=71, selection=random, tol=0.01;, score=-0.158 total time=   0.0s\n",
      "[CV 4/4] END alpha=67, selection=random, tol=0.01;, score=-2.077 total time=   0.0s\n",
      "[CV 4/4] END alpha=74, selection=random, tol=1e-05;, score=-2.180 total time=   0.0s\n",
      "[CV 3/4] END alpha=75, selection=random, tol=1e-07;, score=-0.159 total time=   0.0s\n",
      "[CV 4/4] END alpha=77, selection=cyclic, tol=1e-05;, score=-2.225 total time=   0.0s\n",
      "[CV 1/4] END alpha=79, selection=random, tol=1e-07;, score=-0.735 total time=   0.0s\n",
      "[CV 4/4] END alpha=69, selection=random, tol=1e-05;, score=-2.106 total time=   0.0s\n",
      "[CV 4/4] END alpha=71, selection=random, tol=0.01;, score=-2.136 total time=   0.0s\n",
      "[CV 2/4] END alpha=66, selection=random, tol=1e-07;, score=-0.064 total time=   0.0s\n",
      "[CV 4/4] END alpha=70, selection=random, tol=0.01;, score=-2.121 total time=   0.0s\n",
      "[CV 3/4] END alpha=66, selection=cyclic, tol=1e-05;, score=-0.158 total time=   0.0s\n",
      "[CV 1/4] END alpha=74, selection=random, tol=1e-07;, score=-0.741 total time=   0.0s\n",
      "[CV 1/4] END alpha=81, selection=cyclic, tol=1e-05;, score=-0.732 total time=   0.0s\n",
      "[CV 1/4] END alpha=77, selection=cyclic, tol=1e-07;, score=-0.737 total time=   0.0s\n",
      "[CV 4/4] END alpha=61, selection=random, tol=0.01;, score=-1.990 total time=   0.0s\n",
      "[CV 2/4] END alpha=78, selection=cyclic, tol=0.01;, score=-0.068 total time=   0.0s\n",
      "[CV 1/4] END alpha=69, selection=random, tol=1e-07;, score=-0.748 total time=   0.0s\n",
      "[CV 1/4] END alpha=73, selection=cyclic, tol=0.01;, score=-0.743 total time=   0.0s\n",
      "[CV 1/4] END alpha=71, selection=cyclic, tol=1e-05;, score=-0.745 total time=   0.0s\n",
      "[CV 2/4] END alpha=74, selection=random, tol=1e-07;, score=-0.067 total time=   0.0s\n",
      "[CV 4/4] END alpha=66, selection=cyclic, tol=1e-05;, score=-2.062 total time=   0.0s\n",
      "[CV 2/4] END alpha=77, selection=cyclic, tol=1e-07;, score=-0.068 total time=   0.0s\n",
      "[CV 1/4] END alpha=62, selection=cyclic, tol=1e-05;, score=-0.757 total time=   0.0s\n",
      "[CV 1/4] END alpha=68, selection=cyclic, tol=1e-05;, score=-0.749 total time=   0.0s\n",
      "[CV 3/4] END alpha=78, selection=cyclic, tol=0.01;, score=-0.159 total time=   0.0s\n",
      "[CV 3/4] END alpha=74, selection=random, tol=1e-07;, score=-0.159 total time=   0.0s\n",
      "[CV 2/4] END alpha=81, selection=cyclic, tol=1e-05;, score=-0.070 total time=   0.0s\n",
      "[CV 2/4] END alpha=69, selection=random, tol=1e-07;, score=-0.065 total time=   0.0s\n",
      "[CV 1/4] END alpha=72, selection=cyclic, tol=1e-05;, score=-0.744 total time=   0.0s\n",
      "[CV 2/4] END alpha=71, selection=cyclic, tol=1e-05;, score=-0.066 total time=   0.0s\n",
      "[CV 2/4] END alpha=73, selection=cyclic, tol=0.01;, score=-0.066 total time=   0.0s\n",
      "[CV 4/4] END alpha=74, selection=random, tol=1e-07;, score=-2.180 total time=   0.0s\n",
      "[CV 4/4] END alpha=75, selection=random, tol=1e-07;, score=-2.195 total time=   0.0s\n",
      "[CV 3/4] END alpha=77, selection=cyclic, tol=1e-07;, score=-0.159 total time=   0.0s\n",
      "[CV 1/4] END alpha=66, selection=cyclic, tol=1e-07;, score=-0.752 total time=   0.0s\n",
      "[CV 3/4] END alpha=66, selection=random, tol=1e-07;, score=-0.158 total time=   0.0s\n",
      "[CV 3/4] END alpha=69, selection=random, tol=1e-07;, score=-0.158 total time=   0.0s\n",
      "[CV 2/4] END alpha=72, selection=cyclic, tol=1e-05;, score=-0.066 total time=   0.0s\n",
      "[CV 4/4] END alpha=78, selection=cyclic, tol=0.01;, score=-2.240 total time=   0.0s\n",
      "[CV 1/4] END alpha=74, selection=random, tol=0.01;, score=-0.741 total time=   0.0s[CV 3/4] END alpha=71, selection=cyclic, tol=1e-05;, score=-0.158 total time=   0.0s\n",
      "\n",
      "[CV 3/4] END alpha=73, selection=cyclic, tol=0.01;, score=-0.158 total time=   0.0s\n",
      "[CV 4/4] END alpha=77, selection=cyclic, tol=1e-07;, score=-2.225 total time=   0.0s\n",
      "[CV 1/4] END alpha=82, selection=cyclic, tol=0.01;, score=-0.731 total time=   0.0s\n",
      "[CV 3/4] END alpha=81, selection=cyclic, tol=1e-05;, score=-0.159 total time=   0.0s\n",
      "[CV 4/4] END alpha=69, selection=random, tol=1e-07;, score=-2.106 total time=   0.0s\n",
      "[CV 2/4] END alpha=68, selection=cyclic, tol=1e-05;, score=-0.064 total time=   0.0s\n",
      "[CV 2/4] END alpha=74, selection=random, tol=0.01;, score=-0.067 total time=   0.0s[CV 2/4] END alpha=66, selection=cyclic, tol=1e-07;, score=-0.064 total time=   0.0s\n",
      "\n",
      "[CV 4/4] END alpha=71, selection=cyclic, tol=1e-05;, score=-2.136 total time=   0.0s\n",
      "[CV 4/4] END alpha=73, selection=cyclic, tol=0.01;, score=-2.165 total time=   0.0s\n",
      "[CV 1/4] END alpha=75, selection=random, tol=0.01;, score=-0.728 total time=   0.0s\n",
      "[CV 2/4] END alpha=62, selection=cyclic, tol=1e-05;, score=-0.062 total time=   0.0s\n",
      "[CV 2/4] END alpha=82, selection=cyclic, tol=0.01;, score=-0.070 total time=   0.0s\n",
      "[CV 1/4] END alpha=78, selection=random, tol=1e-05;, score=-0.736 total time=   0.0s\n",
      "[CV 1/4] END alpha=69, selection=random, tol=0.01;, score=-0.764 total time=   0.0s\n",
      "[CV 2/4] END alpha=79, selection=random, tol=1e-07;, score=-0.069 total time=   0.0s\n",
      "[CV 3/4] END alpha=74, selection=random, tol=0.01;, score=-0.157 total time=   0.0s\n",
      "[CV 3/4] END alpha=68, selection=cyclic, tol=1e-05;, score=-0.158 total time=   0.0s[CV 1/4] END alpha=77, selection=cyclic, tol=0.01;, score=-0.738 total time=   0.0s\n",
      "\n",
      "[CV 4/4] END alpha=81, selection=cyclic, tol=1e-05;, score=-2.286 total time=   0.0s\n",
      "[CV 3/4] END alpha=72, selection=cyclic, tol=1e-05;, score=-0.158 total time=   0.0s\n",
      "[CV 1/4] END alpha=71, selection=cyclic, tol=1e-07;, score=-0.745 total time=   0.0s\n",
      "[CV 4/4] END alpha=66, selection=random, tol=1e-07;, score=-2.062 total time=   0.0s\n",
      "[CV 1/4] END alpha=73, selection=random, tol=1e-05;, score=-0.742 total time=   0.0s\n",
      "[CV 4/4] END alpha=74, selection=random, tol=0.01;, score=-2.180 total time=   0.0s[CV 2/4] END alpha=78, selection=random, tol=1e-05;, score=-0.069 total time=   0.0s\n",
      "\n",
      "[CV 2/4] END alpha=69, selection=random, tol=0.01;, score=-0.065 total time=   0.0s\n",
      "[CV 3/4] END alpha=82, selection=cyclic, tol=0.01;, score=-0.160 total time=   0.0s\n",
      "[CV 3/4] END alpha=66, selection=cyclic, tol=1e-07;, score=-0.158 total time=   0.0s[CV 3/4] END alpha=79, selection=random, tol=1e-07;, score=-0.159 total time=   0.0s\n",
      "\n",
      "[CV 2/4] END alpha=75, selection=random, tol=0.01;, score=-0.069 total time=   0.0s\n",
      "[CV 4/4] END alpha=68, selection=cyclic, tol=1e-05;, score=-2.092 total time=   0.0s\n",
      "[CV 2/4] END alpha=73, selection=random, tol=1e-05;, score=-0.066 total time=   0.0s\n",
      "[CV 2/4] END alpha=77, selection=cyclic, tol=0.01;, score=-0.068 total time=   0.0s\n",
      "[CV 3/4] END alpha=62, selection=cyclic, tol=1e-05;, score=-0.157 total time=   0.0s\n",
      "[CV 1/4] END alpha=75, selection=cyclic, tol=1e-05;, score=-0.740 total time=   0.0s\n",
      "[CV 3/4] END alpha=78, selection=random, tol=1e-05;, score=-0.159 total time=   0.0s\n",
      "[CV 3/4] END alpha=69, selection=random, tol=0.01;, score=-0.158 total time=   0.0s\n",
      "[CV 3/4] END alpha=75, selection=random, tol=0.01;, score=-0.159 total time=   0.0s\n",
      "[CV 1/4] END alpha=66, selection=random, tol=0.01;, score=-0.737 total time=   0.0s\n",
      "[CV 4/4] END alpha=82, selection=cyclic, tol=0.01;, score=-2.301 total time=   0.0s\n",
      "[CV 4/4] END alpha=79, selection=random, tol=1e-07;, score=-2.255 total time=   0.0s\n",
      "[CV 2/4] END alpha=75, selection=cyclic, tol=1e-05;, score=-0.067 total time=   0.0s\n",
      "[CV 4/4] END alpha=72, selection=cyclic, tol=1e-05;, score=-2.150 total time=   0.0s\n",
      "[CV 3/4] END alpha=73, selection=random, tol=1e-05;, score=-0.158 total time=   0.0s\n",
      "[CV 1/4] END alpha=68, selection=cyclic, tol=1e-07;, score=-0.749 total time=   0.0s\n",
      "[CV 2/4] END alpha=71, selection=cyclic, tol=1e-07;, score=-0.066 total time=   0.0s\n",
      "[CV 4/4] END alpha=78, selection=random, tol=1e-05;, score=-2.240 total time=   0.0s\n",
      "[CV 1/4] END alpha=81, selection=cyclic, tol=1e-07;, score=-0.732 total time=   0.0s\n",
      "[CV 4/4] END alpha=69, selection=random, tol=0.01;, score=-2.106 total time=   0.0s\n",
      "[CV 3/4] END alpha=75, selection=cyclic, tol=1e-05;, score=-0.159 total time=   0.0s\n",
      "[CV 2/4] END alpha=66, selection=random, tol=0.01;, score=-0.064 total time=   0.0s\n",
      "[CV 1/4] END alpha=72, selection=cyclic, tol=1e-07;, score=-0.744 total time=   0.0s\n",
      "[CV 1/4] END alpha=79, selection=random, tol=0.01;, score=-0.732 total time=   0.0s\n",
      "[CV 1/4] END alpha=82, selection=random, tol=1e-05;, score=-0.731 total time=   0.0s\n",
      "[CV 4/4] END alpha=73, selection=random, tol=1e-05;, score=-2.165 total time=   0.0s\n",
      "[CV 2/4] END alpha=68, selection=cyclic, tol=1e-07;, score=-0.064 total time=   0.0s\n",
      "[CV 4/4] END alpha=75, selection=random, tol=0.01;, score=-2.195 total time=   0.0s\n",
      "[CV 1/4] END alpha=78, selection=random, tol=1e-07;, score=-0.736 total time=   0.0s[CV 2/4] END alpha=81, selection=cyclic, tol=1e-07;, score=-0.070 total time=   0.0s\n",
      "\n",
      "[CV 3/4] END alpha=77, selection=cyclic, tol=0.01;, score=-0.159 total time=   0.0s\n",
      "[CV 1/4] END alpha=70, selection=cyclic, tol=1e-05;, score=-0.746 total time=   0.0s\n",
      "[CV 4/4] END alpha=75, selection=cyclic, tol=1e-05;, score=-2.195 total time=   0.0s\n",
      "[CV 3/4] END alpha=71, selection=cyclic, tol=1e-07;, score=-0.158 total time=   0.0s\n",
      "[CV 4/4] END alpha=62, selection=cyclic, tol=1e-05;, score=-2.005 total time=   0.0s\n",
      "[CV 2/4] END alpha=79, selection=random, tol=0.01;, score=-0.069 total time=   0.0s\n",
      "[CV 3/4] END alpha=81, selection=cyclic, tol=1e-07;, score=-0.159 total time=   0.0s\n",
      "[CV 1/4] END alpha=73, selection=random, tol=1e-07;, score=-0.743 total time=   0.0s\n",
      "[CV 4/4] END alpha=66, selection=cyclic, tol=1e-07;, score=-2.062 total time=   0.0s\n",
      "[CV 3/4] END alpha=68, selection=cyclic, tol=1e-07;, score=-0.158 total time=   0.0s\n",
      "[CV 3/4] END alpha=66, selection=random, tol=0.01;, score=-0.158 total time=   0.0s\n",
      "[CV 4/4] END alpha=77, selection=cyclic, tol=0.01;, score=-2.225 total time=   0.0s[CV 1/4] END alpha=75, selection=cyclic, tol=1e-07;, score=-0.740 total time=   0.0s\n",
      "\n",
      "[CV 2/4] END alpha=70, selection=cyclic, tol=1e-05;, score=-0.065 total time=   0.0s\n",
      "[CV 4/4] END alpha=71, selection=cyclic, tol=1e-07;, score=-2.136 total time=   0.0s\n",
      "[CV 1/4] END alpha=83, selection=random, tol=1e-07;, score=-0.730 total time=   0.0s\n",
      "[CV 2/4] END alpha=78, selection=random, tol=1e-07;, score=-0.069 total time=   0.0s\n",
      "[CV 2/4] END alpha=82, selection=random, tol=1e-05;, score=-0.070 total time=   0.0s\n",
      "[CV 4/4] END alpha=81, selection=cyclic, tol=1e-07;, score=-2.286 total time=   0.0s\n",
      "[CV 3/4] END alpha=79, selection=random, tol=0.01;, score=-0.159 total time=   0.0s\n",
      "[CV 2/4] END alpha=73, selection=random, tol=1e-07;, score=-0.066 total time=   0.0s\n",
      "[CV 4/4] END alpha=68, selection=cyclic, tol=1e-07;, score=-2.092 total time=   0.0s\n",
      "[CV 2/4] END alpha=75, selection=cyclic, tol=1e-07;, score=-0.067 total time=   0.0s\n",
      "[CV 4/4] END alpha=66, selection=random, tol=0.01;, score=-2.062 total time=   0.0s\n",
      "[CV 3/4] END alpha=70, selection=cyclic, tol=1e-05;, score=-0.158 total time=   0.0s\n",
      "[CV 1/4] END alpha=77, selection=random, tol=1e-05;, score=-0.737 total time=   0.0s\n",
      "[CV 1/4] END alpha=71, selection=cyclic, tol=0.01;, score=-0.746 total time=   0.0s\n",
      "[CV 2/4] END alpha=72, selection=cyclic, tol=1e-07;, score=-0.066 total time=   0.0s\n",
      "[CV 1/4] END alpha=81, selection=cyclic, tol=0.01;, score=-0.733 total time=   0.0s\n",
      "[CV 2/4] END alpha=83, selection=random, tol=1e-07;, score=-0.071 total time=   0.0s\n",
      "[CV 1/4] END alpha=62, selection=cyclic, tol=1e-07;, score=-0.757 total time=   0.0s\n",
      "[CV 3/4] END alpha=78, selection=random, tol=1e-07;, score=-0.159 total time=   0.0s\n",
      "[CV 3/4] END alpha=73, selection=random, tol=1e-07;, score=-0.158 total time=   0.0s\n",
      "[CV 1/4] END alpha=68, selection=cyclic, tol=0.01;, score=-0.750 total time=   0.0s\n",
      "[CV 4/4] END alpha=79, selection=random, tol=0.01;, score=-2.255 total time=   0.0s\n",
      "[CV 3/4] END alpha=75, selection=cyclic, tol=1e-07;, score=-0.159 total time=   0.0s\n",
      "[CV 4/4] END alpha=70, selection=cyclic, tol=1e-05;, score=-2.121 total time=   0.0s\n",
      "[CV 2/4] END alpha=81, selection=cyclic, tol=0.01;, score=-0.070 total time=   0.0s\n",
      "[CV 2/4] END alpha=77, selection=random, tol=1e-05;, score=-0.068 total time=   0.0s\n",
      "[CV 2/4] END alpha=71, selection=cyclic, tol=0.01;, score=-0.066 total time=   0.0s\n",
      "[CV 1/4] END alpha=67, selection=cyclic, tol=1e-05;, score=-0.750 total time=   0.0s\n",
      "[CV 2/4] END alpha=62, selection=cyclic, tol=1e-07;, score=-0.062 total time=   0.0s\n",
      "[CV 3/4] END alpha=83, selection=random, tol=1e-07;, score=-0.160 total time=   0.0s[CV 3/4] END alpha=82, selection=random, tol=1e-05;, score=-0.160 total time=   0.0s\n",
      "\n",
      "[CV 4/4] END alpha=73, selection=random, tol=1e-07;, score=-2.165 total time=   0.0s\n",
      "[CV 4/4] END alpha=78, selection=random, tol=1e-07;, score=-2.240 total time=   0.0s\n",
      "[CV 2/4] END alpha=68, selection=cyclic, tol=0.01;, score=-0.064 total time=   0.0s\n",
      "[CV 3/4] END alpha=81, selection=cyclic, tol=0.01;, score=-0.159 total time=   0.0s\n",
      "[CV 4/4] END alpha=75, selection=cyclic, tol=1e-07;, score=-2.195 total time=   0.0s\n",
      "[CV 1/4] END alpha=70, selection=cyclic, tol=1e-07;, score=-0.746 total time=   0.0s\n",
      "[CV 3/4] END alpha=77, selection=random, tol=1e-05;, score=-0.159 total time=   0.0s\n",
      "[CV 1/4] END alpha=76, selection=cyclic, tol=1e-05;, score=-0.739 total time=   0.0s\n",
      "[CV 3/4] END alpha=71, selection=cyclic, tol=0.01;, score=-0.158 total time=   0.0s\n",
      "[CV 1/4] END alpha=85, selection=cyclic, tol=1e-05;, score=-0.727 total time=   0.0s\n",
      "[CV 2/4] END alpha=67, selection=cyclic, tol=1e-05;, score=-0.064 total time=   0.0s\n",
      "[CV 4/4] END alpha=81, selection=cyclic, tol=0.01;, score=-2.286 total time=   0.0s\n",
      "[CV 1/4] END alpha=73, selection=random, tol=0.01;, score=-0.733 total time=   0.0s\n",
      "[CV 3/4] END alpha=72, selection=cyclic, tol=1e-07;, score=-0.158 total time=   0.0s\n",
      "[CV 4/4] END alpha=83, selection=random, tol=1e-07;, score=-2.316 total time=   0.0s\n",
      "[CV 1/4] END alpha=80, selection=cyclic, tol=1e-05;, score=-0.733 total time=   0.0s\n",
      "[CV 3/4] END alpha=68, selection=cyclic, tol=0.01;, score=-0.158 total time=   0.0s\n",
      "[CV 2/4] END alpha=70, selection=cyclic, tol=1e-07;, score=-0.065 total time=   0.0s\n",
      "[CV 1/4] END alpha=75, selection=cyclic, tol=0.01;, score=-0.741 total time=   0.0s\n",
      "[CV 4/4] END alpha=77, selection=random, tol=1e-05;, score=-2.225 total time=   0.0s\n",
      "[CV 1/4] END alpha=78, selection=random, tol=0.01;, score=-0.737 total time=   0.0s\n",
      "[CV 4/4] END alpha=71, selection=cyclic, tol=0.01;, score=-2.136 total time=   0.0s\n",
      "[CV 1/4] END alpha=81, selection=random, tol=1e-05;, score=-0.732 total time=   0.0s\n",
      "[CV 2/4] END alpha=85, selection=cyclic, tol=1e-05;, score=-0.071 total time=   0.0s\n",
      "[CV 2/4] END alpha=73, selection=random, tol=0.01;, score=-0.063 total time=   0.0s\n",
      "[CV 1/4] END alpha=83, selection=random, tol=0.01;, score=-0.729 total time=   0.0s\n",
      "[CV 2/4] END alpha=80, selection=cyclic, tol=1e-05;, score=-0.069 total time=   0.0s[CV 2/4] END alpha=75, selection=cyclic, tol=0.01;, score=-0.067 total time=   0.0s\n",
      "[CV 3/4] END alpha=70, selection=cyclic, tol=1e-07;, score=-0.158 total time=   0.0s\n",
      "\n",
      "[CV 1/4] END alpha=77, selection=random, tol=1e-07;, score=-0.737 total time=   0.0s[CV 4/4] END alpha=68, selection=cyclic, tol=0.01;, score=-2.092 total time=   0.0s\n",
      "\n",
      "[CV 1/4] END alpha=71, selection=random, tol=1e-05;, score=-0.745 total time=   0.0s\n",
      "[CV 3/4] END alpha=67, selection=cyclic, tol=1e-05;, score=-0.158 total time=   0.0s\n",
      "[CV 3/4] END alpha=62, selection=cyclic, tol=1e-07;, score=-0.157 total time=   0.0s\n",
      "[CV 2/4] END alpha=81, selection=random, tol=1e-05;, score=-0.070 total time=   0.0s\n",
      "[CV 2/4] END alpha=78, selection=random, tol=0.01;, score=-0.069 total time=   0.0s\n",
      "[CV 4/4] END alpha=82, selection=random, tol=1e-05;, score=-2.301 total time=   0.0s\n",
      "[CV 3/4] END alpha=85, selection=cyclic, tol=1e-05;, score=-0.160 total time=   0.0s\n",
      "[CV 3/4] END alpha=73, selection=random, tol=0.01;, score=-0.160 total time=   0.0s\n",
      "[CV 4/4] END alpha=70, selection=cyclic, tol=1e-07;, score=-2.121 total time=   0.0s\n",
      "[CV 2/4] END alpha=77, selection=random, tol=1e-07;, score=-0.068 total time=   0.0s\n",
      "[CV 3/4] END alpha=75, selection=cyclic, tol=0.01;, score=-0.159 total time=   0.0s\n",
      "[CV 1/4] END alpha=68, selection=random, tol=1e-05;, score=-0.749 total time=   0.0s\n",
      "[CV 2/4] END alpha=76, selection=cyclic, tol=1e-05;, score=-0.068 total time=   0.0s\n",
      "[CV 2/4] END alpha=71, selection=random, tol=1e-05;, score=-0.066 total time=   0.0s\n",
      "[CV 3/4] END alpha=81, selection=random, tol=1e-05;, score=-0.159 total time=   0.0s\n",
      "[CV 4/4] END alpha=62, selection=cyclic, tol=1e-07;, score=-2.005 total time=   0.0s\n",
      "[CV 3/4] END alpha=80, selection=cyclic, tol=1e-05;, score=-0.159 total time=   0.0s\n",
      "[CV 4/4] END alpha=72, selection=cyclic, tol=1e-07;, score=-2.150 total time=   0.0s\n",
      "[CV 4/4] END alpha=85, selection=cyclic, tol=1e-05;, score=-2.347 total time=   0.0s\n",
      "[CV 4/4] END alpha=81, selection=random, tol=1e-05;, score=-2.286 total time=   0.0s\n",
      "[CV 4/4] END alpha=73, selection=random, tol=0.01;, score=-2.165 total time=   0.0s\n",
      "[CV 3/4] END alpha=77, selection=random, tol=1e-07;, score=-0.159 total time=   0.0s\n",
      "[CV 2/4] END alpha=68, selection=random, tol=1e-05;, score=-0.064 total time=   0.0s\n",
      "[CV 1/4] END alpha=82, selection=random, tol=1e-07;, score=-0.731 total time=   0.0s\n",
      "[CV 4/4] END alpha=75, selection=cyclic, tol=0.01;, score=-2.195 total time=   0.0s\n",
      "[CV 3/4] END alpha=71, selection=random, tol=1e-05;, score=-0.158 total time=   0.0s\n",
      "[CV 4/4] END alpha=80, selection=cyclic, tol=1e-05;, score=-2.271 total time=   0.0s\n",
      "[CV 2/4] END alpha=83, selection=random, tol=0.01;, score=-0.071 total time=   0.0s[CV 1/4] END alpha=72, selection=cyclic, tol=0.01;, score=-0.745 total time=   0.0s\n",
      "\n",
      "[CV 4/4] END alpha=67, selection=cyclic, tol=1e-05;, score=-2.077 total time=   0.0s\n",
      "[CV 1/4] END alpha=81, selection=random, tol=1e-07;, score=-0.732 total time=   0.0s\n",
      "[CV 1/4] END alpha=86, selection=cyclic, tol=0.01;, score=-0.726 total time=   0.0s\n",
      "[CV 1/4] END alpha=85, selection=cyclic, tol=1e-07;, score=-0.727 total time=   0.0s\n",
      "[CV 4/4] END alpha=77, selection=random, tol=1e-07;, score=-2.225 total time=   0.0s\n",
      "[CV 3/4] END alpha=68, selection=random, tol=1e-05;, score=-0.158 total time=   0.0s\n",
      "[CV 1/4] END alpha=74, selection=cyclic, tol=1e-05;, score=-0.741 total time=   0.0s\n",
      "[CV 4/4] END alpha=71, selection=random, tol=1e-05;, score=-2.136 total time=   0.0s\n",
      "[CV 3/4] END alpha=76, selection=cyclic, tol=1e-05;, score=-0.159 total time=   0.0s\n",
      "[CV 1/4] END alpha=75, selection=random, tol=1e-05;, score=-0.740 total time=   0.0s\n",
      "[CV 3/4] END alpha=78, selection=random, tol=0.01;, score=-0.156 total time=   0.0s\n",
      "[CV 2/4] END alpha=81, selection=random, tol=1e-07;, score=-0.070 total time=   0.0s\n",
      "[CV 1/4] END alpha=80, selection=cyclic, tol=1e-07;, score=-0.733 total time=   0.0s\n",
      "[CV 2/4] END alpha=86, selection=cyclic, tol=0.01;, score=-0.072 total time=   0.0s\n",
      "[CV 3/4] END alpha=83, selection=random, tol=0.01;, score=-0.160 total time=   0.0s\n",
      "[CV 1/4] END alpha=77, selection=random, tol=0.01;, score=-0.742 total time=   0.0s\n",
      "[CV 4/4] END alpha=68, selection=random, tol=1e-05;, score=-2.092 total time=   0.0s\n",
      "[CV 2/4] END alpha=85, selection=cyclic, tol=1e-07;, score=-0.071 total time=   0.0s\n",
      "[CV 2/4] END alpha=74, selection=cyclic, tol=1e-05;, score=-0.067 total time=   0.0s\n",
      "[CV 3/4] END alpha=81, selection=random, tol=1e-07;, score=-0.159 total time=   0.0s\n",
      "[CV 1/4] END alpha=67, selection=cyclic, tol=1e-07;, score=-0.750 total time=   0.0s\n",
      "[CV 2/4] END alpha=80, selection=cyclic, tol=1e-07;, score=-0.069 total time=   0.0s\n",
      "[CV 4/4] END alpha=78, selection=random, tol=0.01;, score=-2.240 total time=   0.0s\n",
      "[CV 3/4] END alpha=86, selection=cyclic, tol=0.01;, score=-0.160 total time=   0.0s\n",
      "[CV 1/4] END alpha=87, selection=random, tol=1e-07;, score=-0.724 total time=   0.0s\n",
      "[CV 4/4] END alpha=76, selection=cyclic, tol=1e-05;, score=-2.210 total time=   0.0s\n",
      "[CV 2/4] END alpha=82, selection=random, tol=1e-07;, score=-0.070 total time=   0.0s[CV 2/4] END alpha=77, selection=random, tol=0.01;, score=-0.068 total time=   0.0s\n",
      "\n",
      "[CV 1/4] END alpha=68, selection=random, tol=1e-07;, score=-0.749 total time=   0.0s\n",
      "[CV 3/4] END alpha=85, selection=cyclic, tol=1e-07;, score=-0.160 total time=   0.0s\n",
      "[CV 4/4] END alpha=81, selection=random, tol=1e-07;, score=-2.286 total time=   0.0s\n",
      "[CV 2/4] END alpha=72, selection=cyclic, tol=0.01;, score=-0.066 total time=   0.0s\n",
      "[CV 3/4] END alpha=74, selection=cyclic, tol=1e-05;, score=-0.159 total time=   0.0s[CV 4/4] END alpha=83, selection=random, tol=0.01;, score=-2.316 total time=   0.0s\n",
      "\n",
      "[CV 2/4] END alpha=75, selection=random, tol=1e-05;, score=-0.067 total time=   0.0s\n",
      "[CV 1/4] END alpha=79, selection=cyclic, tol=1e-05;, score=-0.735 total time=   0.0s\n",
      "[CV 2/4] END alpha=87, selection=random, tol=1e-07;, score=-0.072 total time=   0.0s\n",
      "[CV 4/4] END alpha=86, selection=cyclic, tol=0.01;, score=-2.363 total time=   0.0s\n",
      "[CV 3/4] END alpha=77, selection=random, tol=0.01;, score=-0.160 total time=   0.0s\n",
      "[CV 2/4] END alpha=68, selection=random, tol=1e-07;, score=-0.064 total time=   0.0s\n",
      "[CV 4/4] END alpha=85, selection=cyclic, tol=1e-07;, score=-2.347 total time=   0.0s\n",
      "[CV 1/4] END alpha=76, selection=cyclic, tol=1e-07;, score=-0.739 total time=   0.0s\n",
      "[CV 1/4] END alpha=81, selection=random, tol=0.01;, score=-0.732 total time=   0.0s\n",
      "[CV 1/4] END alpha=86, selection=random, tol=1e-05;, score=-0.726 total time=   0.0s[CV 3/4] END alpha=75, selection=random, tol=1e-05;, score=-0.159 total time=   0.0s\n",
      "\n",
      "[CV 3/4] END alpha=82, selection=random, tol=1e-07;, score=-0.160 total time=   0.0s\n",
      "[CV 3/4] END alpha=87, selection=random, tol=1e-07;, score=-0.160 total time=   0.0s\n",
      "[CV 2/4] END alpha=79, selection=cyclic, tol=1e-05;, score=-0.069 total time=   0.0s\n",
      "[CV 3/4] END alpha=68, selection=random, tol=1e-07;, score=-0.158 total time=   0.0s[CV 4/4] END alpha=77, selection=random, tol=0.01;, score=-2.225 total time=   0.0s[CV 3/4] END alpha=80, selection=cyclic, tol=1e-07;, score=-0.159 total time=   0.0s\n",
      "\n",
      "\n",
      "[CV 4/4] END alpha=74, selection=cyclic, tol=1e-05;, score=-2.180 total time=   0.0s\n",
      "[CV 2/4] END alpha=81, selection=random, tol=0.01;, score=-0.070 total time=   0.0s\n",
      "[CV 3/4] END alpha=72, selection=cyclic, tol=0.01;, score=-0.158 total time=   0.0s\n",
      "[CV 1/4] END alpha=85, selection=cyclic, tol=0.01;, score=-0.728 total time=   0.0s\n",
      "[CV 1/4] END alpha=84, selection=cyclic, tol=1e-05;, score=-0.728 total time=   0.0s\n",
      "[CV 2/4] END alpha=76, selection=cyclic, tol=1e-07;, score=-0.068 total time=   0.0s\n",
      "[CV 2/4] END alpha=86, selection=random, tol=1e-05;, score=-0.072 total time=   0.0s\n",
      "[CV 4/4] END alpha=75, selection=random, tol=1e-05;, score=-2.195 total time=   0.0s\n",
      "[CV 3/4] END alpha=79, selection=cyclic, tol=1e-05;, score=-0.159 total time=   0.0s\n",
      "[CV 4/4] END alpha=87, selection=random, tol=1e-07;, score=-2.378 total time=   0.0s\n",
      "[CV 2/4] END alpha=67, selection=cyclic, tol=1e-07;, score=-0.064 total time=   0.0s\n",
      "[CV 4/4] END alpha=80, selection=cyclic, tol=1e-07;, score=-2.271 total time=   0.0s\n",
      "[CV 1/4] END alpha=78, selection=cyclic, tol=1e-05;, score=-0.736 total time=   0.0s\n",
      "[CV 4/4] END alpha=68, selection=random, tol=1e-07;, score=-2.092 total time=   0.0s\n",
      "[CV 3/4] END alpha=81, selection=random, tol=0.01;, score=-0.159 total time=   0.0s\n",
      "[CV 1/4] END alpha=74, selection=cyclic, tol=1e-07;, score=-0.741 total time=   0.0s\n",
      "[CV 2/4] END alpha=85, selection=cyclic, tol=0.01;, score=-0.071 total time=   0.0s\n",
      "[CV 4/4] END alpha=72, selection=cyclic, tol=0.01;, score=-2.150 total time=   0.0s\n",
      "[CV 3/4] END alpha=76, selection=cyclic, tol=1e-07;, score=-0.159 total time=   0.0s\n",
      "[CV 3/4] END alpha=86, selection=random, tol=1e-05;, score=-0.160 total time=   0.0s\n",
      "[CV 1/4] END alpha=87, selection=random, tol=0.01;, score=-0.725 total time=   0.0s\n",
      "[CV 4/4] END alpha=79, selection=cyclic, tol=1e-05;, score=-2.255 total time=   0.0s\n",
      "[CV 3/4] END alpha=67, selection=cyclic, tol=1e-07;, score=-0.158 total time=   0.0s\n",
      "[CV 4/4] END alpha=81, selection=random, tol=0.01;, score=-2.286 total time=   0.0s\n",
      "[CV 2/4] END alpha=84, selection=cyclic, tol=1e-05;, score=-0.071 total time=   0.0s\n",
      "[CV 1/4] END alpha=80, selection=cyclic, tol=0.01;, score=-0.734 total time=   0.0s\n",
      "[CV 2/4] END alpha=78, selection=cyclic, tol=1e-05;, score=-0.069 total time=   0.0s\n",
      "[CV 1/4] END alpha=68, selection=random, tol=0.01;, score=-0.749 total time=   0.0s\n",
      "[CV 2/4] END alpha=74, selection=cyclic, tol=1e-07;, score=-0.067 total time=   0.0s\n",
      "[CV 3/4] END alpha=85, selection=cyclic, tol=0.01;, score=-0.160 total time=   0.0s\n",
      "[CV 4/4] END alpha=82, selection=random, tol=1e-07;, score=-2.301 total time=   0.0s\n",
      "[CV 1/4] END alpha=89, selection=cyclic, tol=1e-05;, score=-0.722 total time=   0.0s\n",
      "[CV 4/4] END alpha=86, selection=random, tol=1e-05;, score=-2.363 total time=   0.0s\n",
      "[CV 2/4] END alpha=87, selection=random, tol=0.01;, score=-0.070 total time=   0.0s\n",
      "[CV 1/4] END alpha=79, selection=cyclic, tol=1e-07;, score=-0.735 total time=   0.0s\n",
      "[CV 3/4] END alpha=78, selection=cyclic, tol=1e-05;, score=-0.159 total time=   0.0s\n",
      "[CV 1/4] END alpha=82, selection=cyclic, tol=1e-05;, score=-0.731 total time=   0.0s\n",
      "[CV 3/4] END alpha=74, selection=cyclic, tol=1e-07;, score=-0.159 total time=   0.0s[CV 2/4] END alpha=68, selection=random, tol=0.01;, score=-0.065 total time=   0.0s\n",
      "\n",
      "[CV 2/4] END alpha=80, selection=cyclic, tol=0.01;, score=-0.069 total time=   0.0s\n",
      "[CV 3/4] END alpha=84, selection=cyclic, tol=1e-05;, score=-0.160 total time=   0.0s\n",
      "[CV 2/4] END alpha=89, selection=cyclic, tol=1e-05;, score=-0.073 total time=   0.0s\n",
      "[CV 4/4] END alpha=85, selection=cyclic, tol=0.01;, score=-2.347 total time=   0.0s\n",
      "[CV 1/4] END alpha=86, selection=random, tol=1e-07;, score=-0.726 total time=   0.0s\n",
      "[CV 3/4] END alpha=87, selection=random, tol=0.01;, score=-0.161 total time=   0.0s\n",
      "[CV 1/4] END alpha=72, selection=random, tol=1e-05;, score=-0.744 total time=   0.0s\n",
      "[CV 2/4] END alpha=82, selection=cyclic, tol=1e-05;, score=-0.070 total time=   0.0s\n",
      "[CV 2/4] END alpha=79, selection=cyclic, tol=1e-07;, score=-0.069 total time=   0.0s\n",
      "[CV 4/4] END alpha=78, selection=cyclic, tol=1e-05;, score=-2.240 total time=   0.0s\n",
      "[CV 3/4] END alpha=80, selection=cyclic, tol=0.01;, score=-0.159 total time=   0.0s\n",
      "[CV 1/4] END alpha=82, selection=random, tol=0.01;, score=-0.745 total time=   0.0s\n",
      "[CV 4/4] END alpha=74, selection=cyclic, tol=1e-07;, score=-2.180 total time=   0.0s\n",
      "[CV 3/4] END alpha=68, selection=random, tol=0.01;, score=-0.161 total time=   0.0s\n",
      "[CV 4/4] END alpha=84, selection=cyclic, tol=1e-05;, score=-2.332 total time=   0.0s\n",
      "[CV 4/4] END alpha=76, selection=cyclic, tol=1e-07;, score=-2.210 total time=   0.0s\n",
      "[CV 2/4] END alpha=86, selection=random, tol=1e-07;, score=-0.072 total time=   0.0s\n",
      "[CV 3/4] END alpha=82, selection=cyclic, tol=1e-05;, score=-0.160 total time=   0.0s\n",
      "[CV 4/4] END alpha=87, selection=random, tol=0.01;, score=-2.378 total time=   0.0s\n",
      "[CV 1/4] END alpha=85, selection=random, tol=1e-05;, score=-0.727 total time=   0.0s\n",
      "[CV 1/4] END alpha=78, selection=cyclic, tol=1e-07;, score=-0.736 total time=   0.0s\n",
      "[CV 3/4] END alpha=89, selection=cyclic, tol=1e-05;, score=-0.160 total time=   0.0s\n",
      "[CV 2/4] END alpha=82, selection=random, tol=0.01;, score=-0.070 total time=   0.0s\n",
      "[CV 4/4] END alpha=80, selection=cyclic, tol=0.01;, score=-2.271 total time=   0.0s\n",
      "[CV 4/4] END alpha=68, selection=random, tol=0.01;, score=-2.092 total time=   0.0s\n",
      "[CV 4/4] END alpha=82, selection=cyclic, tol=1e-05;, score=-2.301 total time=   0.0s\n",
      "[CV 4/4] END alpha=67, selection=cyclic, tol=1e-07;, score=-2.077 total time=   0.0s\n",
      "[CV 3/4] END alpha=86, selection=random, tol=1e-07;, score=-0.160 total time=   0.0s\n",
      "[CV 3/4] END alpha=79, selection=cyclic, tol=1e-07;, score=-0.159 total time=   0.0s\n",
      "[CV 1/4] END alpha=88, selection=cyclic, tol=1e-05;, score=-0.723 total time=   0.0s\n",
      "[CV 1/4] END alpha=90, selection=cyclic, tol=0.01;, score=-0.721 total time=   0.0s\n",
      "[CV 2/4] END alpha=72, selection=random, tol=1e-05;, score=-0.066 total time=   0.0s\n",
      "[CV 2/4] END alpha=78, selection=cyclic, tol=1e-07;, score=-0.069 total time=   0.0s\n",
      "[CV 3/4] END alpha=82, selection=random, tol=0.01;, score=-0.160 total time=   0.0s\n",
      "[CV 1/4] END alpha=80, selection=random, tol=1e-05;, score=-0.733 total time=   0.0s\n",
      "[CV 1/4] END alpha=82, selection=cyclic, tol=1e-07;, score=-0.731 total time=   0.0s\n",
      "[CV 1/4] END alpha=84, selection=cyclic, tol=1e-07;, score=-0.728 total time=   0.0s\n",
      "[CV 1/4] END alpha=67, selection=cyclic, tol=0.01;, score=-0.751 total time=   0.0s[CV 4/4] END alpha=86, selection=random, tol=1e-07;, score=-2.363 total time=   0.0s\n",
      "\n",
      "[CV 1/4] END alpha=76, selection=cyclic, tol=0.01;, score=-0.739 total time=   0.0s\n",
      "[CV 2/4] END alpha=90, selection=cyclic, tol=0.01;, score=-0.073 total time=   0.0s\n",
      "[CV 2/4] END alpha=88, selection=cyclic, tol=1e-05;, score=-0.073 total time=   0.0s\n",
      "[CV 4/4] END alpha=79, selection=cyclic, tol=1e-07;, score=-2.255 total time=   0.0s\n",
      "[CV 3/4] END alpha=72, selection=random, tol=1e-05;, score=-0.158 total time=   0.0s\n",
      "[CV 4/4] END alpha=89, selection=cyclic, tol=1e-05;, score=-2.410 total time=   0.0s\n",
      "[CV 3/4] END alpha=78, selection=cyclic, tol=1e-07;, score=-0.159 total time=   0.0s\n",
      "[CV 1/4] END alpha=91, selection=random, tol=1e-07;, score=-0.719 total time=   0.0s[CV 4/4] END alpha=82, selection=random, tol=0.01;, score=-2.301 total time=   0.0s\n",
      "\n",
      "[CV 2/4] END alpha=82, selection=cyclic, tol=1e-07;, score=-0.070 total time=   0.0s\n",
      "[CV 2/4] END alpha=85, selection=random, tol=1e-05;, score=-0.071 total time=   0.0s\n",
      "[CV 1/4] END alpha=86, selection=random, tol=0.01;, score=-0.716 total time=   0.0s\n",
      "[CV 2/4] END alpha=76, selection=cyclic, tol=0.01;, score=-0.068 total time=   0.0s\n",
      "[CV 3/4] END alpha=88, selection=cyclic, tol=1e-05;, score=-0.160 total time=   0.0s\n",
      "[CV 3/4] END alpha=90, selection=cyclic, tol=0.01;, score=-0.161 total time=   0.0s\n",
      "[CV 4/4] END alpha=78, selection=cyclic, tol=1e-07;, score=-2.240 total time=   0.0s\n",
      "[CV 3/4] END alpha=82, selection=cyclic, tol=1e-07;, score=-0.160 total time=   0.0s\n",
      "[CV 2/4] END alpha=80, selection=random, tol=1e-05;, score=-0.069 total time=   0.0s\n",
      "[CV 1/4] END alpha=83, selection=cyclic, tol=1e-05;, score=-0.730 total time=   0.0s\n",
      "[CV 2/4] END alpha=91, selection=random, tol=1e-07;, score=-0.074 total time=   0.0s\n",
      "[CV 1/4] END alpha=79, selection=cyclic, tol=0.01;, score=-0.735 total time=   0.0s\n",
      "[CV 3/4] END alpha=76, selection=cyclic, tol=0.01;, score=-0.159 total time=   0.0s\n",
      "[CV 2/4] END alpha=86, selection=random, tol=0.01;, score=-0.069 total time=   0.0s\n",
      "[CV 4/4] END alpha=88, selection=cyclic, tol=1e-05;, score=-2.394 total time=   0.0s\n",
      "[CV 2/4] END alpha=67, selection=cyclic, tol=0.01;, score=-0.064 total time=   0.0s\n",
      "[CV 4/4] END alpha=82, selection=cyclic, tol=1e-07;, score=-2.301 total time=   0.0s[CV 1/4] END alpha=89, selection=cyclic, tol=1e-07;, score=-0.722 total time=   0.0s\n",
      "\n",
      "[CV 2/4] END alpha=84, selection=cyclic, tol=1e-07;, score=-0.071 total time=   0.0s\n",
      "[CV 1/4] END alpha=93, selection=cyclic, tol=1e-05;, score=-0.717 total time=   0.0s\n",
      "[CV 2/4] END alpha=83, selection=cyclic, tol=1e-05;, score=-0.071 total time=   0.0s\n",
      "[CV 4/4] END alpha=76, selection=cyclic, tol=0.01;, score=-2.210 total time=   0.0s\n",
      "[CV 3/4] END alpha=91, selection=random, tol=1e-07;, score=-0.161 total time=   0.0s\n",
      "[CV 4/4] END alpha=90, selection=cyclic, tol=0.01;, score=-2.425 total time=   0.0s\n",
      "[CV 3/4] END alpha=85, selection=random, tol=1e-05;, score=-0.160 total time=   0.0s\n",
      "[CV 3/4] END alpha=86, selection=random, tol=0.01;, score=-0.163 total time=   0.0s\n",
      "[CV 1/4] END alpha=88, selection=cyclic, tol=1e-07;, score=-0.723 total time=   0.0s\n",
      "[CV 1/4] END alpha=94, selection=cyclic, tol=0.01;, score=-0.716 total time=   0.0s\n",
      "[CV 4/4] END alpha=72, selection=random, tol=1e-05;, score=-2.150 total time=   0.0s\n",
      "[CV 2/4] END alpha=93, selection=cyclic, tol=1e-05;, score=-0.075 total time=   0.0s\n",
      "[CV 1/4] END alpha=76, selection=random, tol=1e-05;, score=-0.739 total time=   0.0s\n",
      "[CV 3/4] END alpha=80, selection=random, tol=1e-05;, score=-0.159 total time=   0.0s\n",
      "[CV 3/4] END alpha=83, selection=cyclic, tol=1e-05;, score=-0.160 total time=   0.0s\n",
      "[CV 1/4] END alpha=95, selection=random, tol=1e-07;, score=-0.714 total time=   0.0s\n",
      "[CV 4/4] END alpha=91, selection=random, tol=1e-07;, score=-2.441 total time=   0.0s\n",
      "[CV 4/4] END alpha=86, selection=random, tol=0.01;, score=-2.363 total time=   0.0s\n",
      "[CV 2/4] END alpha=88, selection=cyclic, tol=1e-07;, score=-0.073 total time=   0.0s\n",
      "[CV 2/4] END alpha=94, selection=cyclic, tol=0.01;, score=-0.075 total time=   0.0s\n",
      "[CV 4/4] END alpha=85, selection=random, tol=1e-05;, score=-2.347 total time=   0.0s\n",
      "[CV 1/4] END alpha=72, selection=random, tol=1e-07;, score=-0.744 total time=   0.0s\n",
      "[CV 1/4] END alpha=90, selection=random, tol=1e-05;, score=-0.721 total time=   0.0s\n",
      "[CV 3/4] END alpha=93, selection=cyclic, tol=1e-05;, score=-0.161 total time=   0.0s\n",
      "[CV 3/4] END alpha=84, selection=cyclic, tol=1e-07;, score=-0.160 total time=   0.0s\n",
      "[CV 2/4] END alpha=76, selection=random, tol=1e-05;, score=-0.068 total time=   0.0s\n",
      "[CV 4/4] END alpha=83, selection=cyclic, tol=1e-05;, score=-2.316 total time=   0.0s\n",
      "[CV 2/4] END alpha=95, selection=random, tol=1e-07;, score=-0.076 total time=   0.0s\n",
      "[CV 2/4] END alpha=89, selection=cyclic, tol=1e-07;, score=-0.073 total time=   0.0s\n",
      "[CV 1/4] END alpha=87, selection=cyclic, tol=1e-05;, score=-0.724 total time=   0.0s\n",
      "[CV 3/4] END alpha=88, selection=cyclic, tol=1e-07;, score=-0.160 total time=   0.0s\n",
      "[CV 3/4] END alpha=94, selection=cyclic, tol=0.01;, score=-0.161 total time=   0.0s\n",
      "[CV 4/4] END alpha=80, selection=random, tol=1e-05;, score=-2.271 total time=   0.0s\n",
      "[CV 1/4] END alpha=85, selection=random, tol=1e-07;, score=-0.727 total time=   0.0s\n",
      "[CV 1/4] END alpha=91, selection=random, tol=0.01;, score=-0.717 total time=   0.0s\n",
      "[CV 2/4] END alpha=90, selection=random, tol=1e-05;, score=-0.074 total time=   0.0s\n",
      "[CV 2/4] END alpha=79, selection=cyclic, tol=0.01;, score=-0.069 total time=   0.0s\n",
      "[CV 3/4] END alpha=95, selection=random, tol=1e-07;, score=-0.161 total time=   0.0s\n",
      "[CV 4/4] END alpha=93, selection=cyclic, tol=1e-05;, score=-2.473 total time=   0.0s\n",
      "[CV 3/4] END alpha=76, selection=random, tol=1e-05;, score=-0.159 total time=   0.0s\n",
      "[CV 1/4] END alpha=83, selection=cyclic, tol=1e-07;, score=-0.730 total time=   0.0s\n",
      "[CV 3/4] END alpha=67, selection=cyclic, tol=0.01;, score=-0.158 total time=   0.0s\n",
      "[CV 2/4] END alpha=87, selection=cyclic, tol=1e-05;, score=-0.072 total time=   0.0s\n",
      "[CV 4/4] END alpha=88, selection=cyclic, tol=1e-07;, score=-2.394 total time=   0.0s\n",
      "[CV 2/4] END alpha=85, selection=random, tol=1e-07;, score=-0.071 total time=   0.0s\n",
      "[CV 1/4] END alpha=80, selection=random, tol=1e-07;, score=-0.733 total time=   0.0s\n",
      "[CV 4/4] END alpha=94, selection=cyclic, tol=0.01;, score=-2.489 total time=   0.0s\n",
      "[CV 2/4] END alpha=91, selection=random, tol=0.01;, score=-0.075 total time=   0.0s\n",
      "[CV 4/4] END alpha=95, selection=random, tol=1e-07;, score=-2.505 total time=   0.0s\n",
      "[CV 3/4] END alpha=79, selection=cyclic, tol=0.01;, score=-0.159 total time=   0.0s\n",
      "[CV 3/4] END alpha=90, selection=random, tol=1e-05;, score=-0.161 total time=   0.0s\n",
      "[CV 4/4] END alpha=84, selection=cyclic, tol=1e-07;, score=-2.332 total time=   0.0s\n",
      "[CV 1/4] END alpha=93, selection=cyclic, tol=1e-07;, score=-0.717 total time=   0.0s\n",
      "[CV 2/4] END alpha=83, selection=cyclic, tol=1e-07;, score=-0.071 total time=   0.0s\n",
      "[CV 2/4] END alpha=72, selection=random, tol=1e-07;, score=-0.066 total time=   0.0s\n",
      "[CV 4/4] END alpha=76, selection=random, tol=1e-05;, score=-2.210 total time=   0.0s\n",
      "[CV 4/4] END alpha=67, selection=cyclic, tol=0.01;, score=-2.077 total time=   0.0s\n",
      "[CV 3/4] END alpha=89, selection=cyclic, tol=1e-07;, score=-0.160 total time=   0.0s\n",
      "[CV 3/4] END alpha=87, selection=cyclic, tol=1e-05;, score=-0.160 total time=   0.0s\n",
      "[CV 1/4] END alpha=88, selection=cyclic, tol=0.01;, score=-0.724 total time=   0.0s\n",
      "[CV 3/4] END alpha=85, selection=random, tol=1e-07;, score=-0.160 total time=   0.0s\n",
      "[CV 1/4] END alpha=94, selection=random, tol=1e-05;, score=-0.716 total time=   0.0s\n",
      "[CV 2/4] END alpha=80, selection=random, tol=1e-07;, score=-0.069 total time=   0.0s\n",
      "[CV 3/4] END alpha=91, selection=random, tol=0.01;, score=-0.162 total time=   0.0s[CV 1/4] END alpha=95, selection=random, tol=0.01;, score=-0.710 total time=   0.0s\n",
      "\n",
      "[CV 4/4] END alpha=90, selection=random, tol=1e-05;, score=-2.425 total time=   0.0s[CV 2/4] END alpha=93, selection=cyclic, tol=1e-07;, score=-0.075 total time=   0.0s\n",
      "\n",
      "[CV 3/4] END alpha=83, selection=cyclic, tol=1e-07;, score=-0.160 total time=   0.0s\n",
      "[CV 1/4] END alpha=76, selection=random, tol=1e-07;, score=-0.739 total time=   0.0s\n",
      "[CV 4/4] END alpha=87, selection=cyclic, tol=1e-05;, score=-2.378 total time=   0.0s\n",
      "[CV 2/4] END alpha=95, selection=random, tol=0.01;, score=-0.077 total time=   0.0s\n",
      "[CV 2/4] END alpha=88, selection=cyclic, tol=0.01;, score=-0.073 total time=   0.0s\n",
      "[CV 4/4] END alpha=85, selection=random, tol=1e-07;, score=-2.347 total time=   0.0s\n",
      "[CV 2/4] END alpha=94, selection=random, tol=1e-05;, score=-0.075 total time=   0.0s\n",
      "[CV 4/4] END alpha=91, selection=random, tol=0.01;, score=-2.441 total time=   0.0s\n",
      "[CV 3/4] END alpha=80, selection=random, tol=1e-07;, score=-0.159 total time=   0.0s\n",
      "[CV 3/4] END alpha=72, selection=random, tol=1e-07;, score=-0.158 total time=   0.0s\n",
      "[CV 1/4] END alpha=67, selection=random, tol=1e-05;, score=-0.750 total time=   0.0s\n",
      "[CV 4/4] END alpha=79, selection=cyclic, tol=0.01;, score=-2.255 total time=   0.0s\n",
      "[CV 3/4] END alpha=95, selection=random, tol=0.01;, score=-0.166 total time=   0.0s\n",
      "[CV 4/4] END alpha=83, selection=cyclic, tol=1e-07;, score=-2.316 total time=   0.0s\n",
      "[CV 1/4] END alpha=90, selection=random, tol=1e-07;, score=-0.721 total time=   0.0s\n",
      "[CV 1/4] END alpha=87, selection=cyclic, tol=1e-07;, score=-0.724 total time=   0.0s\n",
      "[CV 3/4] END alpha=88, selection=cyclic, tol=0.01;, score=-0.160 total time=   0.0s\n",
      "[CV 1/4] END alpha=85, selection=random, tol=0.01;, score=-0.727 total time=   0.0s\n",
      "[CV 3/4] END alpha=94, selection=random, tol=1e-05;, score=-0.161 total time=   0.0s\n",
      "[CV 2/4] END alpha=76, selection=random, tol=1e-07;, score=-0.068 total time=   0.0s\n",
      "[CV 4/4] END alpha=89, selection=cyclic, tol=1e-07;, score=-2.410 total time=   0.0s\n",
      "[CV 4/4] END alpha=80, selection=random, tol=1e-07;, score=-2.271 total time=   0.0s\n",
      "[CV 4/4] END alpha=72, selection=random, tol=1e-07;, score=-2.150 total time=   0.0s\n",
      "[CV 1/4] END alpha=92, selection=cyclic, tol=1e-05;, score=-0.718 total time=   0.0s\n",
      "[CV 2/4] END alpha=67, selection=random, tol=1e-05;, score=-0.064 total time=   0.0s\n",
      "[CV 3/4] END alpha=93, selection=cyclic, tol=1e-07;, score=-0.161 total time=   0.0s\n",
      "[CV 4/4] END alpha=95, selection=random, tol=0.01;, score=-2.505 total time=   0.0s\n",
      "[CV 1/4] END alpha=83, selection=cyclic, tol=0.01;, score=-0.730 total time=   0.0s\n",
      "[CV 4/4] END alpha=88, selection=cyclic, tol=0.01;, score=-2.394 total time=   0.0s\n",
      "[CV 2/4] END alpha=87, selection=cyclic, tol=1e-07;, score=-0.072 total time=   0.0s\n",
      "[CV 4/4] END alpha=94, selection=random, tol=1e-05;, score=-2.489 total time=   0.0s\n",
      "[CV 3/4] END alpha=76, selection=random, tol=1e-07;, score=-0.159 total time=   0.0s\n",
      "[CV 2/4] END alpha=90, selection=random, tol=1e-07;, score=-0.074 total time=   0.0s\n",
      "[CV 1/4] END alpha=89, selection=cyclic, tol=0.01;, score=-0.722 total time=   0.0s\n",
      "[CV 1/4] END alpha=72, selection=random, tol=0.01;, score=-0.742 total time=   0.0s\n",
      "[CV 2/4] END alpha=85, selection=random, tol=0.01;, score=-0.071 total time=   0.0s\n",
      "[CV 2/4] END alpha=92, selection=cyclic, tol=1e-05;, score=-0.074 total time=   0.0s\n",
      "[CV 1/4] END alpha=96, selection=cyclic, tol=1e-05;, score=-0.713 total time=   0.0s\n",
      "[CV 1/4] END alpha=84, selection=cyclic, tol=0.01;, score=-0.729 total time=   0.0s\n",
      "[CV 1/4] END alpha=79, selection=random, tol=1e-05;, score=-0.735 total time=   0.0s\n",
      "[CV 2/4] END alpha=83, selection=cyclic, tol=0.01;, score=-0.071 total time=   0.0s\n",
      "[CV 3/4] END alpha=87, selection=cyclic, tol=1e-07;, score=-0.160 total time=   0.0s\n",
      "[CV 3/4] END alpha=67, selection=random, tol=1e-05;, score=-0.158 total time=   0.0s\n",
      "[CV 1/4] END alpha=94, selection=random, tol=1e-07;, score=-0.715 total time=   0.0s\n",
      "[CV 2/4] END alpha=96, selection=cyclic, tol=1e-05;, score=-0.076 total time=   0.0s\n",
      "[CV 2/4] END alpha=89, selection=cyclic, tol=0.01;, score=-0.073 total time=   0.0s\n",
      "[CV 2/4] END alpha=72, selection=random, tol=0.01;, score=-0.066 total time=   0.0s\n",
      "[CV 4/4] END alpha=76, selection=random, tol=1e-07;, score=-2.210 total time=   0.0s\n",
      "[CV 2/4] END alpha=84, selection=cyclic, tol=0.01;, score=-0.071 total time=   0.0s\n",
      "[CV 2/4] END alpha=79, selection=random, tol=1e-05;, score=-0.069 total time=   0.0s\n",
      "[CV 3/4] END alpha=92, selection=cyclic, tol=1e-05;, score=-0.161 total time=   0.0s\n",
      "[CV 3/4] END alpha=90, selection=random, tol=1e-07;, score=-0.161 total time=   0.0s\n",
      "[CV 3/4] END alpha=85, selection=random, tol=0.01;, score=-0.158 total time=   0.0s\n",
      "[CV 3/4] END alpha=83, selection=cyclic, tol=0.01;, score=-0.160 total time=   0.0s\n",
      "[CV 3/4] END alpha=96, selection=cyclic, tol=1e-05;, score=-0.161 total time=   0.0s\n",
      "[CV 4/4] END alpha=87, selection=cyclic, tol=1e-07;, score=-2.378 total time=   0.0s\n",
      "[CV 2/4] END alpha=94, selection=random, tol=1e-07;, score=-0.075 total time=   0.0s\n",
      "[CV 4/4] END alpha=93, selection=cyclic, tol=1e-07;, score=-2.473 total time=   0.0s[CV 3/4] END alpha=72, selection=random, tol=0.01;, score=-0.158 total time=   0.0s\n",
      "\n",
      "[CV 1/4] END alpha=80, selection=random, tol=0.01;, score=-0.738 total time=   0.0s\n",
      "[CV 1/4] END alpha=76, selection=random, tol=0.01;, score=-0.739 total time=   0.0s\n",
      "[CV 3/4] END alpha=89, selection=cyclic, tol=0.01;, score=-0.161 total time=   0.0s\n",
      "[CV 1/4] END alpha=88, selection=random, tol=1e-05;, score=-0.723 total time=   0.0s\n",
      "[CV 4/4] END alpha=67, selection=random, tol=1e-05;, score=-2.077 total time=   0.0s\n",
      "[CV 4/4] END alpha=92, selection=cyclic, tol=1e-05;, score=-2.457 total time=   0.0s\n",
      "[CV 4/4] END alpha=85, selection=random, tol=0.01;, score=-2.347 total time=   0.0s\n",
      "[CV 4/4] END alpha=96, selection=cyclic, tol=1e-05;, score=-2.521 total time=   0.0s\n",
      "[CV 4/4] END alpha=90, selection=random, tol=1e-07;, score=-2.425 total time=   0.0s\n",
      "[CV 4/4] END alpha=83, selection=cyclic, tol=0.01;, score=-2.316 total time=   0.0s\n",
      "[CV 1/4] END alpha=87, selection=cyclic, tol=0.01;, score=-0.725 total time=   0.0s\n",
      "[CV 4/4] END alpha=72, selection=random, tol=0.01;, score=-2.150 total time=   0.0s\n",
      "[CV 3/4] END alpha=94, selection=random, tol=1e-07;, score=-0.161 total time=   0.0s\n",
      "[CV 3/4] END alpha=84, selection=cyclic, tol=0.01;, score=-0.160 total time=   0.0s[CV 4/4] END alpha=89, selection=cyclic, tol=0.01;, score=-2.410 total time=   0.0s\n",
      "\n",
      "[CV 1/4] END alpha=92, selection=cyclic, tol=1e-07;, score=-0.718 total time=   0.0s\n",
      "[CV 3/4] END alpha=79, selection=random, tol=1e-05;, score=-0.159 total time=   0.0s\n",
      "[CV 1/4] END alpha=86, selection=cyclic, tol=1e-05;, score=-0.726 total time=   0.0s\n",
      "[CV 2/4] END alpha=87, selection=cyclic, tol=0.01;, score=-0.072 total time=   0.0s\n",
      "[CV 1/4] END alpha=83, selection=random, tol=1e-05;, score=-0.730 total time=   0.0s\n",
      "[CV 1/4] END alpha=93, selection=cyclic, tol=0.01;, score=-0.717 total time=   0.0s\n",
      "[CV 4/4] END alpha=94, selection=random, tol=1e-07;, score=-2.489 total time=   0.0s\n",
      "[CV 2/4] END alpha=80, selection=random, tol=0.01;, score=-0.069 total time=   0.0s\n",
      "[CV 2/4] END alpha=76, selection=random, tol=0.01;, score=-0.067 total time=   0.0s\n",
      "[CV 1/4] END alpha=97, selection=cyclic, tol=1e-05;, score=-0.712 total time=   0.0s\n",
      "[CV 4/4] END alpha=84, selection=cyclic, tol=0.01;, score=-2.332 total time=   0.0s\n",
      "[CV 1/4] END alpha=96, selection=cyclic, tol=1e-07;, score=-0.713 total time=   0.0s\n",
      "[CV 2/4] END alpha=92, selection=cyclic, tol=1e-07;, score=-0.074 total time=   0.0s\n",
      "[CV 4/4] END alpha=79, selection=random, tol=1e-05;, score=-2.255 total time=   0.0s\n",
      "[CV 2/4] END alpha=88, selection=random, tol=1e-05;, score=-0.073 total time=   0.0s\n",
      "[CV 1/4] END alpha=94, selection=random, tol=0.01;, score=-0.716 total time=   0.0s\n",
      "[CV 2/4] END alpha=97, selection=cyclic, tol=1e-05;, score=-0.077 total time=   0.0s\n",
      "[CV 3/4] END alpha=76, selection=random, tol=0.01;, score=-0.158 total time=   0.0s\n",
      "[CV 1/4] END alpha=90, selection=random, tol=0.01;, score=-0.721 total time=   0.0s\n",
      "[CV 2/4] END alpha=93, selection=cyclic, tol=0.01;, score=-0.075 total time=   0.0s\n",
      "[CV 3/4] END alpha=80, selection=random, tol=0.01;, score=-0.159 total time=   0.0s\n",
      "[CV 3/4] END alpha=97, selection=cyclic, tol=1e-05;, score=-0.162 total time=   0.0s\n",
      "[CV 3/4] END alpha=88, selection=random, tol=1e-05;, score=-0.160 total time=   0.0s\n",
      "[CV 2/4] END alpha=86, selection=cyclic, tol=1e-05;, score=-0.072 total time=   0.0s[CV 3/4] END alpha=87, selection=cyclic, tol=0.01;, score=-0.160 total time=   0.0s\n",
      "\n",
      "[CV 4/4] END alpha=76, selection=random, tol=0.01;, score=-2.210 total time=   0.0s\n",
      "[CV 2/4] END alpha=90, selection=random, tol=0.01;, score=-0.074 total time=   0.0s\n",
      "[CV 2/4] END alpha=96, selection=cyclic, tol=1e-07;, score=-0.076 total time=   0.0s\n",
      "[CV 2/4] END alpha=94, selection=random, tol=0.01;, score=-0.076 total time=   0.0s\n",
      "[CV 4/4] END alpha=97, selection=cyclic, tol=1e-05;, score=-2.537 total time=   0.0s\n",
      "[CV 1/4] END alpha=98, selection=cyclic, tol=0.01;, score=-0.713 total time=   0.0s\n",
      "[CV 3/4] END alpha=96, selection=cyclic, tol=1e-07;, score=-0.161 total time=   0.0s\n",
      "[CV 3/4] END alpha=90, selection=random, tol=0.01;, score=-0.160 total time=   0.0s\n",
      "[CV 3/4] END alpha=93, selection=cyclic, tol=0.01;, score=-0.161 total time=   0.0s\n",
      "[CV 4/4] END alpha=87, selection=cyclic, tol=0.01;, score=-2.378 total time=   0.0s\n",
      "[CV 3/4] END alpha=94, selection=random, tol=0.01;, score=-0.161 total time=   0.0s\n",
      "[CV 1/4] END alpha=84, selection=random, tol=1e-05;, score=-0.728 total time=   0.0s\n",
      "[CV 1/4] END alpha=89, selection=random, tol=1e-05;, score=-0.722 total time=   0.0s\n",
      "[CV 4/4] END alpha=80, selection=random, tol=0.01;, score=-2.271 total time=   0.0s\n",
      "[CV 2/4] END alpha=83, selection=random, tol=1e-05;, score=-0.071 total time=   0.0s\n",
      "[CV 2/4] END alpha=98, selection=cyclic, tol=0.01;, score=-0.077 total time=   0.0s\n",
      "[CV 4/4] END alpha=96, selection=cyclic, tol=1e-07;, score=-2.521 total time=   0.0s\n",
      "[CV 1/4] END alpha=99, selection=random, tol=1e-07;, score=-0.709 total time=   0.0s\n",
      "[CV 3/4] END alpha=86, selection=cyclic, tol=1e-05;, score=-0.160 total time=   0.0s\n",
      "[CV 4/4] END alpha=93, selection=cyclic, tol=0.01;, score=-2.473 total time=   0.0s\n",
      "[CV 4/4] END alpha=90, selection=random, tol=0.01;, score=-2.425 total time=   0.0s\n",
      "[CV 4/4] END alpha=94, selection=random, tol=0.01;, score=-2.489 total time=   0.0s[CV 1/4] END alpha=87, selection=random, tol=1e-05;, score=-0.724 total time=   0.0s\n",
      "\n",
      "[CV 2/4] END alpha=84, selection=random, tol=1e-05;, score=-0.071 total time=   0.0s\n",
      "[CV 1/4] END alpha=97, selection=cyclic, tol=1e-07;, score=-0.712 total time=   0.0s\n",
      "[CV 1/4] END alpha=93, selection=random, tol=1e-05;, score=-0.717 total time=   0.0s\n",
      "[CV 4/4] END alpha=86, selection=cyclic, tol=1e-05;, score=-2.363 total time=   0.0s\n",
      "[CV 2/4] END alpha=87, selection=random, tol=1e-05;, score=-0.072 total time=   0.0s\n",
      "[CV 4/4] END alpha=88, selection=random, tol=1e-05;, score=-2.394 total time=   0.0s\n",
      "[CV 3/4] END alpha=92, selection=cyclic, tol=1e-07;, score=-0.161 total time=   0.0s\n",
      "[CV 1/4] END alpha=95, selection=cyclic, tol=1e-05;, score=-0.714 total time=   0.0s\n",
      "[CV 2/4] END alpha=99, selection=random, tol=1e-07;, score=-0.077 total time=   0.0s\n",
      "[CV 1/4] END alpha=91, selection=cyclic, tol=1e-05;, score=-0.719 total time=   0.0s\n",
      "[CV 3/4] END alpha=98, selection=cyclic, tol=0.01;, score=-0.162 total time=   0.0s\n",
      "[CV 3/4] END alpha=87, selection=random, tol=1e-05;, score=-0.160 total time=   0.0s\n",
      "[CV 1/4] END alpha=86, selection=cyclic, tol=1e-07;, score=-0.726 total time=   0.0s\n",
      "[CV 2/4] END alpha=93, selection=random, tol=1e-05;, score=-0.075 total time=   0.0s\n",
      "[CV 1/4] END alpha=88, selection=random, tol=1e-07;, score=-0.723 total time=   0.0s\n",
      "[CV 3/4] END alpha=83, selection=random, tol=1e-05;, score=-0.160 total time=   0.0s\n",
      "[CV 2/4] END alpha=97, selection=cyclic, tol=1e-07;, score=-0.077 total time=   0.0s\n",
      "[CV 2/4] END alpha=91, selection=cyclic, tol=1e-05;, score=-0.074 total time=   0.0s\n",
      "[CV 4/4] END alpha=92, selection=cyclic, tol=1e-07;, score=-2.457 total time=   0.0s\n",
      "[CV 4/4] END alpha=87, selection=random, tol=1e-05;, score=-2.378 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.1, selection=cyclic, tol=1e-05;, score=0.654 total time=   0.0s\n",
      "[CV 2/4] END alpha=86, selection=cyclic, tol=1e-07;, score=-0.072 total time=   0.0s\n",
      "[CV 3/4] END alpha=93, selection=random, tol=1e-05;, score=-0.161 total time=   0.0s\n",
      "[CV 2/4] END alpha=88, selection=random, tol=1e-07;, score=-0.073 total time=   0.0s\n",
      "[CV 2/4] END alpha=95, selection=cyclic, tol=1e-05;, score=-0.076 total time=   0.0s\n",
      "[CV 4/4] END alpha=98, selection=cyclic, tol=0.01;, score=-2.553 total time=   0.0s\n",
      "[CV 2/4] END alpha=89, selection=random, tol=1e-05;, score=-0.073 total time=   0.0s\n",
      "[CV 3/4] END alpha=84, selection=random, tol=1e-05;, score=-0.160 total time=   0.0s\n",
      "[CV 3/4] END alpha=99, selection=random, tol=1e-07;, score=-0.162 total time=   0.0s\n",
      "[CV 1/4] END alpha=92, selection=cyclic, tol=0.01;, score=-0.719 total time=   0.0s\n",
      "[CV 3/4] END alpha=86, selection=cyclic, tol=1e-07;, score=-0.160 total time=   0.0s\n",
      "[CV 4/4] END alpha=83, selection=random, tol=1e-05;, score=-2.316 total time=   0.0s\n",
      "[CV 3/4] END alpha=91, selection=cyclic, tol=1e-05;, score=-0.161 total time=   0.0s\n",
      "[CV 1/4] END alpha=98, selection=random, tol=1e-05;, score=-0.710 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.2, selection=cyclic, tol=0.01;, score=0.644 total time=   0.0s\n",
      "[CV 4/4] END alpha=93, selection=random, tol=1e-05;, score=-2.473 total time=   0.0s\n",
      "[CV 4/4] END alpha=84, selection=random, tol=1e-05;, score=-2.332 total time=   0.0s\n",
      "[CV 3/4] END alpha=97, selection=cyclic, tol=1e-07;, score=-0.162 total time=   0.0s\n",
      "[CV 4/4] END alpha=86, selection=cyclic, tol=1e-07;, score=-2.363 total time=   0.0s\n",
      "[CV 2/4] END alpha=92, selection=cyclic, tol=0.01;, score=-0.074 total time=   0.0s\n",
      "[CV 4/4] END alpha=99, selection=random, tol=1e-07;, score=-2.569 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.2, selection=cyclic, tol=0.01;, score=0.603 total time=   0.0s\n",
      "[CV 3/4] END alpha=95, selection=cyclic, tol=1e-05;, score=-0.161 total time=   0.0s\n",
      "[CV 1/4] END alpha=96, selection=cyclic, tol=0.01;, score=-0.715 total time=   0.0s\n",
      "[CV 4/4] END alpha=97, selection=cyclic, tol=1e-07;, score=-2.537 total time=   0.0s\n",
      "[CV 4/4] END alpha=91, selection=cyclic, tol=1e-05;, score=-2.441 total time=   0.0s\n",
      "[CV 1/4] END alpha=93, selection=random, tol=1e-07;, score=-0.717 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.1, selection=cyclic, tol=1e-05;, score=0.599 total time=   0.0s\n",
      "[CV 3/4] END alpha=92, selection=cyclic, tol=0.01;, score=-0.161 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.2, selection=cyclic, tol=0.01;, score=0.377 total time=   0.0s\n",
      "[CV 2/4] END alpha=96, selection=cyclic, tol=0.01;, score=-0.076 total time=   0.0s\n",
      "[CV 3/4] END alpha=88, selection=random, tol=1e-07;, score=-0.160 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.3, selection=random, tol=1e-07;, score=0.637 total time=   0.0s\n",
      "[CV 1/4] END alpha=99, selection=random, tol=0.01;, score=-0.770 total time=   0.0s\n",
      "[CV 2/4] END alpha=93, selection=random, tol=1e-07;, score=-0.075 total time=   0.0s[CV 3/4] END alpha=89, selection=random, tol=1e-05;, score=-0.160 total time=   0.0s\n",
      "\n",
      "[CV 3/4] END alpha=96, selection=cyclic, tol=0.01;, score=-0.161 total time=   0.0s\n",
      "[CV 2/4] END alpha=98, selection=random, tol=1e-05;, score=-0.077 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.2, selection=cyclic, tol=0.01;, score=-0.776 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.5, selection=cyclic, tol=1e-05;, score=0.617 total time=   0.0s\n",
      "[CV 4/4] END alpha=88, selection=random, tol=1e-07;, score=-2.394 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.3, selection=random, tol=1e-07;, score=0.585 total time=   0.0s\n",
      "[CV 2/4] END alpha=99, selection=random, tol=0.01;, score=-0.075 total time=   0.0s\n",
      "[CV 1/4] END alpha=91, selection=cyclic, tol=1e-07;, score=-0.719 total time=   0.0s\n",
      "[CV 4/4] END alpha=92, selection=cyclic, tol=0.01;, score=-2.457 total time=   0.0s\n",
      "[CV 1/4] END alpha=97, selection=cyclic, tol=0.01;, score=-0.714 total time=   0.0s\n",
      "[CV 3/4] END alpha=93, selection=random, tol=1e-07;, score=-0.161 total time=   0.0s\n",
      "[CV 1/4] END alpha=84, selection=random, tol=1e-07;, score=-0.728 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.2, selection=random, tol=1e-05;, score=0.644 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.1, selection=cyclic, tol=1e-05;, score=0.363 total time=   0.0s\n",
      "[CV 2/4] END alpha=97, selection=cyclic, tol=0.01;, score=-0.076 total time=   0.0s[CV 4/4] END alpha=95, selection=cyclic, tol=1e-05;, score=-2.505 total time=   0.0s\n",
      "\n",
      "[CV 1/4] END alpha=92, selection=random, tol=1e-05;, score=-0.718 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.6, selection=cyclic, tol=0.01;, score=0.605 total time=   0.0s\n",
      "[CV 3/4] END alpha=99, selection=random, tol=0.01;, score=-0.162 total time=   0.0s\n",
      "[CV 4/4] END alpha=93, selection=random, tol=1e-07;, score=-2.473 total time=   0.0s\n",
      "[CV 2/4] END alpha=84, selection=random, tol=1e-07;, score=-0.071 total time=   0.0s\n",
      "[CV 3/4] END alpha=98, selection=random, tol=1e-05;, score=-0.162 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.5, selection=cyclic, tol=1e-05;, score=0.541 total time=   0.0s\n",
      "[CV 2/4] END alpha=91, selection=cyclic, tol=1e-07;, score=-0.074 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.2, selection=random, tol=1e-05;, score=0.603 total time=   0.0s\n",
      "[CV 4/4] END alpha=89, selection=random, tol=1e-05;, score=-2.410 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.1, selection=cyclic, tol=1e-05;, score=-0.848 total time=   0.0s\n",
      "[CV 1/4] END alpha=88, selection=random, tol=0.01;, score=-0.733 total time=   0.0s\n",
      "[CV 1/4] END alpha=93, selection=random, tol=0.01;, score=-0.717 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.3, selection=random, tol=1e-07;, score=0.389 total time=   0.0s\n",
      "[CV 4/4] END alpha=99, selection=random, tol=0.01;, score=-2.569 total time=   0.0s\n",
      "[CV 4/4] END alpha=98, selection=random, tol=1e-05;, score=-2.553 total time=   0.0s\n",
      "[CV 2/4] END alpha=92, selection=random, tol=1e-05;, score=-0.074 total time=   0.0s\n",
      "[CV 3/4] END alpha=91, selection=cyclic, tol=1e-07;, score=-0.161 total time=   0.0s\n",
      "[CV 4/4] END alpha=96, selection=cyclic, tol=0.01;, score=-2.521 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.1, selection=cyclic, tol=1e-07;, score=0.654 total time=   0.0s\n",
      "[CV 2/4] END alpha=93, selection=random, tol=0.01;, score=-0.075 total time=   0.0s\n",
      "[CV 2/4] END alpha=88, selection=random, tol=0.01;, score=-0.072 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.5, selection=cyclic, tol=1e-05;, score=0.403 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.2, selection=random, tol=1e-05;, score=0.378 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.6, selection=cyclic, tol=0.01;, score=0.531 total time=   0.0s\n",
      "[CV 1/4] END alpha=95, selection=cyclic, tol=1e-07;, score=-0.714 total time=   0.0s\n",
      "[CV 3/4] END alpha=84, selection=random, tol=1e-07;, score=-0.160 total time=   0.0s\n",
      "[CV 1/4] END alpha=96, selection=random, tol=1e-05;, score=-0.713 total time=   0.0s\n",
      "[CV 3/4] END alpha=97, selection=cyclic, tol=0.01;, score=-0.162 total time=   0.0s\n",
      "[CV 4/4] END alpha=91, selection=cyclic, tol=1e-07;, score=-2.441 total time=   0.0s\n",
      "[CV 3/4] END alpha=92, selection=random, tol=1e-05;, score=-0.161 total time=   0.0s\n",
      "[CV 1/4] END alpha=89, selection=random, tol=1e-07;, score=-0.722 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.1, selection=cyclic, tol=1e-07;, score=0.599 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.5, selection=cyclic, tol=1e-05;, score=-0.682 total time=   0.0s\n",
      "[CV 3/4] END alpha=93, selection=random, tol=0.01;, score=-0.166 total time=   0.0s\n",
      "[CV 3/4] END alpha=88, selection=random, tol=0.01;, score=-0.160 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.2, selection=random, tol=1e-05;, score=-0.776 total time=   0.0s\n",
      "[CV 2/4] END alpha=95, selection=cyclic, tol=1e-07;, score=-0.076 total time=   0.0s\n",
      "[CV 4/4] END alpha=97, selection=cyclic, tol=0.01;, score=-2.537 total time=   0.0s\n",
      "[CV 1/4] END alpha=91, selection=cyclic, tol=0.01;, score=-0.720 total time=   0.0s\n",
      "[CV 4/4] END alpha=92, selection=random, tol=1e-05;, score=-2.457 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.3, selection=random, tol=1e-07;, score=-0.772 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.1, selection=cyclic, tol=1e-07;, score=0.363 total time=   0.0s\n",
      "[CV 4/4] END alpha=88, selection=random, tol=0.01;, score=-2.394 total time=   0.0s\n",
      "[CV 4/4] END alpha=93, selection=random, tol=0.01;, score=-2.473 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.5, selection=cyclic, tol=1e-07;, score=0.617 total time=   0.0s\n",
      "[CV 1/4] END alpha=98, selection=random, tol=1e-07;, score=-0.710 total time=   0.0s\n",
      "[CV 2/4] END alpha=89, selection=random, tol=1e-07;, score=-0.073 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.2, selection=random, tol=1e-07;, score=0.644 total time=   0.0s\n",
      "[CV 2/4] END alpha=91, selection=cyclic, tol=0.01;, score=-0.074 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.0, selection=cyclic, tol=1e-05;, score=0.602 total time=   0.0s\n",
      "[CV 1/4] END alpha=92, selection=random, tol=1e-07;, score=-0.718 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.3, selection=random, tol=0.01;, score=0.637 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.1, selection=cyclic, tol=1e-07;, score=-0.848 total time=   0.0s\n",
      "[CV 1/4] END alpha=94, selection=cyclic, tol=1e-05;, score=-0.716 total time=   0.0s[CV 2/4] END alpha=98, selection=random, tol=1e-07;, score=-0.077 total time=   0.0s\n",
      "\n",
      "[CV 2/4] END alpha=0.5, selection=cyclic, tol=1e-07;, score=0.541 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.6, selection=cyclic, tol=0.01;, score=0.402 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.2, selection=random, tol=1e-07;, score=0.603 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.7, selection=random, tol=1e-07;, score=0.591 total time=   0.0s\n",
      "[CV 3/4] END alpha=91, selection=cyclic, tol=0.01;, score=-0.161 total time=   0.0s\n",
      "[CV 2/4] END alpha=96, selection=random, tol=1e-05;, score=-0.076 total time=   0.0s\n",
      "[CV 4/4] END alpha=84, selection=random, tol=1e-07;, score=-2.332 total time=   0.0s\n",
      "[CV 2/4] END alpha=94, selection=cyclic, tol=1e-05;, score=-0.075 total time=   0.0s[CV 1/4] END alpha=97, selection=random, tol=1e-05;, score=-0.712 total time=   0.0s\n",
      "\n",
      "[CV 1/4] END alpha=0.1, selection=cyclic, tol=0.01;, score=0.654 total time=   0.0s\n",
      "[CV 3/4] END alpha=96, selection=random, tol=1e-05;, score=-0.161 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.3, selection=random, tol=0.01;, score=0.585 total time=   0.0s\n",
      "[CV 4/4] END alpha=91, selection=cyclic, tol=0.01;, score=-2.441 total time=   0.0s\n",
      "[CV 3/4] END alpha=89, selection=random, tol=1e-07;, score=-0.160 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.7, selection=random, tol=1e-07;, score=0.529 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.2, selection=random, tol=1e-07;, score=0.378 total time=   0.0s\n",
      "[CV 4/4] END alpha=96, selection=random, tol=1e-05;, score=-2.521 total time=   0.0s\n",
      "[CV 3/4] END alpha=94, selection=cyclic, tol=1e-05;, score=-0.161 total time=   0.0s\n",
      "[CV 3/4] END alpha=95, selection=cyclic, tol=1e-07;, score=-0.161 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.1, selection=cyclic, tol=0.01;, score=0.599 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.0, selection=cyclic, tol=1e-05;, score=0.604 total time=   0.0s\n",
      "[CV 3/4] END alpha=98, selection=random, tol=1e-07;, score=-0.162 total time=   0.0s\n",
      "[CV 1/4] END alpha=91, selection=random, tol=1e-05;, score=-0.719 total time=   0.0s\n",
      "[CV 4/4] END alpha=89, selection=random, tol=1e-07;, score=-2.410 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.7, selection=random, tol=1e-07;, score=0.404 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.2, selection=random, tol=1e-07;, score=-0.776 total time=   0.0s\n",
      "[CV 1/4] END alpha=96, selection=random, tol=1e-07;, score=-0.713 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.5, selection=cyclic, tol=1e-07;, score=0.403 total time=   0.0s\n",
      "[CV 4/4] END alpha=94, selection=cyclic, tol=1e-05;, score=-2.489 total time=   0.0s\n",
      "[CV 4/4] END alpha=95, selection=cyclic, tol=1e-07;, score=-2.505 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.6, selection=cyclic, tol=0.01;, score=-0.590 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.1, selection=cyclic, tol=0.01;, score=0.362 total time=   0.0s\n",
      "[CV 2/4] END alpha=91, selection=random, tol=1e-05;, score=-0.074 total time=   0.0s\n",
      "[CV 2/4] END alpha=96, selection=random, tol=1e-07;, score=-0.076 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.7, selection=random, tol=1e-07;, score=-0.514 total time=   0.0s\n",
      "[CV 2/4] END alpha=92, selection=random, tol=1e-07;, score=-0.074 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.2, selection=random, tol=0.01;, score=0.644 total time=   0.0s\n",
      "[CV 2/4] END alpha=97, selection=random, tol=1e-05;, score=-0.077 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.5, selection=cyclic, tol=1e-07;, score=-0.682 total time=   0.0s\n",
      "[CV 1/4] END alpha=94, selection=cyclic, tol=1e-07;, score=-0.715 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.1, selection=cyclic, tol=0.01;, score=-0.849 total time=   0.0s\n",
      "[CV 3/4] END alpha=96, selection=random, tol=1e-07;, score=-0.161 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.6, selection=random, tol=1e-05;, score=0.605 total time=   0.0s\n",
      "[CV 3/4] END alpha=91, selection=random, tol=1e-05;, score=-0.161 total time=   0.0s\n",
      "[CV 4/4] END alpha=98, selection=random, tol=1e-07;, score=-2.553 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.7, selection=random, tol=0.01;, score=0.591 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.0, selection=cyclic, tol=1e-05;, score=0.359 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.2, selection=random, tol=0.01;, score=0.603 total time=   0.0s\n",
      "[CV 3/4] END alpha=92, selection=random, tol=1e-07;, score=-0.161 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.5, selection=cyclic, tol=0.01;, score=0.617 total time=   0.0s\n",
      "[CV 2/4] END alpha=94, selection=cyclic, tol=1e-07;, score=-0.075 total time=   0.0s\n",
      "[CV 4/4] END alpha=91, selection=random, tol=1e-05;, score=-2.441 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.1, selection=random, tol=1e-05;, score=0.654 total time=   0.0s\n",
      "[CV 1/4] END alpha=98, selection=random, tol=0.01;, score=-0.710 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.7, selection=random, tol=0.01;, score=0.529 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.3, selection=random, tol=0.01;, score=0.389 total time=   0.0s\n",
      "[CV 1/4] END alpha=84, selection=random, tol=0.01;, score=-0.729 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.2, selection=random, tol=0.01;, score=0.378 total time=   0.0s\n",
      "[CV 3/4] END alpha=94, selection=cyclic, tol=1e-07;, score=-0.161 total time=   0.0s\n",
      "[CV 1/4] END alpha=89, selection=random, tol=0.01;, score=-0.723 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.5, selection=cyclic, tol=0.01;, score=0.542 total time=   0.0s\n",
      "[CV 3/4] END alpha=97, selection=random, tol=1e-05;, score=-0.162 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.7, selection=random, tol=0.01;, score=0.402 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.1, selection=random, tol=1e-05;, score=0.599 total time=   0.0s\n",
      "[CV 4/4] END alpha=96, selection=random, tol=1e-07;, score=-2.521 total time=   0.0s\n",
      "[CV 1/4] END alpha=95, selection=cyclic, tol=0.01;, score=-0.717 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.9, selection=cyclic, tol=1e-05;, score=0.560 total time=   0.0s[CV 4/4] END alpha=0.2, selection=random, tol=0.01;, score=-0.767 total time=   0.0s\n",
      "\n",
      "[CV 4/4] END alpha=0.0, selection=cyclic, tol=1e-05;, score=-1.109 total time=   0.0s\n",
      "[CV 4/4] END alpha=94, selection=cyclic, tol=1e-07;, score=-2.489 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.6, selection=random, tol=1e-05;, score=0.531 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.5, selection=cyclic, tol=0.01;, score=0.400 total time=   0.0s\n",
      "[CV 4/4] END alpha=97, selection=random, tol=1e-05;, score=-2.537 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.7, selection=random, tol=0.01;, score=-0.511 total time=   0.0s\n",
      "[CV 4/4] END alpha=92, selection=random, tol=1e-07;, score=-2.457 total time=   0.0s\n",
      "[CV 2/4] END alpha=95, selection=cyclic, tol=0.01;, score=-0.076 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.1, selection=random, tol=1e-05;, score=0.363 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.3, selection=cyclic, tol=1e-05;, score=0.637 total time=   0.0s\n",
      "[CV 2/4] END alpha=89, selection=random, tol=0.01;, score=-0.073 total time=   0.0s\n",
      "[CV 2/4] END alpha=98, selection=random, tol=0.01;, score=-0.077 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.5, selection=cyclic, tol=0.01;, score=-0.677 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.9, selection=cyclic, tol=1e-05;, score=0.524 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.3, selection=random, tol=0.01;, score=-0.771 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.8, selection=cyclic, tol=1e-05;, score=0.577 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.9, selection=cyclic, tol=1e-05;, score=0.392 total time=   0.0s\n",
      "[CV 1/4] END alpha=96, selection=random, tol=0.01;, score=-0.713 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.1, selection=random, tol=1e-05;, score=-0.848 total time=   0.0s\n",
      "[CV 2/4] END alpha=84, selection=random, tol=0.01;, score=-0.072 total time=   0.0s\n",
      "[CV 3/4] END alpha=89, selection=random, tol=0.01;, score=-0.160 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.3, selection=cyclic, tol=1e-05;, score=0.585 total time=   0.0s\n",
      "[CV 1/4] END alpha=92, selection=random, tol=0.01;, score=-0.718 total time=   0.0s\n",
      "[CV 3/4] END alpha=98, selection=random, tol=0.01;, score=-0.161 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.4, selection=cyclic, tol=1e-05;, score=0.627 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.8, selection=cyclic, tol=1e-05;, score=0.526 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.5, selection=random, tol=1e-05;, score=0.617 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.9, selection=cyclic, tol=1e-05;, score=-0.368 total time=   0.0s\n",
      "[CV 3/4] END alpha=95, selection=cyclic, tol=0.01;, score=-0.161 total time=   0.0s\n",
      "[CV 2/4] END alpha=96, selection=random, tol=0.01;, score=-0.076 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.9, selection=cyclic, tol=1e-07;, score=0.560 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.1, selection=random, tol=1e-07;, score=0.654 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.6, selection=random, tol=1e-05;, score=0.405 total time=   0.0s\n",
      "[CV 2/4] END alpha=92, selection=random, tol=0.01;, score=-0.072 total time=   0.0s[CV 3/4] END alpha=0.3, selection=cyclic, tol=1e-05;, score=0.389 total time=   0.0s\n",
      "\n",
      "[CV 4/4] END alpha=98, selection=random, tol=0.01;, score=-2.553 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.4, selection=cyclic, tol=1e-05;, score=0.564 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.0, selection=cyclic, tol=1e-07;, score=0.602 total time=   0.0s\n",
      "[CV 3/4] END alpha=96, selection=random, tol=0.01;, score=-0.161 total time=   0.0s\n",
      "[CV 1/4] END alpha=97, selection=random, tol=1e-07;, score=-0.712 total time=   0.0s\n",
      "[CV 3/4] END alpha=84, selection=random, tol=0.01;, score=-0.159 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.5, selection=random, tol=1e-05;, score=0.541 total time=   0.0s\n",
      "[CV 1/4] END alpha=99, selection=cyclic, tol=1e-05;, score=-0.709 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.3, selection=cyclic, tol=1e-05;, score=-0.772 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.1, selection=random, tol=1e-07;, score=0.599 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.6, selection=random, tol=1e-05;, score=-0.595 total time=   0.0s\n",
      "[CV 4/4] END alpha=96, selection=random, tol=0.01;, score=-2.521 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.4, selection=cyclic, tol=1e-05;, score=0.397 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.9, selection=cyclic, tol=1e-07;, score=0.524 total time=   0.0s\n",
      "[CV 3/4] END alpha=92, selection=random, tol=0.01;, score=-0.161 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.8, selection=cyclic, tol=1e-05;, score=0.400 total time=   0.0s\n",
      "[CV 4/4] END alpha=95, selection=cyclic, tol=0.01;, score=-2.505 total time=   0.0s\n",
      "[CV 2/4] END alpha=97, selection=random, tol=1e-07;, score=-0.077 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.5, selection=random, tol=1e-05;, score=0.403 total time=   0.0s\n",
      "[CV 2/4] END alpha=99, selection=cyclic, tol=1e-05;, score=-0.077 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.3, selection=cyclic, tol=1e-07;, score=0.637 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.1, selection=random, tol=1e-07;, score=0.363 total time=   0.0s[CV 4/4] END alpha=0.4, selection=cyclic, tol=1e-05;, score=-0.775 total time=   0.0s\n",
      "\n",
      "[CV 4/4] END alpha=84, selection=random, tol=0.01;, score=-2.332 total time=   0.0s\n",
      "[CV 3/4] END alpha=97, selection=random, tol=1e-07;, score=-0.162 total time=   0.0s\n",
      "[CV 1/4] END alpha=95, selection=random, tol=1e-05;, score=-0.714 total time=   0.0s\n",
      "[CV 4/4] END alpha=89, selection=random, tol=0.01;, score=-2.410 total time=   0.0s\n",
      "[CV 4/4] END alpha=92, selection=random, tol=0.01;, score=-2.457 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.0, selection=cyclic, tol=1e-07;, score=0.604 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.8, selection=cyclic, tol=1e-05;, score=-0.438 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.5, selection=random, tol=1e-05;, score=-0.682 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.3, selection=cyclic, tol=1e-07;, score=0.585 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.9, selection=cyclic, tol=1e-07;, score=0.392 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.1, selection=random, tol=1e-07;, score=-0.848 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.4, selection=cyclic, tol=1e-07;, score=0.627 total time=   0.0s\n",
      "[CV 4/4] END alpha=97, selection=random, tol=1e-07;, score=-2.537 total time=   0.0s[CV 1/4] END alpha=0.8, selection=cyclic, tol=1e-07;, score=0.577 total time=   0.0s\n",
      "\n",
      "[CV 1/4] END alpha=90, selection=cyclic, tol=1e-05;, score=-0.721 total time=   0.0s\n",
      "[CV 2/4] END alpha=95, selection=random, tol=1e-05;, score=-0.076 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.9, selection=cyclic, tol=0.01;, score=0.560 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.5, selection=random, tol=1e-07;, score=0.617 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.9, selection=cyclic, tol=1e-07;, score=-0.368 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.3, selection=cyclic, tol=1e-07;, score=0.389 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.6, selection=random, tol=1e-07;, score=0.605 total time=   0.0s\n",
      "[CV 3/4] END alpha=99, selection=cyclic, tol=1e-05;, score=-0.162 total time=   0.0s\n",
      "[CV 1/4] END alpha=97, selection=random, tol=0.01;, score=-0.717 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.4, selection=cyclic, tol=1e-07;, score=0.564 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.1, selection=random, tol=0.01;, score=0.653 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.9, selection=cyclic, tol=0.01;, score=0.523 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.9, selection=cyclic, tol=0.01;, score=0.390 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.0, selection=cyclic, tol=1e-07;, score=0.359 total time=   0.0s\n",
      "[CV 3/4] END alpha=95, selection=random, tol=1e-05;, score=-0.161 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.3, selection=cyclic, tol=1e-07;, score=-0.772 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.8, selection=cyclic, tol=1e-07;, score=0.526 total time=   0.0s\n",
      "[CV 2/4] END alpha=90, selection=cyclic, tol=1e-05;, score=-0.074 total time=   0.0s\n",
      "[CV 2/4] END alpha=97, selection=random, tol=0.01;, score=-0.078 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.5, selection=random, tol=1e-07;, score=0.541 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.4, selection=cyclic, tol=1e-07;, score=0.397 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.1, selection=random, tol=0.01;, score=0.599 total time=   0.0s\n",
      "[CV 4/4] END alpha=95, selection=random, tol=1e-05;, score=-2.505 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.3, selection=cyclic, tol=0.01;, score=0.637 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.9, selection=cyclic, tol=0.01;, score=-0.363 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.8, selection=cyclic, tol=1e-07;, score=0.400 total time=   0.0s\n",
      "[CV 3/4] END alpha=97, selection=random, tol=0.01;, score=-0.161 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.6, selection=random, tol=1e-07;, score=0.531 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.4, selection=cyclic, tol=1e-07;, score=-0.775 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.5, selection=random, tol=1e-07;, score=0.403 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.1, selection=random, tol=0.01;, score=0.362 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.9, selection=random, tol=1e-05;, score=0.560 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.0, selection=cyclic, tol=1e-07;, score=-1.109 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.3, selection=cyclic, tol=0.01;, score=0.586 total time=   0.0s\n",
      "[CV 3/4] END alpha=90, selection=cyclic, tol=1e-05;, score=-0.161 total time=   0.0s\n",
      "[CV 4/4] END alpha=97, selection=random, tol=0.01;, score=-2.537 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.8, selection=cyclic, tol=1e-07;, score=-0.438 total time=   0.0s[CV 3/4] END alpha=0.9, selection=random, tol=1e-05;, score=0.392 total time=   0.0s\n",
      "\n",
      "[CV 2/4] END alpha=0.9, selection=random, tol=1e-05;, score=0.524 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.1, selection=random, tol=0.01;, score=-0.843 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.4, selection=cyclic, tol=0.01;, score=0.627 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.5, selection=random, tol=1e-07;, score=-0.682 total time=   0.0s\n",
      "[CV 4/4] END alpha=99, selection=cyclic, tol=1e-05;, score=-2.569 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.3, selection=cyclic, tol=0.01;, score=0.388 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.9, selection=random, tol=1e-05;, score=-0.368 total time=   0.0s\n",
      "[CV 1/4] END alpha=98, selection=cyclic, tol=1e-05;, score=-0.710 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.6, selection=random, tol=1e-07;, score=0.405 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.8, selection=cyclic, tol=0.01;, score=0.576 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.4, selection=cyclic, tol=0.01;, score=0.564 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.2, selection=cyclic, tol=1e-05;, score=0.644 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.9, selection=random, tol=1e-07;, score=0.560 total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.884e+03, tolerance: 3.891e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.694e+03, tolerance: 2.696e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.726e+03, tolerance: 2.895e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.028e+03, tolerance: 2.951e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.884e+03, tolerance: 3.891e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.694e+03, tolerance: 2.696e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.726e+03, tolerance: 2.895e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.028e+03, tolerance: 2.951e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.884e+03, tolerance: 3.891e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.694e+03, tolerance: 2.696e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.726e+03, tolerance: 2.895e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/4] END alpha=0.5, selection=random, tol=0.01;, score=0.616 total time=   0.0s\n",
      "[CV 1/4] END alpha=99, selection=cyclic, tol=1e-07;, score=-0.709 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.3, selection=cyclic, tol=0.01;, score=-0.775 total time=   0.0s\n",
      "[CV 2/4] END alpha=98, selection=cyclic, tol=1e-05;, score=-0.077 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.8, selection=cyclic, tol=0.01;, score=0.526 total time=   0.0s\n",
      "[CV 4/4] END alpha=90, selection=cyclic, tol=1e-05;, score=-2.425 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.4, selection=cyclic, tol=0.01;, score=0.396 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.9, selection=random, tol=1e-07;, score=0.524 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.0, selection=cyclic, tol=0.01;, score=0.602 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.5, selection=random, tol=0.01;, score=0.541 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.2, selection=cyclic, tol=1e-05;, score=0.603 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.9, selection=random, tol=1e-07;, score=-0.368 total time=   0.0s\n",
      "[CV 2/4] END alpha=99, selection=cyclic, tol=1e-07;, score=-0.077 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.9, selection=random, tol=0.01;, score=0.390 total time=   0.0s\n",
      "[CV 3/4] END alpha=98, selection=cyclic, tol=1e-05;, score=-0.162 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.9, selection=random, tol=1e-07;, score=0.392 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.3, selection=random, tol=1e-05;, score=0.637 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.9, selection=random, tol=0.01;, score=0.560 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.8, selection=cyclic, tol=0.01;, score=0.397 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.4, selection=cyclic, tol=0.01;, score=-0.771 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.5, selection=random, tol=0.01;, score=0.401 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.2, selection=cyclic, tol=1e-05;, score=0.378 total time=   0.0s\n",
      "[CV 4/4] END alpha=98, selection=cyclic, tol=1e-05;, score=-2.553 total time=   0.0s[CV 3/4] END alpha=99, selection=cyclic, tol=1e-07;, score=-0.162 total time=   0.0s\n",
      "\n",
      "[CV 4/4] END alpha=0.6, selection=random, tol=1e-07;, score=-0.595 total time=   0.0s[CV 4/4] END alpha=0.9, selection=random, tol=0.01;, score=-0.361 total time=   0.0s\n",
      "\n",
      "[CV 2/4] END alpha=0.9, selection=random, tol=0.01;, score=0.523 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.3, selection=random, tol=1e-05;, score=0.585 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.8, selection=cyclic, tol=0.01;, score=-0.433 total time=   0.0s\n",
      "[CV 1/4] END alpha=90, selection=cyclic, tol=1e-07;, score=-0.721 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.4, selection=random, tol=1e-05;, score=0.627 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.5, selection=random, tol=0.01;, score=-0.670 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.2, selection=cyclic, tol=1e-05;, score=-0.776 total time=   0.0s\n",
      "[CV 1/4] END alpha=98, selection=cyclic, tol=1e-07;, score=-0.710 total time=   0.0s\n",
      "[CV 4/4] END alpha=99, selection=cyclic, tol=1e-07;, score=-2.569 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.6, selection=random, tol=0.01;, score=0.605 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.3, selection=random, tol=1e-05;, score=0.389 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.0, selection=cyclic, tol=0.01;, score=0.604 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.8, selection=random, tol=1e-05;, score=0.577 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.4, selection=random, tol=1e-05;, score=0.564 total time=   0.0s\n",
      "[CV 2/4] END alpha=98, selection=cyclic, tol=1e-07;, score=-0.077 total time=   0.0s\n",
      "[CV 2/4] END alpha=90, selection=cyclic, tol=1e-07;, score=-0.074 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.6, selection=cyclic, tol=1e-05;, score=0.605 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.2, selection=cyclic, tol=1e-07;, score=0.644 total time=   0.0s\n",
      "[CV 1/4] END alpha=99, selection=cyclic, tol=0.01;, score=-0.711 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.3, selection=random, tol=1e-05;, score=-0.772 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.6, selection=random, tol=0.01;, score=0.531 total time=   0.0s\n",
      "[CV 3/4] END alpha=98, selection=cyclic, tol=1e-07;, score=-0.162 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.8, selection=random, tol=1e-05;, score=0.526 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.4, selection=random, tol=1e-05;, score=0.397 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.6, selection=cyclic, tol=1e-05;, score=0.531 total time=   0.0s\n",
      "[CV 3/4] END alpha=90, selection=cyclic, tol=1e-07;, score=-0.161 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.2, selection=cyclic, tol=1e-07;, score=0.603 total time=   0.0s\n",
      "[CV 2/4] END alpha=99, selection=cyclic, tol=0.01;, score=-0.077 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.6, selection=random, tol=0.01;, score=0.403 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.0, selection=cyclic, tol=0.01;, score=0.359 total time=   0.0s\n",
      "[CV 4/4] END alpha=98, selection=cyclic, tol=1e-07;, score=-2.553 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.4, selection=random, tol=1e-05;, score=-0.775 total time=   0.0s\n",
      "[CV 4/4] END alpha=90, selection=cyclic, tol=1e-07;, score=-2.425 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.8, selection=random, tol=1e-05;, score=0.400 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.6, selection=cyclic, tol=1e-05;, score=0.405 total time=   0.0s\n",
      "[CV 3/4] END alpha=99, selection=cyclic, tol=0.01;, score=-0.162 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.6, selection=random, tol=0.01;, score=-0.608 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.2, selection=cyclic, tol=1e-07;, score=0.378 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.4, selection=random, tol=1e-07;, score=0.627 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.8, selection=random, tol=1e-05;, score=-0.438 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.6, selection=cyclic, tol=1e-05;, score=-0.595 total time=   0.0s\n",
      "[CV 4/4] END alpha=99, selection=cyclic, tol=0.01;, score=-2.569 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.7, selection=cyclic, tol=1e-05;, score=0.591 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.2, selection=cyclic, tol=1e-07;, score=-0.776 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.0, selection=cyclic, tol=0.01;, score=-1.109 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.4, selection=random, tol=1e-07;, score=0.564 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.6, selection=cyclic, tol=1e-07;, score=0.605 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.8, selection=random, tol=1e-07;, score=0.577 total time=   0.0s\n",
      "[CV 1/4] END alpha=99, selection=random, tol=1e-05;, score=-0.709 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.7, selection=cyclic, tol=1e-05;, score=0.529 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.4, selection=random, tol=1e-07;, score=0.397 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.6, selection=cyclic, tol=1e-07;, score=0.531 total time=   0.0s\n",
      "[CV 2/4] END alpha=99, selection=random, tol=1e-05;, score=-0.077 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.8, selection=random, tol=1e-07;, score=0.526 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.7, selection=cyclic, tol=1e-05;, score=0.404 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.4, selection=random, tol=1e-07;, score=-0.775 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.0, selection=random, tol=1e-05;, score=0.602 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.6, selection=cyclic, tol=1e-07;, score=0.405 total time=   0.0s\n",
      "[CV 3/4] END alpha=99, selection=random, tol=1e-05;, score=-0.162 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.4, selection=random, tol=0.01;, score=0.627 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.8, selection=random, tol=1e-07;, score=0.400 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.7, selection=cyclic, tol=1e-05;, score=-0.514 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.6, selection=cyclic, tol=1e-07;, score=-0.595 total time=   0.0s\n",
      "[CV 4/4] END alpha=99, selection=random, tol=1e-05;, score=-2.569 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.4, selection=random, tol=0.01;, score=0.564 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.7, selection=cyclic, tol=1e-07;, score=0.591 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.0, selection=random, tol=1e-05;, score=0.604 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.4, selection=random, tol=0.01;, score=0.397 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.8, selection=random, tol=1e-07;, score=-0.438 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.7, selection=cyclic, tol=1e-07;, score=0.529 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.4, selection=random, tol=0.01;, score=-0.768 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.8, selection=random, tol=0.01;, score=0.577 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.7, selection=cyclic, tol=1e-07;, score=0.404 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.0, selection=random, tol=1e-05;, score=0.359 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.8, selection=random, tol=0.01;, score=0.526 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.7, selection=cyclic, tol=1e-07;, score=-0.514 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.7, selection=cyclic, tol=0.01;, score=0.591 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.0, selection=random, tol=1e-05;, score=-1.109 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.7, selection=cyclic, tol=0.01;, score=0.528 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.8, selection=random, tol=0.01;, score=0.396 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.7, selection=cyclic, tol=0.01;, score=0.401 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.8, selection=random, tol=0.01;, score=-0.434 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.0, selection=random, tol=1e-07;, score=0.602 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.7, selection=cyclic, tol=0.01;, score=-0.509 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.7, selection=random, tol=1e-05;, score=0.591 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.0, selection=random, tol=1e-07;, score=0.604 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.7, selection=random, tol=1e-05;, score=0.529 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.7, selection=random, tol=1e-05;, score=0.404 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.0, selection=random, tol=1e-07;, score=0.359 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.7, selection=random, tol=1e-05;, score=-0.514 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.0, selection=random, tol=1e-07;, score=-1.109 total time=   0.0s\n",
      "[CV 1/4] END alpha=0.0, selection=random, tol=0.01;, score=0.602 total time=   0.0s\n",
      "[CV 2/4] END alpha=0.0, selection=random, tol=0.01;, score=0.604 total time=   0.0s\n",
      "[CV 3/4] END alpha=0.0, selection=random, tol=0.01;, score=0.359 total time=   0.0s\n",
      "[CV 4/4] END alpha=0.0, selection=random, tol=0.01;, score=-1.109 total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.028e+03, tolerance: 2.951e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.884e+03, tolerance: 3.891e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.694e+03, tolerance: 2.696e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.726e+03, tolerance: 2.895e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.028e+03, tolerance: 2.951e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.884e+03, tolerance: 3.891e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.694e+03, tolerance: 2.696e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.726e+03, tolerance: 2.895e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.028e+03, tolerance: 2.951e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.884e+03, tolerance: 3.891e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.694e+03, tolerance: 2.696e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.726e+03, tolerance: 2.895e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.028e+03, tolerance: 2.951e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score=-1000, estimator=Lasso(), n_jobs=-1,\n",
       "             param_grid={'alpha': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,\n",
       "                                   14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,\n",
       "                                   25, 26, 27, 28, 29, 30, ...],\n",
       "                         'selection': ['cyclic', 'random'],\n",
       "                         'tol': [1e-05, 1e-07, 0.01]},\n",
       "             scoring='r2', verbose=5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "519a71d6-2016-4d41-aaa6-f03b50eb3549",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:48:02.849259Z",
     "start_time": "2024-10-05T13:48:02.846615Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:38:24.469558Z",
     "iopub.status.busy": "2023-02-25T15:38:24.469336Z",
     "iopub.status.idle": "2023-02-25T15:38:24.473244Z",
     "shell.execute_reply": "2023-02-25T15:38:24.472666Z",
     "shell.execute_reply.started": "2023-02-25T15:38:24.469542Z"
    },
    "id": "519a71d6-2016-4d41-aaa6-f03b50eb3549",
    "outputId": "9e859f48-7f69-4989-90c3-1e47d986d8c8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.29678944695787396\n"
     ]
    }
   ],
   "source": [
    "print(\"Best score: \" + str(search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ee2497f-06d2-40ac-8374-0ea6d5d4462f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:48:02.946898Z",
     "start_time": "2024-10-05T13:48:02.943781Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:40:44.641790Z",
     "iopub.status.busy": "2023-02-25T15:40:44.641264Z",
     "iopub.status.idle": "2023-02-25T15:40:44.646852Z",
     "shell.execute_reply": "2023-02-25T15:40:44.646173Z",
     "shell.execute_reply.started": "2023-02-25T15:40:44.641762Z"
    },
    "id": "8ee2497f-06d2-40ac-8374-0ea6d5d4462f",
    "outputId": "55467113-13ba-4d5b-b6da-8b0ef0ba475c",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=2, selection='random', tol=0.01)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb35071c-f9c3-4f0f-b274-59a3724eff74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:48:03.044898Z",
     "start_time": "2024-10-05T13:48:03.041801Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:40:57.696310Z",
     "iopub.status.busy": "2023-02-25T15:40:57.695983Z",
     "iopub.status.idle": "2023-02-25T15:40:57.700688Z",
     "shell.execute_reply": "2023-02-25T15:40:57.700206Z",
     "shell.execute_reply.started": "2023-02-25T15:40:57.696287Z"
    },
    "id": "bb35071c-f9c3-4f0f-b274-59a3724eff74",
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary = pd.DataFrame(search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5998f6b5-f6a9-46ca-a6d8-93798bf3420c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:48:03.175957Z",
     "start_time": "2024-10-05T13:48:03.149574Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:42:03.139698Z",
     "iopub.status.busy": "2023-02-25T15:42:03.139430Z",
     "iopub.status.idle": "2023-02-25T15:42:03.159496Z",
     "shell.execute_reply": "2023-02-25T15:42:03.158981Z",
     "shell.execute_reply.started": "2023-02-25T15:42:03.139681Z"
    },
    "id": "5998f6b5-f6a9-46ca-a6d8-93798bf3420c",
    "outputId": "252038ee-0a1f-4e72-a210-db8f8c1f9615",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>param_selection</th>\n",
       "      <th>param_tol</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.006326</td>\n",
       "      <td>0.002748</td>\n",
       "      <td>0.006760</td>\n",
       "      <td>0.003892</td>\n",
       "      <td>2</td>\n",
       "      <td>random</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'alpha': 2, 'selection': 'random', 'tol': 0.01}</td>\n",
       "      <td>0.404666</td>\n",
       "      <td>0.473217</td>\n",
       "      <td>0.278515</td>\n",
       "      <td>0.030760</td>\n",
       "      <td>0.296789</td>\n",
       "      <td>0.168723</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.007401</td>\n",
       "      <td>0.004726</td>\n",
       "      <td>0.003622</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>2</td>\n",
       "      <td>cyclic</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'alpha': 2, 'selection': 'cyclic', 'tol': 0.01}</td>\n",
       "      <td>0.400732</td>\n",
       "      <td>0.472319</td>\n",
       "      <td>0.281414</td>\n",
       "      <td>0.026274</td>\n",
       "      <td>0.295185</td>\n",
       "      <td>0.169573</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.013189</td>\n",
       "      <td>0.005749</td>\n",
       "      <td>0.003456</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>2</td>\n",
       "      <td>cyclic</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'alpha': 2, 'selection': 'cyclic', 'tol': 1e-07}</td>\n",
       "      <td>0.403640</td>\n",
       "      <td>0.473144</td>\n",
       "      <td>0.278769</td>\n",
       "      <td>0.024032</td>\n",
       "      <td>0.294896</td>\n",
       "      <td>0.171191</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.006665</td>\n",
       "      <td>0.002769</td>\n",
       "      <td>0.007560</td>\n",
       "      <td>0.006657</td>\n",
       "      <td>2</td>\n",
       "      <td>random</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'alpha': 2, 'selection': 'random', 'tol': 1e-07}</td>\n",
       "      <td>0.403640</td>\n",
       "      <td>0.473144</td>\n",
       "      <td>0.278769</td>\n",
       "      <td>0.024032</td>\n",
       "      <td>0.294896</td>\n",
       "      <td>0.171191</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.008086</td>\n",
       "      <td>0.005202</td>\n",
       "      <td>0.003894</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>2</td>\n",
       "      <td>random</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>{'alpha': 2, 'selection': 'random', 'tol': 1e-05}</td>\n",
       "      <td>0.403635</td>\n",
       "      <td>0.473143</td>\n",
       "      <td>0.278766</td>\n",
       "      <td>0.024038</td>\n",
       "      <td>0.294896</td>\n",
       "      <td>0.171188</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>0.006186</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>0.007434</td>\n",
       "      <td>0.004146</td>\n",
       "      <td>99</td>\n",
       "      <td>random</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'alpha': 99, 'selection': 'random', 'tol': 1e...</td>\n",
       "      <td>-0.709215</td>\n",
       "      <td>-0.077437</td>\n",
       "      <td>-0.161790</td>\n",
       "      <td>-2.568880</td>\n",
       "      <td>-0.879330</td>\n",
       "      <td>1.005163</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.003148</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>99</td>\n",
       "      <td>cyclic</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'alpha': 99, 'selection': 'cyclic', 'tol': 1e...</td>\n",
       "      <td>-0.709215</td>\n",
       "      <td>-0.077437</td>\n",
       "      <td>-0.161790</td>\n",
       "      <td>-2.568880</td>\n",
       "      <td>-0.879330</td>\n",
       "      <td>1.005163</td>\n",
       "      <td>651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>0.005999</td>\n",
       "      <td>0.004045</td>\n",
       "      <td>0.007466</td>\n",
       "      <td>0.008058</td>\n",
       "      <td>99</td>\n",
       "      <td>cyclic</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>{'alpha': 99, 'selection': 'cyclic', 'tol': 1e...</td>\n",
       "      <td>-0.709215</td>\n",
       "      <td>-0.077437</td>\n",
       "      <td>-0.161790</td>\n",
       "      <td>-2.568880</td>\n",
       "      <td>-0.879331</td>\n",
       "      <td>1.005163</td>\n",
       "      <td>652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>0.003591</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.002947</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>99</td>\n",
       "      <td>cyclic</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'alpha': 99, 'selection': 'cyclic', 'tol': 0.01}</td>\n",
       "      <td>-0.711440</td>\n",
       "      <td>-0.077355</td>\n",
       "      <td>-0.162067</td>\n",
       "      <td>-2.568880</td>\n",
       "      <td>-0.879936</td>\n",
       "      <td>1.005037</td>\n",
       "      <td>653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>0.006083</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.003933</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>99</td>\n",
       "      <td>random</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'alpha': 99, 'selection': 'random', 'tol': 0.01}</td>\n",
       "      <td>-0.770103</td>\n",
       "      <td>-0.074714</td>\n",
       "      <td>-0.161587</td>\n",
       "      <td>-2.568880</td>\n",
       "      <td>-0.893821</td>\n",
       "      <td>1.003523</td>\n",
       "      <td>654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>654 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time param_alpha  \\\n",
       "11        0.006326      0.002748         0.006760        0.003892           2   \n",
       "8         0.007401      0.004726         0.003622        0.000221           2   \n",
       "7         0.013189      0.005749         0.003456        0.000261           2   \n",
       "10        0.006665      0.002769         0.007560        0.006657           2   \n",
       "9         0.008086      0.005202         0.003894        0.000845           2   \n",
       "..             ...           ...              ...             ...         ...   \n",
       "592       0.006186      0.000867         0.007434        0.004146          99   \n",
       "589       0.004393      0.000295         0.003148        0.000161          99   \n",
       "588       0.005999      0.004045         0.007466        0.008058          99   \n",
       "590       0.003591      0.000196         0.002947        0.000036          99   \n",
       "593       0.006083      0.001116         0.003933        0.000444          99   \n",
       "\n",
       "    param_selection param_tol  \\\n",
       "11           random      0.01   \n",
       "8            cyclic      0.01   \n",
       "7            cyclic       0.0   \n",
       "10           random       0.0   \n",
       "9            random   0.00001   \n",
       "..              ...       ...   \n",
       "592          random       0.0   \n",
       "589          cyclic       0.0   \n",
       "588          cyclic   0.00001   \n",
       "590          cyclic      0.01   \n",
       "593          random      0.01   \n",
       "\n",
       "                                                params  split0_test_score  \\\n",
       "11    {'alpha': 2, 'selection': 'random', 'tol': 0.01}           0.404666   \n",
       "8     {'alpha': 2, 'selection': 'cyclic', 'tol': 0.01}           0.400732   \n",
       "7    {'alpha': 2, 'selection': 'cyclic', 'tol': 1e-07}           0.403640   \n",
       "10   {'alpha': 2, 'selection': 'random', 'tol': 1e-07}           0.403640   \n",
       "9    {'alpha': 2, 'selection': 'random', 'tol': 1e-05}           0.403635   \n",
       "..                                                 ...                ...   \n",
       "592  {'alpha': 99, 'selection': 'random', 'tol': 1e...          -0.709215   \n",
       "589  {'alpha': 99, 'selection': 'cyclic', 'tol': 1e...          -0.709215   \n",
       "588  {'alpha': 99, 'selection': 'cyclic', 'tol': 1e...          -0.709215   \n",
       "590  {'alpha': 99, 'selection': 'cyclic', 'tol': 0.01}          -0.711440   \n",
       "593  {'alpha': 99, 'selection': 'random', 'tol': 0.01}          -0.770103   \n",
       "\n",
       "     split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
       "11            0.473217           0.278515           0.030760         0.296789   \n",
       "8             0.472319           0.281414           0.026274         0.295185   \n",
       "7             0.473144           0.278769           0.024032         0.294896   \n",
       "10            0.473144           0.278769           0.024032         0.294896   \n",
       "9             0.473143           0.278766           0.024038         0.294896   \n",
       "..                 ...                ...                ...              ...   \n",
       "592          -0.077437          -0.161790          -2.568880        -0.879330   \n",
       "589          -0.077437          -0.161790          -2.568880        -0.879330   \n",
       "588          -0.077437          -0.161790          -2.568880        -0.879331   \n",
       "590          -0.077355          -0.162067          -2.568880        -0.879936   \n",
       "593          -0.074714          -0.161587          -2.568880        -0.893821   \n",
       "\n",
       "     std_test_score  rank_test_score  \n",
       "11         0.168723                1  \n",
       "8          0.169573                2  \n",
       "7          0.171191                3  \n",
       "10         0.171191                4  \n",
       "9          0.171188                5  \n",
       "..              ...              ...  \n",
       "592        1.005163              650  \n",
       "589        1.005163              651  \n",
       "588        1.005163              652  \n",
       "590        1.005037              653  \n",
       "593        1.003523              654  \n",
       "\n",
       "[654 rows x 15 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.sort_values(by = \"rank_test_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "068702f9-b776-4113-b054-4dc8fdd0d618",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:48:03.296565Z",
     "start_time": "2024-10-05T13:48:03.277529Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:42:29.796848Z",
     "iopub.status.busy": "2023-02-25T15:42:29.796614Z",
     "iopub.status.idle": "2023-02-25T15:42:29.810894Z",
     "shell.execute_reply": "2023-02-25T15:42:29.810422Z",
     "shell.execute_reply.started": "2023-02-25T15:42:29.796832Z"
    },
    "id": "068702f9-b776-4113-b054-4dc8fdd0d618",
    "outputId": "f8c6ff83-1b99-42e5-fb45-e21b80b48945",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>2.31390</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6050</td>\n",
       "      <td>5.880</td>\n",
       "      <td>97.3</td>\n",
       "      <td>2.3887</td>\n",
       "      <td>5.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>348.13</td>\n",
       "      <td>12.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.12083</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4450</td>\n",
       "      <td>8.069</td>\n",
       "      <td>76.0</td>\n",
       "      <td>3.4952</td>\n",
       "      <td>2.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>2.44668</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8710</td>\n",
       "      <td>5.272</td>\n",
       "      <td>94.0</td>\n",
       "      <td>1.7364</td>\n",
       "      <td>5.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>88.63</td>\n",
       "      <td>16.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0.10574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6090</td>\n",
       "      <td>5.983</td>\n",
       "      <td>98.8</td>\n",
       "      <td>1.8681</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>390.11</td>\n",
       "      <td>18.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>3.83684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7700</td>\n",
       "      <td>6.251</td>\n",
       "      <td>91.1</td>\n",
       "      <td>2.2955</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>350.65</td>\n",
       "      <td>14.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>0.05023</td>\n",
       "      <td>35.0</td>\n",
       "      <td>6.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4379</td>\n",
       "      <td>5.706</td>\n",
       "      <td>28.4</td>\n",
       "      <td>6.6407</td>\n",
       "      <td>1.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>16.9</td>\n",
       "      <td>394.02</td>\n",
       "      <td>12.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>0.28955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4890</td>\n",
       "      <td>5.412</td>\n",
       "      <td>9.8</td>\n",
       "      <td>3.5875</td>\n",
       "      <td>4.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>348.93</td>\n",
       "      <td>29.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>0.36894</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4310</td>\n",
       "      <td>8.259</td>\n",
       "      <td>8.4</td>\n",
       "      <td>8.9067</td>\n",
       "      <td>7.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>19.1</td>\n",
       "      <td>396.90</td>\n",
       "      <td>3.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0.27957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5850</td>\n",
       "      <td>5.926</td>\n",
       "      <td>42.6</td>\n",
       "      <td>2.3817</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>0.16211</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4640</td>\n",
       "      <td>6.240</td>\n",
       "      <td>16.3</td>\n",
       "      <td>4.4290</td>\n",
       "      <td>3.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>396.90</td>\n",
       "      <td>6.59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS     NOX     RM   AGE     DIS   RAD    TAX  \\\n",
       "171  2.31390   0.0  19.58   0.0  0.6050  5.880  97.3  2.3887   5.0  403.0   \n",
       "97   0.12083   0.0   2.89   0.0  0.4450  8.069  76.0  3.4952   2.0  276.0   \n",
       "156  2.44668   0.0  19.58   0.0  0.8710  5.272  94.0  1.7364   5.0  403.0   \n",
       "491  0.10574   0.0  27.74   0.0  0.6090  5.983  98.8  1.8681   4.0  711.0   \n",
       "361  3.83684   0.0  18.10   0.0  0.7700  6.251  91.1  2.2955  24.0  666.0   \n",
       "..       ...   ...    ...   ...     ...    ...   ...     ...   ...    ...   \n",
       "331  0.05023  35.0   6.06   0.0  0.4379  5.706  28.4  6.6407   1.0  304.0   \n",
       "214  0.28955   0.0  10.59   0.0  0.4890  5.412   9.8  3.5875   4.0  277.0   \n",
       "253  0.36894  22.0   5.86   0.0  0.4310  8.259   8.4  8.9067   7.0  330.0   \n",
       "494  0.27957   0.0   9.69   0.0  0.5850  5.926  42.6  2.3817   6.0  391.0   \n",
       "271  0.16211  20.0   6.96   0.0  0.4640  6.240  16.3  4.4290   3.0  223.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  \n",
       "171     14.7  348.13  12.03  \n",
       "97      18.0  396.90   4.21  \n",
       "156     14.7   88.63  16.14  \n",
       "491     20.1  390.11  18.07  \n",
       "361     20.2  350.65  14.19  \n",
       "..       ...     ...    ...  \n",
       "331     16.9  394.02  12.43  \n",
       "214     18.6  348.93  29.55  \n",
       "253     19.1  396.90   3.54  \n",
       "494     19.2  396.90  13.59  \n",
       "271     18.6  396.90   6.59  \n",
       "\n",
       "[152 rows x 13 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a9aefd3-483f-4f67-8cf0-515d5720df00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:48:03.524223Z",
     "start_time": "2024-10-05T13:48:03.522194Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:43:16.555888Z",
     "iopub.status.busy": "2023-02-25T15:43:16.555608Z",
     "iopub.status.idle": "2023-02-25T15:43:16.558245Z",
     "shell.execute_reply": "2023-02-25T15:43:16.557808Z",
     "shell.execute_reply.started": "2023-02-25T15:43:16.555870Z"
    },
    "id": "7a9aefd3-483f-4f67-8cf0-515d5720df00",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dc_scores = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afc29ba6-15e6-46e4-b517-e14a4a13ea24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:48:03.638386Z",
     "start_time": "2024-10-05T13:48:03.635864Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:43:17.100281Z",
     "iopub.status.busy": "2023-02-25T15:43:17.099920Z",
     "iopub.status.idle": "2023-02-25T15:43:17.104486Z",
     "shell.execute_reply": "2023-02-25T15:43:17.103706Z",
     "shell.execute_reply.started": "2023-02-25T15:43:17.100259Z"
    },
    "id": "afc29ba6-15e6-46e4-b517-e14a4a13ea24",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dc_scores[str(lasso).split(\"(\")[0]] = {\"model\": search.best_estimator_, \"score\": search.best_score_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46ecf3d6-b4d3-42c9-8834-ff42a8651f1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:48:03.757489Z",
     "start_time": "2024-10-05T13:48:03.753303Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:43:17.579237Z",
     "iopub.status.busy": "2023-02-25T15:43:17.579040Z",
     "iopub.status.idle": "2023-02-25T15:43:17.583118Z",
     "shell.execute_reply": "2023-02-25T15:43:17.582519Z",
     "shell.execute_reply.started": "2023-02-25T15:43:17.579221Z"
    },
    "id": "46ecf3d6-b4d3-42c9-8834-ff42a8651f1a",
    "outputId": "42c6aac9-9cd3-476f-881c-dcb8fbf3ff7c",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Lasso': {'model': Lasso(alpha=2, selection='random', tol=0.01),\n",
       "  'score': 0.29678944695787396}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e400f5-f6d4-4e34-ac15-3fa858d1db3e",
   "metadata": {
    "id": "16e400f5-f6d4-4e34-ac15-3fa858d1db3e"
   },
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8406dbf-2f60-4a2e-acdb-9293c10e611d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:48:03.872044Z",
     "start_time": "2024-10-05T13:48:03.869351Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:43:23.216942Z",
     "iopub.status.busy": "2023-02-25T15:43:23.216490Z",
     "iopub.status.idle": "2023-02-25T15:43:23.219720Z",
     "shell.execute_reply": "2023-02-25T15:43:23.219057Z",
     "shell.execute_reply.started": "2023-02-25T15:43:23.216917Z"
    },
    "id": "a8406dbf-2f60-4a2e-acdb-9293c10e611d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ridge = Ridge()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f91c58f-21d2-405b-a6e8-8575e2ec8aaf",
   "metadata": {
    "id": "5f91c58f-21d2-405b-a6e8-8575e2ec8aaf"
   },
   "source": [
    "#### Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdb7b40-9702-400f-840b-c7b09ec665bf",
   "metadata": {
    "id": "6fdb7b40-9702-400f-840b-c7b09ec665bf"
   },
   "source": [
    "#### Hiperparametrización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad2928fc-352d-4e05-874c-2a1fba4b10df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:48:06.331264Z",
     "start_time": "2024-10-05T13:48:06.327452Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:44:03.154427Z",
     "iopub.status.busy": "2023-02-25T15:44:03.153845Z",
     "iopub.status.idle": "2023-02-25T15:44:03.157769Z",
     "shell.execute_reply": "2023-02-25T15:44:03.157029Z",
     "shell.execute_reply.started": "2023-02-25T15:44:03.154406Z"
    },
    "id": "ad2928fc-352d-4e05-874c-2a1fba4b10df",
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"alpha\": [x for x in range(1, 100)] + [y/10 for y in range(10)],\n",
    "    \"tol\": [0.00001, 0.0000001, 0.01],\n",
    "    \"solver\": ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ff726a0-f038-4463-97c9-8632843b89b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:48:08.662262Z",
     "start_time": "2024-10-05T13:48:08.658077Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:44:26.267429Z",
     "iopub.status.busy": "2023-02-25T15:44:26.267227Z",
     "iopub.status.idle": "2023-02-25T15:44:26.270975Z",
     "shell.execute_reply": "2023-02-25T15:44:26.270497Z",
     "shell.execute_reply.started": "2023-02-25T15:44:26.267413Z"
    },
    "id": "7ff726a0-f038-4463-97c9-8632843b89b8",
    "outputId": "45da0287-4dc7-400f-9f26-f8023014f518",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2289"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod(list(map(len, param_grid.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f3de3c0-e0ea-4e66-987e-4cb9a39c1db2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:48:08.968951Z",
     "start_time": "2024-10-05T13:48:08.965956Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:48:47.694042Z",
     "iopub.status.busy": "2023-02-25T15:48:47.693769Z",
     "iopub.status.idle": "2023-02-25T15:48:47.696844Z",
     "shell.execute_reply": "2023-02-25T15:48:47.696210Z",
     "shell.execute_reply.started": "2023-02-25T15:48:47.694024Z"
    },
    "id": "8f3de3c0-e0ea-4e66-987e-4cb9a39c1db2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 100 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score=-1000, estimator=Ridge(), n_iter=100,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'alpha': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10,\n",
       "                                                  11, 12, 13, 14, 15, 16, 17,\n",
       "                                                  18, 19, 20, 21, 22, 23, 24,\n",
       "                                                  25, 26, 27, 28, 29, 30, ...],\n",
       "                                        'solver': ['auto', 'svd', 'cholesky',\n",
       "                                                   'lsqr', 'sparse_cg', 'sag',\n",
       "                                                   'saga'],\n",
       "                                        'tol': [1e-05, 1e-07, 0.01]},\n",
       "                   scoring='r2', verbose=1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search = RandomizedSearchCV(ridge, param_grid, cv=4, error_score=-1_000, n_jobs=-1, scoring=\"r2\", verbose=1, n_iter=100)\n",
    "search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c172fe1-86aa-40a0-9824-bfa0c3185a54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:48:13.376960Z",
     "start_time": "2024-10-05T13:48:13.373925Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:49:23.177783Z",
     "iopub.status.busy": "2023-02-25T15:49:23.177479Z",
     "iopub.status.idle": "2023-02-25T15:49:23.182829Z",
     "shell.execute_reply": "2023-02-25T15:49:23.182313Z",
     "shell.execute_reply.started": "2023-02-25T15:49:23.177762Z"
    },
    "id": "9c172fe1-86aa-40a0-9824-bfa0c3185a54",
    "outputId": "01c1f8b3-a75f-4f96-de0b-5b1b9daca06d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.4, solver='svd', tol=1e-07)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4a25b9c-816b-4bdd-b29c-4b706c9246ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:48:13.483905Z",
     "start_time": "2024-10-05T13:48:13.480963Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:50:07.793499Z",
     "iopub.status.busy": "2023-02-25T15:50:07.792892Z",
     "iopub.status.idle": "2023-02-25T15:50:07.797912Z",
     "shell.execute_reply": "2023-02-25T15:50:07.797427Z",
     "shell.execute_reply.started": "2023-02-25T15:50:07.793464Z"
    },
    "id": "c4a25b9c-816b-4bdd-b29c-4b706c9246ab",
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary = pd.DataFrame(search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d43a7205-2fc0-4006-8a68-531964e41402",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:48:13.603147Z",
     "start_time": "2024-10-05T13:48:13.586903Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:50:10.807608Z",
     "iopub.status.busy": "2023-02-25T15:50:10.807370Z",
     "iopub.status.idle": "2023-02-25T15:50:10.825560Z",
     "shell.execute_reply": "2023-02-25T15:50:10.824783Z",
     "shell.execute_reply.started": "2023-02-25T15:50:10.807591Z"
    },
    "id": "d43a7205-2fc0-4006-8a68-531964e41402",
    "outputId": "04c91846-9b0d-45e1-f27b-c8428d17f77a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_tol</th>\n",
       "      <th>param_solver</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.003180</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.002077</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.0</td>\n",
       "      <td>svd</td>\n",
       "      <td>0.4</td>\n",
       "      <td>{'tol': 1e-07, 'solver': 'svd', 'alpha': 0.4}</td>\n",
       "      <td>0.509336</td>\n",
       "      <td>0.755953</td>\n",
       "      <td>0.776124</td>\n",
       "      <td>0.755312</td>\n",
       "      <td>0.699181</td>\n",
       "      <td>0.109926</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.002927</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>cholesky</td>\n",
       "      <td>0.7</td>\n",
       "      <td>{'tol': 1e-05, 'solver': 'cholesky', 'alpha': ...</td>\n",
       "      <td>0.507498</td>\n",
       "      <td>0.758810</td>\n",
       "      <td>0.774804</td>\n",
       "      <td>0.754712</td>\n",
       "      <td>0.698956</td>\n",
       "      <td>0.110793</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.006387</td>\n",
       "      <td>0.002696</td>\n",
       "      <td>0.003218</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sparse_cg</td>\n",
       "      <td>0.7</td>\n",
       "      <td>{'tol': 1e-07, 'solver': 'sparse_cg', 'alpha':...</td>\n",
       "      <td>0.507497</td>\n",
       "      <td>0.758810</td>\n",
       "      <td>0.774804</td>\n",
       "      <td>0.754712</td>\n",
       "      <td>0.698955</td>\n",
       "      <td>0.110793</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.004832</td>\n",
       "      <td>0.003090</td>\n",
       "      <td>0.002633</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>cholesky</td>\n",
       "      <td>1</td>\n",
       "      <td>{'tol': 1e-05, 'solver': 'cholesky', 'alpha': 1}</td>\n",
       "      <td>0.506304</td>\n",
       "      <td>0.760362</td>\n",
       "      <td>0.773696</td>\n",
       "      <td>0.754054</td>\n",
       "      <td>0.698604</td>\n",
       "      <td>0.111251</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.006130</td>\n",
       "      <td>0.001132</td>\n",
       "      <td>0.004411</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.01</td>\n",
       "      <td>auto</td>\n",
       "      <td>13</td>\n",
       "      <td>{'tol': 0.01, 'solver': 'auto', 'alpha': 13}</td>\n",
       "      <td>0.509946</td>\n",
       "      <td>0.768638</td>\n",
       "      <td>0.764313</td>\n",
       "      <td>0.749020</td>\n",
       "      <td>0.697979</td>\n",
       "      <td>0.108806</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.007073</td>\n",
       "      <td>0.002516</td>\n",
       "      <td>0.004496</td>\n",
       "      <td>0.003076</td>\n",
       "      <td>0.01</td>\n",
       "      <td>lsqr</td>\n",
       "      <td>38</td>\n",
       "      <td>{'tol': 0.01, 'solver': 'lsqr', 'alpha': 38}</td>\n",
       "      <td>0.505170</td>\n",
       "      <td>0.655216</td>\n",
       "      <td>0.586920</td>\n",
       "      <td>0.617251</td>\n",
       "      <td>0.591139</td>\n",
       "      <td>0.055218</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.004957</td>\n",
       "      <td>0.002575</td>\n",
       "      <td>0.003594</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.01</td>\n",
       "      <td>lsqr</td>\n",
       "      <td>3</td>\n",
       "      <td>{'tol': 0.01, 'solver': 'lsqr', 'alpha': 3}</td>\n",
       "      <td>0.506423</td>\n",
       "      <td>0.654293</td>\n",
       "      <td>0.587133</td>\n",
       "      <td>0.616083</td>\n",
       "      <td>0.590983</td>\n",
       "      <td>0.054321</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.010817</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.010528</td>\n",
       "      <td>0.004412</td>\n",
       "      <td>0.01</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.9</td>\n",
       "      <td>{'tol': 0.01, 'solver': 'saga', 'alpha': 0.9}</td>\n",
       "      <td>0.378755</td>\n",
       "      <td>0.669622</td>\n",
       "      <td>0.563691</td>\n",
       "      <td>0.667842</td>\n",
       "      <td>0.569978</td>\n",
       "      <td>0.118440</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.006273</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.002306</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.01</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'tol': 0.01, 'solver': 'saga', 'alpha': 0.8}</td>\n",
       "      <td>0.378956</td>\n",
       "      <td>0.669501</td>\n",
       "      <td>0.562552</td>\n",
       "      <td>0.668361</td>\n",
       "      <td>0.569842</td>\n",
       "      <td>0.118457</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.007564</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>0.005504</td>\n",
       "      <td>0.003687</td>\n",
       "      <td>0.01</td>\n",
       "      <td>saga</td>\n",
       "      <td>51</td>\n",
       "      <td>{'tol': 0.01, 'solver': 'saga', 'alpha': 51}</td>\n",
       "      <td>0.376490</td>\n",
       "      <td>0.669050</td>\n",
       "      <td>0.563788</td>\n",
       "      <td>0.667192</td>\n",
       "      <td>0.569130</td>\n",
       "      <td>0.119100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_tol  \\\n",
       "84       0.003180      0.000688         0.002077        0.000211       0.0   \n",
       "78       0.002927      0.000691         0.002100        0.000190   0.00001   \n",
       "26       0.006387      0.002696         0.003218        0.000268       0.0   \n",
       "83       0.004832      0.003090         0.002633        0.000382   0.00001   \n",
       "3        0.006130      0.001132         0.004411        0.000591      0.01   \n",
       "..            ...           ...              ...             ...       ...   \n",
       "95       0.007073      0.002516         0.004496        0.003076      0.01   \n",
       "90       0.004957      0.002575         0.003594        0.002219      0.01   \n",
       "8        0.010817      0.001071         0.010528        0.004412      0.01   \n",
       "54       0.006273      0.000564         0.002306        0.000278      0.01   \n",
       "94       0.007564      0.001812         0.005504        0.003687      0.01   \n",
       "\n",
       "   param_solver param_alpha  \\\n",
       "84          svd         0.4   \n",
       "78     cholesky         0.7   \n",
       "26    sparse_cg         0.7   \n",
       "83     cholesky           1   \n",
       "3          auto          13   \n",
       "..          ...         ...   \n",
       "95         lsqr          38   \n",
       "90         lsqr           3   \n",
       "8          saga         0.9   \n",
       "54         saga         0.8   \n",
       "94         saga          51   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "84      {'tol': 1e-07, 'solver': 'svd', 'alpha': 0.4}           0.509336   \n",
       "78  {'tol': 1e-05, 'solver': 'cholesky', 'alpha': ...           0.507498   \n",
       "26  {'tol': 1e-07, 'solver': 'sparse_cg', 'alpha':...           0.507497   \n",
       "83   {'tol': 1e-05, 'solver': 'cholesky', 'alpha': 1}           0.506304   \n",
       "3        {'tol': 0.01, 'solver': 'auto', 'alpha': 13}           0.509946   \n",
       "..                                                ...                ...   \n",
       "95       {'tol': 0.01, 'solver': 'lsqr', 'alpha': 38}           0.505170   \n",
       "90        {'tol': 0.01, 'solver': 'lsqr', 'alpha': 3}           0.506423   \n",
       "8       {'tol': 0.01, 'solver': 'saga', 'alpha': 0.9}           0.378755   \n",
       "54      {'tol': 0.01, 'solver': 'saga', 'alpha': 0.8}           0.378956   \n",
       "94       {'tol': 0.01, 'solver': 'saga', 'alpha': 51}           0.376490   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
       "84           0.755953           0.776124           0.755312         0.699181   \n",
       "78           0.758810           0.774804           0.754712         0.698956   \n",
       "26           0.758810           0.774804           0.754712         0.698955   \n",
       "83           0.760362           0.773696           0.754054         0.698604   \n",
       "3            0.768638           0.764313           0.749020         0.697979   \n",
       "..                ...                ...                ...              ...   \n",
       "95           0.655216           0.586920           0.617251         0.591139   \n",
       "90           0.654293           0.587133           0.616083         0.590983   \n",
       "8            0.669622           0.563691           0.667842         0.569978   \n",
       "54           0.669501           0.562552           0.668361         0.569842   \n",
       "94           0.669050           0.563788           0.667192         0.569130   \n",
       "\n",
       "    std_test_score  rank_test_score  \n",
       "84        0.109926                1  \n",
       "78        0.110793                2  \n",
       "26        0.110793                3  \n",
       "83        0.111251                4  \n",
       "3         0.108806                5  \n",
       "..             ...              ...  \n",
       "95        0.055218               96  \n",
       "90        0.054321               97  \n",
       "8         0.118440               98  \n",
       "54        0.118457               99  \n",
       "94        0.119100              100  \n",
       "\n",
       "[100 rows x 15 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.sort_values(by = \"rank_test_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6cc075c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.4, solver='svd', tol=1e-07)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dd217c9e-e4e4-4617-a077-005021d6787e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:48:13.834469Z",
     "start_time": "2024-10-05T13:48:13.831267Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:51:43.200828Z",
     "iopub.status.busy": "2023-02-25T15:51:43.200527Z",
     "iopub.status.idle": "2023-02-25T15:51:43.204167Z",
     "shell.execute_reply": "2023-02-25T15:51:43.203613Z",
     "shell.execute_reply.started": "2023-02-25T15:51:43.200807Z"
    },
    "id": "dd217c9e-e4e4-4617-a077-005021d6787e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dc_scores[str(ridge).split(\"(\")[0]] = {\"model\": search.best_estimator_, \"score\": search.best_score_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "790559a0-f1ac-4c0c-83a4-168e7a4f57fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:48:13.961228Z",
     "start_time": "2024-10-05T13:48:13.957533Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:51:43.567254Z",
     "iopub.status.busy": "2023-02-25T15:51:43.566986Z",
     "iopub.status.idle": "2023-02-25T15:51:43.572412Z",
     "shell.execute_reply": "2023-02-25T15:51:43.571645Z",
     "shell.execute_reply.started": "2023-02-25T15:51:43.567231Z"
    },
    "id": "790559a0-f1ac-4c0c-83a4-168e7a4f57fc",
    "outputId": "9991e229-43b7-4f06-8df8-33cf38767ebd",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Lasso': {'model': Lasso(alpha=2, selection='random', tol=0.01),\n",
       "  'score': 0.29678944695787396},\n",
       " 'Ridge': {'model': Ridge(alpha=0.4, solver='svd', tol=1e-07),\n",
       "  'score': 0.6991812602048931}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92ca020-281e-4ab4-a857-f062ff8ccb99",
   "metadata": {
    "id": "c92ca020-281e-4ab4-a857-f062ff8ccb99"
   },
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "00874597-30a2-4636-9977-7a7f75084b95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:48:14.079260Z",
     "start_time": "2024-10-05T13:48:14.077345Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:52:08.970443Z",
     "iopub.status.busy": "2023-02-25T15:52:08.969951Z",
     "iopub.status.idle": "2023-02-25T15:52:08.973800Z",
     "shell.execute_reply": "2023-02-25T15:52:08.972817Z",
     "shell.execute_reply.started": "2023-02-25T15:52:08.970419Z"
    },
    "id": "00874597-30a2-4636-9977-7a7f75084b95",
    "tags": []
   },
   "outputs": [],
   "source": [
    "elnet = ElasticNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8128672-d392-46e0-b047-f91224853b9c",
   "metadata": {
    "id": "f8128672-d392-46e0-b047-f91224853b9c"
   },
   "source": [
    "#### Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc72e7af-91cb-4753-8f8a-16e32a1ded87",
   "metadata": {
    "id": "dc72e7af-91cb-4753-8f8a-16e32a1ded87"
   },
   "source": [
    "#### Entrenamiento con datos escalados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affec156-c952-4fa2-a940-b005dd56fd28",
   "metadata": {
    "id": "affec156-c952-4fa2-a940-b005dd56fd28"
   },
   "source": [
    "#### Hiperparametrización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d686d90e-bad3-4b2b-9762-9d83d63e74fb",
   "metadata": {
    "id": "d686d90e-bad3-4b2b-9762-9d83d63e74fb"
   },
   "source": [
    "##### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1f1980ca-4fbd-46d7-a6ec-44e0264867b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:48:14.911071Z",
     "start_time": "2024-10-05T13:48:14.908050Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:52:59.035565Z",
     "iopub.status.busy": "2023-02-25T15:52:59.035102Z",
     "iopub.status.idle": "2023-02-25T15:52:59.038500Z",
     "shell.execute_reply": "2023-02-25T15:52:59.037886Z",
     "shell.execute_reply.started": "2023-02-25T15:52:59.035546Z"
    },
    "id": "1f1980ca-4fbd-46d7-a6ec-44e0264867b2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"alpha\": [x for x in range(1, 100)] + [y/10 for y in range(10)],\n",
    "    \"l1_ratio\": [x/1_00 for x in range(1, 100)] + [y/10 for y in range(10)],\n",
    "    \"selection\": [\"cyclic\", \"random\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff961536-e889-4502-aecb-4cbbc459d4e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:48:15.021835Z",
     "start_time": "2024-10-05T13:48:15.018608Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:52:59.666823Z",
     "iopub.status.busy": "2023-02-25T15:52:59.666368Z",
     "iopub.status.idle": "2023-02-25T15:52:59.670474Z",
     "shell.execute_reply": "2023-02-25T15:52:59.669907Z",
     "shell.execute_reply.started": "2023-02-25T15:52:59.666805Z"
    },
    "id": "ff961536-e889-4502-aecb-4cbbc459d4e9",
    "outputId": "f19ba9a4-815f-4682-c9f8-dc82e798b3c7",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23762"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod(list(map(len, param_grid.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65ca3cb8-8004-4778-99c5-decc6c51c040",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:48:15.133745Z",
     "start_time": "2024-10-05T13:48:15.131171Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:53:27.398240Z",
     "iopub.status.busy": "2023-02-25T15:53:27.397980Z",
     "iopub.status.idle": "2023-02-25T15:53:27.401411Z",
     "shell.execute_reply": "2023-02-25T15:53:27.400694Z",
     "shell.execute_reply.started": "2023-02-25T15:53:27.398224Z"
    },
    "id": "65ca3cb8-8004-4778-99c5-decc6c51c040",
    "tags": []
   },
   "outputs": [],
   "source": [
    "search = GridSearchCV(elnet, param_grid, cv=4, error_score=-1_000, n_jobs=-1, scoring=\"r2\", verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bef6563d-b865-4e2d-b51c-f7babd5dc9e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:48:53.337789Z",
     "start_time": "2024-10-05T13:48:15.240598Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:53:28.044495Z",
     "iopub.status.busy": "2023-02-25T15:53:28.044058Z",
     "iopub.status.idle": "2023-02-25T15:54:30.159695Z",
     "shell.execute_reply": "2023-02-25T15:54:30.159241Z",
     "shell.execute_reply.started": "2023-02-25T15:53:28.044467Z"
    },
    "id": "bef6563d-b865-4e2d-b51c-f7babd5dc9e1",
    "outputId": "370a0271-3b0b-4e25-e7b6-c0bb5b1913ac",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 23762 candidates, totalling 95048 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.638e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.063e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.698e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.818e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.638e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.063e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.698e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.818e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.918e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.370e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.960e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.120e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.918e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.370e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.960e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.120e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.115e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.598e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.153e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.346e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.115e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.598e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.153e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.346e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.270e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.786e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.311e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.534e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.270e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.786e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.311e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.534e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.400e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.948e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.447e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.697e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.400e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.948e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.447e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.697e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.512e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.091e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.567e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.842e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.512e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.091e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.567e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.842e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.610e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.221e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.675e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.972e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.610e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.221e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.675e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.972e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.698e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.339e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.774e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.092e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.698e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.339e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.774e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.092e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.778e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.448e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.865e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.202e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.778e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.448e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.865e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.202e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.850e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.549e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.950e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.304e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.850e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.549e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.950e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.304e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.917e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.643e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.029e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.398e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.917e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.643e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.029e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.398e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.979e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.731e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.102e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.487e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.979e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.731e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.102e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.487e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.037e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.814e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.171e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.571e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.037e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.814e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.171e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.571e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.090e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.891e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.237e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.649e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.090e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.891e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.237e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.649e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.141e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.965e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.298e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.724e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.141e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.965e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.298e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.724e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.188e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.035e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.357e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.794e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.188e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.035e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.357e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.794e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.233e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.101e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.412e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.861e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.233e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.101e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.412e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.861e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.275e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.164e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.465e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.924e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.275e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.164e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.465e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.924e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.315e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.224e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.515e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.985e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.315e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.224e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.515e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.985e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.353e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.281e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.563e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.042e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.353e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.281e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.563e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.042e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.389e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.336e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.609e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.098e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.389e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.336e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.609e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.098e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.424e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.388e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.653e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.150e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.424e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.388e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.653e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.150e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.457e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.438e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.695e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.201e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.457e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.438e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.695e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.201e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.488e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.487e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.736e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.249e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.488e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.487e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.736e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.249e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.518e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.533e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.775e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.296e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.518e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.533e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.775e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.296e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.547e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.577e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.812e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.340e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.547e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.577e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.812e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.340e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.575e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.620e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.848e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.383e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.575e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.620e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.848e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.383e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.602e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.661e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.882e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.424e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.602e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.661e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.882e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.424e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.627e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.701e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.916e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.464e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.627e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.701e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.916e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.464e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.652e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.739e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.948e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.503e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.652e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.739e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.948e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.503e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.676e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.776e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.979e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.540e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.676e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.776e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.979e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.540e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.699e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.811e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.009e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.575e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.699e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.811e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.009e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.575e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.721e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.846e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.038e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.610e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.721e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.846e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.038e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.610e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.742e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.879e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.066e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.643e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.742e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.879e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.066e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.643e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.763e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.911e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.094e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.676e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.763e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.911e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.094e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.676e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.783e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.943e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.120e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.707e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.783e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.943e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.120e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.707e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.802e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.973e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.146e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.802e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.973e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.146e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.821e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.002e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.171e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.767e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.821e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.002e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.171e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.767e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.839e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.031e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.195e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.795e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.839e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.031e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.195e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.795e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.856e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.058e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.218e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.823e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.856e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.058e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.218e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.823e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.874e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.085e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.241e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.850e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.874e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.085e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.241e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.850e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.890e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.111e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.263e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.876e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.890e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.111e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.263e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.876e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.906e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.137e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.284e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.901e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.906e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.137e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.284e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.901e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.922e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.161e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.305e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.926e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.922e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.161e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.305e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.926e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.937e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.185e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.326e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.950e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.937e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.185e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.326e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.950e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.952e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.209e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.346e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.973e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.952e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.209e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.346e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.973e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.966e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.232e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.365e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.996e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.966e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.232e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.365e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.996e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.980e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.254e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.384e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.018e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.980e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.254e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.384e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.018e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.994e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.275e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.402e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.040e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.994e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.275e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.402e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.040e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.008e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.296e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.420e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.061e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.008e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.296e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.420e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.061e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.317e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.438e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.081e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.317e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.438e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.081e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.033e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.337e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.455e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.102e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.033e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.337e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.455e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.102e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.046e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.357e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.471e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.121e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.046e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.357e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.471e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.121e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.058e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.376e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.488e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.140e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.058e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.376e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.488e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.140e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.070e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.394e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.504e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.159e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.070e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.394e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.504e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.159e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.081e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.413e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.519e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.177e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.081e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.413e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.519e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.177e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.093e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.430e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.534e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.195e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.093e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.430e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.534e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.195e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.104e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.448e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.549e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.212e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.104e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.448e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.549e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.212e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.114e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.465e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.564e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.229e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.114e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.465e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.564e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.229e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.125e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.482e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.578e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.246e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.125e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.482e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.578e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.246e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.135e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.498e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.592e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.262e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.135e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.498e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.592e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.262e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.145e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.514e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.606e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.278e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.145e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.514e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.606e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.278e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.155e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.529e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.619e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.293e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.155e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.529e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.619e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.293e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.165e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.545e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.632e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.308e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.165e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.545e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.632e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.308e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.175e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.560e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.645e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.323e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.175e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.560e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.645e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.323e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.184e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.574e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.658e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.338e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.184e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.574e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.658e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.338e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.193e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.589e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.670e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.352e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.193e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.589e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.670e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.352e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.202e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.603e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.682e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.366e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.202e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.603e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.682e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.366e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.211e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.617e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.694e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.380e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.211e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.617e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.694e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.380e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.220e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.630e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.706e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.394e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.220e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.630e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.706e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.394e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.228e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.644e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.717e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.407e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.228e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.644e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.717e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.407e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.236e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.657e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.729e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.420e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.236e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.657e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.729e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.420e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.244e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.669e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.740e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.432e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.244e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.669e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.740e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.432e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.253e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.682e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.750e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.445e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.253e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.682e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.750e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.445e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.260e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.694e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.761e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.457e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.260e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.694e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.761e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.457e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.268e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.706e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.772e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.469e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.268e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.706e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.772e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.469e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.276e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.718e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.782e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.481e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.276e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.718e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.782e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.481e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.283e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.730e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.792e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.493e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.283e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.730e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.792e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.493e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.291e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.742e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.802e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.504e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.291e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.742e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.802e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.504e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.298e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.753e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.812e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.515e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.298e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.753e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.812e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.515e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.305e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.764e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.821e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.526e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.305e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.764e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.821e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.526e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.312e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.775e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.831e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.537e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.312e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.775e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.831e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.537e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.319e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.786e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.840e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.548e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.319e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.786e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.840e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.548e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.326e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.796e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.849e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.558e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.326e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.796e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.849e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.558e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.332e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.806e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.858e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.568e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.332e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.806e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.858e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.568e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.339e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.817e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.867e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.578e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.339e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.817e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.867e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.578e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.345e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.827e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.876e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.588e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.345e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.827e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.876e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.588e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.352e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.837e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.884e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.598e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.352e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.837e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.884e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.598e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.358e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.846e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.893e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.608e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.358e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.846e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.893e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.608e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.364e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.856e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.901e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.617e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.364e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.856e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.901e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.617e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.370e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.865e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.909e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.627e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.370e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.865e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.909e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.627e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.376e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.875e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.917e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.636e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.376e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.875e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.917e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.636e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.382e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.884e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.925e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.382e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.884e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.925e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.388e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.893e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.933e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.654e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.388e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.893e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.933e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.654e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.394e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.902e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.941e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.662e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.394e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.902e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.941e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.662e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.399e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.910e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.948e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.671e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.399e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.910e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.948e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.671e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.405e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.919e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.956e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.679e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.405e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.919e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.956e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.679e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.410e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.927e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.963e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.688e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.410e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.927e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.963e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.688e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.416e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.936e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.971e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.696e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.416e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.936e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.971e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.696e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:681: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.069e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.481e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.253e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.069e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.481e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.253e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.195e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.607e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.314e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.376e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.195e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.607e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.314e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.376e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.286e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.698e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.389e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.464e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.286e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.698e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.389e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.464e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.358e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.771e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.450e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.535e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.358e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.771e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.450e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.535e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.418e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.833e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.502e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.595e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.418e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.833e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.502e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.595e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.471e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.887e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.549e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.648e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.471e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.887e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.549e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.648e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.518e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.937e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.590e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.696e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.518e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.937e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.590e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.696e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.561e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.982e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.629e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.740e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.561e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.982e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.629e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.740e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.601e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.665e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.601e+03, tolerance: 1.925e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.665e+03, tolerance: 2.139e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.024e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.024e+03, tolerance: 2.362e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.780e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.780e+03, tolerance: 2.189e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/model_selection/_search.py:926: UserWarning:\n",
      "\n",
      "With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning:\n",
      "\n",
      "Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning:\n",
      "\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.693e+03, tolerance: 2.873e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score=-1000, estimator=ElasticNet(), n_jobs=-1,\n",
       "             param_grid={'alpha': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,\n",
       "                                   14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,\n",
       "                                   25, 26, 27, 28, 29, 30, ...],\n",
       "                         'l1_ratio': [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07,\n",
       "                                      0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14,\n",
       "                                      0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21,\n",
       "                                      0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28,\n",
       "                                      0.29, 0.3, ...],\n",
       "                         'selection': ['cyclic', 'random']},\n",
       "             scoring='r2', verbose=1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f229339c-dd08-494c-96bb-8da0fc42760c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:48:53.504359Z",
     "start_time": "2024-10-05T13:48:53.500800Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:55:33.661253Z",
     "iopub.status.busy": "2023-02-25T15:55:33.661035Z",
     "iopub.status.idle": "2023-02-25T15:55:33.665697Z",
     "shell.execute_reply": "2023-02-25T15:55:33.664847Z",
     "shell.execute_reply.started": "2023-02-25T15:55:33.661237Z"
    },
    "id": "f229339c-dd08-494c-96bb-8da0fc42760c",
    "outputId": "ba47405d-dfe0-4a97-fa33-f36c998c16c7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.6977964719438308\n"
     ]
    }
   ],
   "source": [
    "print(\"Best score: \" + str(search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6d2afb01-09c3-4137-a208-aa8a1fc93f42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:48:53.774575Z",
     "start_time": "2024-10-05T13:48:53.770517Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:55:37.054490Z",
     "iopub.status.busy": "2023-02-25T15:55:37.054263Z",
     "iopub.status.idle": "2023-02-25T15:55:37.058848Z",
     "shell.execute_reply": "2023-02-25T15:55:37.058368Z",
     "shell.execute_reply.started": "2023-02-25T15:55:37.054474Z"
    },
    "id": "6d2afb01-09c3-4137-a208-aa8a1fc93f42",
    "outputId": "dd148ed5-6fbb-4606-cad9-7d6f34fadafd",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNet(alpha=0.0, l1_ratio=0.34, selection='random')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3d7f34ac-30ba-4f2c-90aa-c020625a647a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:48:53.975224Z",
     "start_time": "2024-10-05T13:48:53.961076Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:55:44.833900Z",
     "iopub.status.busy": "2023-02-25T15:55:44.833661Z",
     "iopub.status.idle": "2023-02-25T15:55:44.842247Z",
     "shell.execute_reply": "2023-02-25T15:55:44.841714Z",
     "shell.execute_reply.started": "2023-02-25T15:55:44.833881Z"
    },
    "id": "3d7f34ac-30ba-4f2c-90aa-c020625a647a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary = pd.DataFrame(search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ed17b1dc-6b25-44df-b6b0-3c35f30283d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:48:54.201701Z",
     "start_time": "2024-10-05T13:48:54.175469Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:55:47.662810Z",
     "iopub.status.busy": "2023-02-25T15:55:47.662509Z",
     "iopub.status.idle": "2023-02-25T15:55:47.679623Z",
     "shell.execute_reply": "2023-02-25T15:55:47.679131Z",
     "shell.execute_reply.started": "2023-02-25T15:55:47.662787Z"
    },
    "id": "ed17b1dc-6b25-44df-b6b0-3c35f30283d8",
    "outputId": "4ac65618-c896-49d4-8428-4d9ad2565fd5",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>param_l1_ratio</th>\n",
       "      <th>param_selection</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21717</th>\n",
       "      <td>0.010210</td>\n",
       "      <td>0.001543</td>\n",
       "      <td>0.011203</td>\n",
       "      <td>0.008647</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.68</td>\n",
       "      <td>random</td>\n",
       "      <td>{'alpha': 0.0, 'l1_ratio': 0.68, 'selection': ...</td>\n",
       "      <td>0.513730</td>\n",
       "      <td>0.745402</td>\n",
       "      <td>0.777483</td>\n",
       "      <td>0.754571</td>\n",
       "      <td>0.697796</td>\n",
       "      <td>0.106911</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21769</th>\n",
       "      <td>0.009128</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.004139</td>\n",
       "      <td>0.002243</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.94</td>\n",
       "      <td>random</td>\n",
       "      <td>{'alpha': 0.0, 'l1_ratio': 0.94, 'selection': ...</td>\n",
       "      <td>0.513730</td>\n",
       "      <td>0.745402</td>\n",
       "      <td>0.777483</td>\n",
       "      <td>0.754571</td>\n",
       "      <td>0.697796</td>\n",
       "      <td>0.106911</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21649</th>\n",
       "      <td>0.018649</td>\n",
       "      <td>0.009437</td>\n",
       "      <td>0.003380</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.34</td>\n",
       "      <td>random</td>\n",
       "      <td>{'alpha': 0.0, 'l1_ratio': 0.34, 'selection': ...</td>\n",
       "      <td>0.513730</td>\n",
       "      <td>0.745402</td>\n",
       "      <td>0.777483</td>\n",
       "      <td>0.754571</td>\n",
       "      <td>0.697796</td>\n",
       "      <td>0.106911</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21749</th>\n",
       "      <td>0.009487</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>0.003220</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.84</td>\n",
       "      <td>random</td>\n",
       "      <td>{'alpha': 0.0, 'l1_ratio': 0.84, 'selection': ...</td>\n",
       "      <td>0.513730</td>\n",
       "      <td>0.745402</td>\n",
       "      <td>0.777483</td>\n",
       "      <td>0.754571</td>\n",
       "      <td>0.697796</td>\n",
       "      <td>0.106911</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21741</th>\n",
       "      <td>0.009244</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.003270</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>random</td>\n",
       "      <td>{'alpha': 0.0, 'l1_ratio': 0.8, 'selection': '...</td>\n",
       "      <td>0.513730</td>\n",
       "      <td>0.745402</td>\n",
       "      <td>0.777483</td>\n",
       "      <td>0.754571</td>\n",
       "      <td>0.697796</td>\n",
       "      <td>0.106911</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21342</th>\n",
       "      <td>0.005780</td>\n",
       "      <td>0.002367</td>\n",
       "      <td>0.005930</td>\n",
       "      <td>0.004168</td>\n",
       "      <td>98</td>\n",
       "      <td>0.99</td>\n",
       "      <td>cyclic</td>\n",
       "      <td>{'alpha': 98, 'l1_ratio': 0.99, 'selection': '...</td>\n",
       "      <td>-0.003532</td>\n",
       "      <td>0.269309</td>\n",
       "      <td>0.185627</td>\n",
       "      <td>0.316971</td>\n",
       "      <td>0.192094</td>\n",
       "      <td>0.122339</td>\n",
       "      <td>23758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21558</th>\n",
       "      <td>0.003457</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.003354</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>cyclic</td>\n",
       "      <td>{'alpha': 99, 'l1_ratio': 0.98, 'selection': '...</td>\n",
       "      <td>-0.003525</td>\n",
       "      <td>0.269306</td>\n",
       "      <td>0.185624</td>\n",
       "      <td>0.316964</td>\n",
       "      <td>0.192092</td>\n",
       "      <td>0.122334</td>\n",
       "      <td>23759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21343</th>\n",
       "      <td>0.004125</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.002822</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>98</td>\n",
       "      <td>0.99</td>\n",
       "      <td>random</td>\n",
       "      <td>{'alpha': 98, 'l1_ratio': 0.99, 'selection': '...</td>\n",
       "      <td>-0.003532</td>\n",
       "      <td>0.269308</td>\n",
       "      <td>0.186050</td>\n",
       "      <td>0.316434</td>\n",
       "      <td>0.192065</td>\n",
       "      <td>0.122197</td>\n",
       "      <td>23760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21560</th>\n",
       "      <td>0.006111</td>\n",
       "      <td>0.002111</td>\n",
       "      <td>0.007834</td>\n",
       "      <td>0.004897</td>\n",
       "      <td>99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>cyclic</td>\n",
       "      <td>{'alpha': 99, 'l1_ratio': 0.99, 'selection': '...</td>\n",
       "      <td>-0.003269</td>\n",
       "      <td>0.269133</td>\n",
       "      <td>0.185461</td>\n",
       "      <td>0.316266</td>\n",
       "      <td>0.191898</td>\n",
       "      <td>0.122029</td>\n",
       "      <td>23761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21561</th>\n",
       "      <td>0.010035</td>\n",
       "      <td>0.005067</td>\n",
       "      <td>0.005088</td>\n",
       "      <td>0.003419</td>\n",
       "      <td>99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>random</td>\n",
       "      <td>{'alpha': 99, 'l1_ratio': 0.99, 'selection': '...</td>\n",
       "      <td>-0.003269</td>\n",
       "      <td>0.269108</td>\n",
       "      <td>0.185461</td>\n",
       "      <td>0.316276</td>\n",
       "      <td>0.191894</td>\n",
       "      <td>0.122027</td>\n",
       "      <td>23762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23762 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "21717       0.010210      0.001543         0.011203        0.008647   \n",
       "21769       0.009128      0.000588         0.004139        0.002243   \n",
       "21649       0.018649      0.009437         0.003380        0.000786   \n",
       "21749       0.009487      0.000907         0.003220        0.000151   \n",
       "21741       0.009244      0.000294         0.003270        0.000399   \n",
       "...              ...           ...              ...             ...   \n",
       "21342       0.005780      0.002367         0.005930        0.004168   \n",
       "21558       0.003457      0.000276         0.003354        0.000995   \n",
       "21343       0.004125      0.000960         0.002822        0.000044   \n",
       "21560       0.006111      0.002111         0.007834        0.004897   \n",
       "21561       0.010035      0.005067         0.005088        0.003419   \n",
       "\n",
       "      param_alpha param_l1_ratio param_selection  \\\n",
       "21717         0.0           0.68          random   \n",
       "21769         0.0           0.94          random   \n",
       "21649         0.0           0.34          random   \n",
       "21749         0.0           0.84          random   \n",
       "21741         0.0            0.8          random   \n",
       "...           ...            ...             ...   \n",
       "21342          98           0.99          cyclic   \n",
       "21558          99           0.98          cyclic   \n",
       "21343          98           0.99          random   \n",
       "21560          99           0.99          cyclic   \n",
       "21561          99           0.99          random   \n",
       "\n",
       "                                                  params  split0_test_score  \\\n",
       "21717  {'alpha': 0.0, 'l1_ratio': 0.68, 'selection': ...           0.513730   \n",
       "21769  {'alpha': 0.0, 'l1_ratio': 0.94, 'selection': ...           0.513730   \n",
       "21649  {'alpha': 0.0, 'l1_ratio': 0.34, 'selection': ...           0.513730   \n",
       "21749  {'alpha': 0.0, 'l1_ratio': 0.84, 'selection': ...           0.513730   \n",
       "21741  {'alpha': 0.0, 'l1_ratio': 0.8, 'selection': '...           0.513730   \n",
       "...                                                  ...                ...   \n",
       "21342  {'alpha': 98, 'l1_ratio': 0.99, 'selection': '...          -0.003532   \n",
       "21558  {'alpha': 99, 'l1_ratio': 0.98, 'selection': '...          -0.003525   \n",
       "21343  {'alpha': 98, 'l1_ratio': 0.99, 'selection': '...          -0.003532   \n",
       "21560  {'alpha': 99, 'l1_ratio': 0.99, 'selection': '...          -0.003269   \n",
       "21561  {'alpha': 99, 'l1_ratio': 0.99, 'selection': '...          -0.003269   \n",
       "\n",
       "       split1_test_score  split2_test_score  split3_test_score  \\\n",
       "21717           0.745402           0.777483           0.754571   \n",
       "21769           0.745402           0.777483           0.754571   \n",
       "21649           0.745402           0.777483           0.754571   \n",
       "21749           0.745402           0.777483           0.754571   \n",
       "21741           0.745402           0.777483           0.754571   \n",
       "...                  ...                ...                ...   \n",
       "21342           0.269309           0.185627           0.316971   \n",
       "21558           0.269306           0.185624           0.316964   \n",
       "21343           0.269308           0.186050           0.316434   \n",
       "21560           0.269133           0.185461           0.316266   \n",
       "21561           0.269108           0.185461           0.316276   \n",
       "\n",
       "       mean_test_score  std_test_score  rank_test_score  \n",
       "21717         0.697796        0.106911                1  \n",
       "21769         0.697796        0.106911                1  \n",
       "21649         0.697796        0.106911                1  \n",
       "21749         0.697796        0.106911                4  \n",
       "21741         0.697796        0.106911                4  \n",
       "...                ...             ...              ...  \n",
       "21342         0.192094        0.122339            23758  \n",
       "21558         0.192092        0.122334            23759  \n",
       "21343         0.192065        0.122197            23760  \n",
       "21560         0.191898        0.122029            23761  \n",
       "21561         0.191894        0.122027            23762  \n",
       "\n",
       "[23762 rows x 15 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.sort_values(by = \"rank_test_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cc2d8a-84b1-42c2-941f-08eac478db7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:48:54.368571Z",
     "start_time": "2024-10-05T13:48:54.361116Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:56:10.222012Z",
     "iopub.status.busy": "2023-02-25T15:56:10.221662Z",
     "iopub.status.idle": "2023-02-25T15:56:10.229448Z",
     "shell.execute_reply": "2023-02-25T15:56:10.228787Z",
     "shell.execute_reply.started": "2023-02-25T15:56:10.221987Z"
    },
    "id": "a0cc2d8a-84b1-42c2-941f-08eac478db7e",
    "outputId": "cdd70973-4eb8-477a-ab76-da7a1786ea19",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd6a5df5-c196-4dcc-b442-9529489d4f22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:48:54.527100Z",
     "start_time": "2024-10-05T13:48:54.524003Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:56:11.950603Z",
     "iopub.status.busy": "2023-02-25T15:56:11.950373Z",
     "iopub.status.idle": "2023-02-25T15:56:11.992194Z",
     "shell.execute_reply": "2023-02-25T15:56:11.991523Z",
     "shell.execute_reply.started": "2023-02-25T15:56:11.950587Z"
    },
    "id": "fd6a5df5-c196-4dcc-b442-9529489d4f22",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dc_scores[str(elnet).split(\"(\")[0]] = {\"model\": search.best_estimator_, \"score\": search.best_score_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e6b5a536-523f-4ece-afe9-f1d9d9924c8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:48:54.707624Z",
     "start_time": "2024-10-05T13:48:54.699258Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:56:12.450039Z",
     "iopub.status.busy": "2023-02-25T15:56:12.449789Z",
     "iopub.status.idle": "2023-02-25T15:56:13.882345Z",
     "shell.execute_reply": "2023-02-25T15:56:13.881731Z",
     "shell.execute_reply.started": "2023-02-25T15:56:12.450022Z"
    },
    "id": "e6b5a536-523f-4ece-afe9-f1d9d9924c8a",
    "outputId": "8563e7d1-237b-4fcf-9860-29a31f684bf9",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Lasso': {'model': Lasso(alpha=2, selection='random', tol=0.01),\n",
       "  'score': 0.29678944695787396},\n",
       " 'Ridge': {'model': Ridge(alpha=0.4, solver='svd', tol=1e-07),\n",
       "  'score': 0.6991812602048931},\n",
       " 'ElasticNet': {'model': ElasticNet(alpha=0.0, l1_ratio=0.34, selection='random'),\n",
       "  'score': 0.6977964719438308}}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be65d0d7-13fa-4d40-b3fd-944aa219d3ee",
   "metadata": {
    "id": "be65d0d7-13fa-4d40-b3fd-944aa219d3ee"
   },
   "source": [
    "## Preservación y consumo del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0c450bce-7794-450a-97c0-8bde8a507274",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:49:03.229454Z",
     "start_time": "2024-10-05T13:49:03.225197Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:58:47.073219Z",
     "iopub.status.busy": "2023-02-25T15:58:47.072941Z",
     "iopub.status.idle": "2023-02-25T15:58:47.077905Z",
     "shell.execute_reply": "2023-02-25T15:58:47.077258Z",
     "shell.execute_reply.started": "2023-02-25T15:58:47.073202Z"
    },
    "id": "0c450bce-7794-450a-97c0-8bde8a507274",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.to_pickle(dc_scores[\"Ridge\"][\"model\"], \"best_model_boston.diplo\")\n",
    "pd.to_pickle(sc, \"scaler_boston.diplo\")\n",
    "#pd.to_pickle(sc_y, \"scaler_target_boston.diplo\")\n",
    "pd.to_pickle(X_train.columns, \"features_boston.diplo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "79fb5986-e904-4bac-80ec-b68a604c4d8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:49:03.379094Z",
     "start_time": "2024-10-05T13:49:03.374964Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:58:50.075087Z",
     "iopub.status.busy": "2023-02-25T15:58:50.074857Z",
     "iopub.status.idle": "2023-02-25T15:58:50.079275Z",
     "shell.execute_reply": "2023-02-25T15:58:50.078691Z",
     "shell.execute_reply.started": "2023-02-25T15:58:50.075070Z"
    },
    "id": "79fb5986-e904-4bac-80ec-b68a604c4d8d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = pd.read_pickle(\"best_model_boston.diplo\")\n",
    "scaler = pd.read_pickle(\"scaler_boston.diplo\")\n",
    "#scaler_target = pd.read_pickle(\"scaler_target_boston.diplo\")\n",
    "features = pd.read_pickle(\"features_boston.diplo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e0371e43-29ef-4514-b601-3b038ce52726",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:49:03.557820Z",
     "start_time": "2024-10-05T13:49:03.543776Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T16:03:32.781063Z",
     "iopub.status.busy": "2023-02-25T16:03:32.780834Z",
     "iopub.status.idle": "2023-02-25T16:03:32.790020Z",
     "shell.execute_reply": "2023-02-25T16:03:32.789618Z",
     "shell.execute_reply.started": "2023-02-25T16:03:32.781027Z"
    },
    "id": "e0371e43-29ef-4514-b601-3b038ce52726",
    "outputId": "117a587b-6cd2-48bb-e170-43b57370d049",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#scaler_target.inverse_transform(model.predict(scaler.transform(df[features])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a8e3b4aa-9096-4cc8-910e-d86de999708b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:49:03.790918Z",
     "start_time": "2024-10-05T13:49:03.779950Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:58:51.767350Z",
     "iopub.status.busy": "2023-02-25T15:58:51.767126Z",
     "iopub.status.idle": "2023-02-25T15:58:51.772063Z",
     "shell.execute_reply": "2023-02-25T15:58:51.771592Z",
     "shell.execute_reply.started": "2023-02-25T15:58:51.767334Z"
    },
    "id": "a8e3b4aa-9096-4cc8-910e-d86de999708b",
    "outputId": "089cc719-3f34-4757-bcd2-ff049285451d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/base.py:441: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but Ridge was fitted with feature names\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X[\"y_hat\"] = model.predict(scaler.transform(X[features]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f32464d8-7c41-4476-a076-0732646e0d7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:49:04.011437Z",
     "start_time": "2024-10-05T13:49:03.978763Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:58:52.478483Z",
     "iopub.status.busy": "2023-02-25T15:58:52.477893Z",
     "iopub.status.idle": "2023-02-25T15:58:52.502582Z",
     "shell.execute_reply": "2023-02-25T15:58:52.501605Z",
     "shell.execute_reply.started": "2023-02-25T15:58:52.478432Z"
    },
    "id": "f32464d8-7c41-4476-a076-0732646e0d7e",
    "outputId": "8d0439be-0f96-422c-95c9-0fec5f016ff5",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>y_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>40.911722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>44.945998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>49.006062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>48.193735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>48.702375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>35.999109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>33.938143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>38.383096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>37.285337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>33.451944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
       "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
       "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
       "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
       "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
       "505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT      y_hat  \n",
       "0       15.3  396.90   4.98  40.911722  \n",
       "1       17.8  396.90   9.14  44.945998  \n",
       "2       17.8  392.83   4.03  49.006062  \n",
       "3       18.7  394.63   2.94  48.193735  \n",
       "4       18.7  396.90   5.33  48.702375  \n",
       "..       ...     ...    ...        ...  \n",
       "501     21.0  391.99   9.67  35.999109  \n",
       "502     21.0  396.90   9.08  33.938143  \n",
       "503     21.0  396.90   5.64  38.383096  \n",
       "504     21.0  393.45   6.48  37.285337  \n",
       "505     21.0  396.90   7.88  33.451944  \n",
       "\n",
       "[506 rows x 14 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7ebe69-1a64-4f9b-8079-2a2627e68a9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:49:04.225384Z",
     "start_time": "2024-10-05T13:49:04.219300Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:59:27.153845Z",
     "iopub.status.busy": "2023-02-25T15:59:27.153378Z",
     "iopub.status.idle": "2023-02-25T15:59:27.158646Z",
     "shell.execute_reply": "2023-02-25T15:59:27.158119Z",
     "shell.execute_reply.started": "2023-02-25T15:59:27.153821Z"
    },
    "id": "8a7ebe69-1a64-4f9b-8079-2a2627e68a9d",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c71a7f4d-f4f7-42e2-b91f-8f4de7988c61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:49:04.396061Z",
     "start_time": "2024-10-05T13:49:04.393007Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:59:27.731322Z",
     "iopub.status.busy": "2023-02-25T15:59:27.730837Z",
     "iopub.status.idle": "2023-02-25T15:59:27.734306Z",
     "shell.execute_reply": "2023-02-25T15:59:27.733675Z",
     "shell.execute_reply.started": "2023-02-25T15:59:27.731300Z"
    },
    "id": "c71a7f4d-f4f7-42e2-b91f-8f4de7988c61",
    "tags": []
   },
   "outputs": [],
   "source": [
    "X[\"y\"] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f85a96c8-620b-4bb8-99f1-e3f5026b43b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T13:49:04.580423Z",
     "start_time": "2024-10-05T13:49:04.556609Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-25T15:59:30.233342Z",
     "iopub.status.busy": "2023-02-25T15:59:30.233054Z",
     "iopub.status.idle": "2023-02-25T15:59:30.255607Z",
     "shell.execute_reply": "2023-02-25T15:59:30.255063Z",
     "shell.execute_reply.started": "2023-02-25T15:59:30.233318Z"
    },
    "id": "f85a96c8-620b-4bb8-99f1-e3f5026b43b8",
    "outputId": "d2d474fa-9518-4089-9542-f0639d1f5658",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>y_hat</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>40.911722</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>44.945998</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>49.006062</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>48.193735</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>48.702375</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>35.999109</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>33.938143</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>38.383096</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>37.285337</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>33.451944</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
       "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
       "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
       "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
       "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
       "505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT      y_hat     y  \n",
       "0       15.3  396.90   4.98  40.911722  24.0  \n",
       "1       17.8  396.90   9.14  44.945998  21.6  \n",
       "2       17.8  392.83   4.03  49.006062  34.7  \n",
       "3       18.7  394.63   2.94  48.193735  33.4  \n",
       "4       18.7  396.90   5.33  48.702375  36.2  \n",
       "..       ...     ...    ...        ...   ...  \n",
       "501     21.0  391.99   9.67  35.999109  22.4  \n",
       "502     21.0  396.90   9.08  33.938143  20.6  \n",
       "503     21.0  396.90   5.64  38.383096  23.9  \n",
       "504     21.0  393.45   6.48  37.285337  22.0  \n",
       "505     21.0  396.90   7.88  33.451944  11.9  \n",
       "\n",
       "[506 rows x 15 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d472aceb-0e98-4ca5-bb02-d687b64f57cd",
   "metadata": {
    "id": "d472aceb-0e98-4ca5-bb02-d687b64f57cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso(alpha=2, selection='random', tol=0.01)\n",
      "0.29678944695787396\n",
      "0.6091250714730269\n",
      "Ridge(alpha=0.4, solver='svd', tol=1e-07)\n",
      "0.6991812602048931\n",
      "0.7214351028077954\n",
      "ElasticNet(alpha=0.0, l1_ratio=0.34, selection='random')\n",
      "0.6977964719438308\n",
      "0.7237945767258424\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "for model in dc_scores.values():\n",
    "    print(model[\"model\"])\n",
    "    print(model[\"score\"])\n",
    "    print(r2_score(y_test, model[\"model\"].predict(X_test)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3f5950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c499e5e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c93397cd",
   "metadata": {},
   "source": [
    "## Búsqueda de la mejor combinación de modelo y escalador\n",
    "En esta sección se probarán diferentes modelos de regresión, escaladores y combinaciones de hiperparámetros para encontrar la mejor pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3f565b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/home/dbh/envs/diplo_cdd/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaler</th>\n",
       "      <th>model</th>\n",
       "      <th>best_params</th>\n",
       "      <th>best_score</th>\n",
       "      <th>best_estimator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>Ridge</td>\n",
       "      <td>{'model__alpha': 10, 'model__solver': 'lsqr', ...</td>\n",
       "      <td>0.701749</td>\n",
       "      <td>(RobustScaler(), Ridge(alpha=10, solver='lsqr'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>BayesianRidge</td>\n",
       "      <td>{'model__alpha_1': 1e-06, 'model__alpha_2': 0....</td>\n",
       "      <td>0.701015</td>\n",
       "      <td>(RobustScaler(), BayesianRidge(alpha_2=0.0001))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>{'model__alpha': 0.01, 'model__l1_ratio': 0.1,...</td>\n",
       "      <td>0.700622</td>\n",
       "      <td>(RobustScaler(), ElasticNet(alpha=0.01, l1_rat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>BayesianRidge</td>\n",
       "      <td>{'model__alpha_1': 1e-06, 'model__alpha_2': 0....</td>\n",
       "      <td>0.699633</td>\n",
       "      <td>(MinMaxScaler(), BayesianRidge(alpha_2=0.0001))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>BayesianRidge</td>\n",
       "      <td>{'model__alpha_1': 1e-06, 'model__alpha_2': 1e...</td>\n",
       "      <td>0.699472</td>\n",
       "      <td>(StandardScaler(), BayesianRidge())</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           scaler          model  \\\n",
       "0    RobustScaler          Ridge   \n",
       "1    RobustScaler  BayesianRidge   \n",
       "2    RobustScaler     ElasticNet   \n",
       "3    MinMaxScaler  BayesianRidge   \n",
       "4  StandardScaler  BayesianRidge   \n",
       "\n",
       "                                         best_params  best_score  \\\n",
       "0  {'model__alpha': 10, 'model__solver': 'lsqr', ...    0.701749   \n",
       "1  {'model__alpha_1': 1e-06, 'model__alpha_2': 0....    0.701015   \n",
       "2  {'model__alpha': 0.01, 'model__l1_ratio': 0.1,...    0.700622   \n",
       "3  {'model__alpha_1': 1e-06, 'model__alpha_2': 0....    0.699633   \n",
       "4  {'model__alpha_1': 1e-06, 'model__alpha_2': 1e...    0.699472   \n",
       "\n",
       "                                      best_estimator  \n",
       "0  (RobustScaler(), Ridge(alpha=10, solver='lsqr'...  \n",
       "1    (RobustScaler(), BayesianRidge(alpha_2=0.0001))  \n",
       "2  (RobustScaler(), ElasticNet(alpha=0.01, l1_rat...  \n",
       "3    (MinMaxScaler(), BayesianRidge(alpha_2=0.0001))  \n",
       "4                (StandardScaler(), BayesianRidge())  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet, BayesianRidge, Lars\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "scalers = {\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'MinMaxScaler': MinMaxScaler(),\n",
    "    'RobustScaler': RobustScaler()\n",
    "}\n",
    "models = {\n",
    "    'Lasso': (Lasso(), {\n",
    "        'model__alpha': [0.01, 0.1, 1, 10],\n",
    "        'model__tol': [1e-4, 1e-6],\n",
    "        'model__selection': ['cyclic', 'random']\n",
    "    }),\n",
    "    'Ridge': (Ridge(), {\n",
    "        'model__alpha': [0.01, 0.1, 1, 10],\n",
    "        'model__tol': [1e-4, 1e-6],\n",
    "        'model__solver': ['auto', 'svd', 'cholesky', 'lsqr']\n",
    "    }),\n",
    "    'ElasticNet': (ElasticNet(), {\n",
    "        'model__alpha': [0.01, 0.1, 1, 10],\n",
    "        'model__l1_ratio': [0.1, 0.5, 0.9],\n",
    "        'model__selection': ['cyclic', 'random']\n",
    "    }),\n",
    "    'BayesianRidge': (BayesianRidge(), {\n",
    "        'model__alpha_1': [1e-6, 1e-4],\n",
    "        'model__alpha_2': [1e-6, 1e-4]\n",
    "    }),\n",
    "    'Lars': (Lars(), {\n",
    "        'model__n_nonzero_coefs': [1, 2, 3, 4, 5]\n",
    "    })\n",
    "}\n",
    "results = []\n",
    "for scaler_name, scaler in scalers.items():\n",
    "    for model_name, (model, param_grid) in models.items():\n",
    "        pipe = Pipeline([('scaler', scaler), ('model', model)])\n",
    "        search = GridSearchCV(pipe, param_grid, cv=4, scoring='r2', n_jobs=-1, error_score='raise', verbose=0)\n",
    "        search.fit(X_train, y_train.values.ravel())\n",
    "        score = search.best_score_\n",
    "        results.append({\n",
    "            'scaler': scaler_name,\n",
    "            'model': model_name,\n",
    "            'best_params': search.best_params_,\n",
    "            'best_score': score,\n",
    "            'best_estimator': search.best_estimator_\n",
    "        })\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by='best_score', ascending=False).reset_index(drop=True)\n",
    "results_df.to_pickle('all_model_scaler_results.diplo')\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "31fd0d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaler</th>\n",
       "      <th>model</th>\n",
       "      <th>best_params</th>\n",
       "      <th>best_score</th>\n",
       "      <th>best_estimator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>Ridge</td>\n",
       "      <td>{'model__alpha': 10, 'model__solver': 'lsqr', ...</td>\n",
       "      <td>0.701749</td>\n",
       "      <td>(RobustScaler(), Ridge(alpha=10, solver='lsqr'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>BayesianRidge</td>\n",
       "      <td>{'model__alpha_1': 1e-06, 'model__alpha_2': 0....</td>\n",
       "      <td>0.701015</td>\n",
       "      <td>(RobustScaler(), BayesianRidge(alpha_2=0.0001))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>{'model__alpha': 0.01, 'model__l1_ratio': 0.1,...</td>\n",
       "      <td>0.700622</td>\n",
       "      <td>(RobustScaler(), ElasticNet(alpha=0.01, l1_rat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>BayesianRidge</td>\n",
       "      <td>{'model__alpha_1': 1e-06, 'model__alpha_2': 0....</td>\n",
       "      <td>0.699633</td>\n",
       "      <td>(MinMaxScaler(), BayesianRidge(alpha_2=0.0001))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>BayesianRidge</td>\n",
       "      <td>{'model__alpha_1': 1e-06, 'model__alpha_2': 1e...</td>\n",
       "      <td>0.699472</td>\n",
       "      <td>(StandardScaler(), BayesianRidge())</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>Ridge</td>\n",
       "      <td>{'model__alpha': 10, 'model__solver': 'lsqr', ...</td>\n",
       "      <td>0.699145</td>\n",
       "      <td>(StandardScaler(), Ridge(alpha=10, solver='lsq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>{'model__alpha': 0.01, 'model__l1_ratio': 0.1,...</td>\n",
       "      <td>0.698728</td>\n",
       "      <td>(StandardScaler(), ElasticNet(alpha=0.01, l1_r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>Ridge</td>\n",
       "      <td>{'model__alpha': 0.1, 'model__solver': 'lsqr',...</td>\n",
       "      <td>0.698701</td>\n",
       "      <td>(MinMaxScaler(), Ridge(alpha=0.1, solver='lsqr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>Lasso</td>\n",
       "      <td>{'model__alpha': 0.01, 'model__selection': 'ra...</td>\n",
       "      <td>0.698345</td>\n",
       "      <td>(RobustScaler(), Lasso(alpha=0.01, selection='...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>Lasso</td>\n",
       "      <td>{'model__alpha': 0.01, 'model__selection': 'cy...</td>\n",
       "      <td>0.698066</td>\n",
       "      <td>(StandardScaler(), Lasso(alpha=0.01, tol=1e-06))</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           scaler          model  \\\n",
       "0    RobustScaler          Ridge   \n",
       "1    RobustScaler  BayesianRidge   \n",
       "2    RobustScaler     ElasticNet   \n",
       "3    MinMaxScaler  BayesianRidge   \n",
       "4  StandardScaler  BayesianRidge   \n",
       "5  StandardScaler          Ridge   \n",
       "6  StandardScaler     ElasticNet   \n",
       "7    MinMaxScaler          Ridge   \n",
       "8    RobustScaler          Lasso   \n",
       "9  StandardScaler          Lasso   \n",
       "\n",
       "                                         best_params  best_score  \\\n",
       "0  {'model__alpha': 10, 'model__solver': 'lsqr', ...    0.701749   \n",
       "1  {'model__alpha_1': 1e-06, 'model__alpha_2': 0....    0.701015   \n",
       "2  {'model__alpha': 0.01, 'model__l1_ratio': 0.1,...    0.700622   \n",
       "3  {'model__alpha_1': 1e-06, 'model__alpha_2': 0....    0.699633   \n",
       "4  {'model__alpha_1': 1e-06, 'model__alpha_2': 1e...    0.699472   \n",
       "5  {'model__alpha': 10, 'model__solver': 'lsqr', ...    0.699145   \n",
       "6  {'model__alpha': 0.01, 'model__l1_ratio': 0.1,...    0.698728   \n",
       "7  {'model__alpha': 0.1, 'model__solver': 'lsqr',...    0.698701   \n",
       "8  {'model__alpha': 0.01, 'model__selection': 'ra...    0.698345   \n",
       "9  {'model__alpha': 0.01, 'model__selection': 'cy...    0.698066   \n",
       "\n",
       "                                      best_estimator  \n",
       "0  (RobustScaler(), Ridge(alpha=10, solver='lsqr'...  \n",
       "1    (RobustScaler(), BayesianRidge(alpha_2=0.0001))  \n",
       "2  (RobustScaler(), ElasticNet(alpha=0.01, l1_rat...  \n",
       "3    (MinMaxScaler(), BayesianRidge(alpha_2=0.0001))  \n",
       "4                (StandardScaler(), BayesianRidge())  \n",
       "5  (StandardScaler(), Ridge(alpha=10, solver='lsq...  \n",
       "6  (StandardScaler(), ElasticNet(alpha=0.01, l1_r...  \n",
       "7  (MinMaxScaler(), Ridge(alpha=0.1, solver='lsqr...  \n",
       "8  (RobustScaler(), Lasso(alpha=0.01, selection='...  \n",
       "9   (StandardScaler(), Lasso(alpha=0.01, tol=1e-06))  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.sort_values(by='best_score', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6dbdbd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor combinación:\n",
      "Escalador: RobustScaler\n",
      "Modelo: Ridge\n",
      "Mejores hiperparámetros: {'model__alpha': 10, 'model__solver': 'lsqr', 'model__tol': 1e-06}\n",
      "Mejor score: 0.7017490011757013\n"
     ]
    }
   ],
   "source": [
    "# Seleccionar el mejor pipeline y guardarlo\n",
    "best_row = results_df.iloc[0]\n",
    "best_pipeline = best_row['best_estimator']\n",
    "import joblib\n",
    "joblib.dump(best_pipeline, 'best_pipeline_boston.diplo')\n",
    "print('Mejor combinación:')\n",
    "print('Escalador:', best_row['scaler'])\n",
    "print('Modelo:', best_row['model'])\n",
    "print('Mejores hiperparámetros:', best_row['best_params'])\n",
    "print('Mejor score:', best_row['best_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d285ea9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 en test: 0.7283\n",
      "MSE en test: 24.9688\n",
      "MAE en test: 3.4629\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Predecir en el conjunto de test con el mejor pipeline\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "\n",
    "# Calcular métricas\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"R2 en test: {r2:.4f}\")\n",
    "print(f\"MSE en test: {mse:.4f}\")\n",
    "print(f\"MAE en test: {mae:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
