{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f75e6d95",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ellipsis' object has no attribute 'sort_values'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# 1) Preparar datos\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[32m     59\u001b[39m df = ...  \u001b[38;5;66;03m# <- aquí cargas lo que extraigas de FoLiA a este esquema\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m df = \u001b[43mbuild_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m df = df.dropna(subset=[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33my_cont\u001b[39m\u001b[33m\"\u001b[39m])              \u001b[38;5;66;03m# sin target continuo no entrenas regresión\u001b[39;00m\n\u001b[32m     62\u001b[39m df = df[df[\u001b[33m\"\u001b[39m\u001b[33my_cont\u001b[39m\u001b[33m\"\u001b[39m] >= \u001b[32m0\u001b[39m]                             \u001b[38;5;66;03m# sanity check\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mbuild_targets\u001b[39m\u001b[34m(df, fast_minutes)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild_targets\u001b[39m(df: pd.DataFrame, fast_minutes=\u001b[32m5\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     df = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43msort_values\u001b[49m([\u001b[33m\"\u001b[39m\u001b[33mchat_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m\"\u001b[39m]).copy()\n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# siguiente mensaje (misma conversación) donde cambia speaker\u001b[39;00m\n\u001b[32m     19\u001b[39m     df[\u001b[33m\"\u001b[39m\u001b[33mnext_speaker\u001b[39m\u001b[33m\"\u001b[39m] = df.groupby(\u001b[33m\"\u001b[39m\u001b[33mchat_id\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[33m\"\u001b[39m\u001b[33mspeaker\u001b[39m\u001b[33m\"\u001b[39m].shift(-\u001b[32m1\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'ellipsis' object has no attribute 'sort_values'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor, Ridge\n",
    "from sklearn.metrics import roc_auc_score, r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# df esperado (tú lo construyes desde FoLiA):\n",
    "# df columns: chat_id, speaker, timestamp (datetime64), text (str)\n",
    "\n",
    "def build_targets(df: pd.DataFrame, fast_minutes=5):\n",
    "    df = df.sort_values([\"chat_id\", \"timestamp\"]).copy()\n",
    "\n",
    "    # siguiente mensaje (misma conversación) donde cambia speaker\n",
    "    df[\"next_speaker\"] = df.groupby(\"chat_id\")[\"speaker\"].shift(-1)\n",
    "    df[\"next_time\"]    = df.groupby(\"chat_id\")[\"timestamp\"].shift(-1)\n",
    "\n",
    "    # buscamos el siguiente evento donde next_speaker != speaker\n",
    "    # (truco: iterar por chat; simple y claro)\n",
    "    delta = []\n",
    "    for _, g in df.groupby(\"chat_id\", sort=False):\n",
    "        t = g[\"timestamp\"].to_numpy()\n",
    "        sp = g[\"speaker\"].to_numpy()\n",
    "        dt = np.full(len(g), np.nan, dtype=\"float64\")\n",
    "        for i in range(len(g)-1):\n",
    "            j = i + 1\n",
    "            while j < len(g) and sp[j] == sp[i]:\n",
    "                j += 1\n",
    "            if j < len(g):\n",
    "                dt[i] = (t[j] - t[i]) / np.timedelta64(1, \"s\")\n",
    "        delta.append(dt)\n",
    "    df[\"reply_seconds\"] = np.concatenate(delta)\n",
    "\n",
    "    # Target continua\n",
    "    df[\"y_cont\"] = df[\"reply_seconds\"]\n",
    "\n",
    "    # Target discreta (elige UNA de estas dos):\n",
    "    # Opción interpretable por umbral fijo:\n",
    "    df[\"y_disc\"] = (df[\"reply_seconds\"] <= fast_minutes*60).astype(int)\n",
    "\n",
    "    # alternativa estilo tu notebook: umbral = media del train (lo haces después del split)\n",
    "    return df\n",
    "\n",
    "def make_features(df):\n",
    "    out = df.copy()\n",
    "    out[\"len_chars\"] = out[\"text\"].fillna(\"\").str.len()\n",
    "    out[\"n_qmark\"]   = out[\"text\"].fillna(\"\").str.count(r\"\\?\")\n",
    "    out[\"hour\"]      = out[\"timestamp\"].dt.hour\n",
    "    out[\"dow\"]       = out[\"timestamp\"].dt.dayofweek\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# 1) Preparar datos\n",
    "# =========================\n",
    "df = ...  # <- aquí cargas lo que extraigas de FoLiA a este esquema\n",
    "df = build_targets(df)\n",
    "df = df.dropna(subset=[\"text\", \"y_cont\"])              # sin target continuo no entrenas regresión\n",
    "df = df[df[\"y_cont\"] >= 0]                             # sanity check\n",
    "df = make_features(df)\n",
    "\n",
    "X = df[[\"text\", \"len_chars\", \"n_qmark\", \"hour\", \"dow\"]]\n",
    "y_cls = df[\"y_disc\"].values\n",
    "y_reg = np.log1p(df[\"y_cont\"].values)                  # log1p recomendado\n",
    "groups = df[\"chat_id\"].values\n",
    "\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"txt\", TfidfVectorizer(ngram_range=(1,2), min_df=3, max_df=0.9), \"text\"),\n",
    "        (\"num\", StandardScaler(with_mean=False), [\"len_chars\", \"n_qmark\", \"hour\", \"dow\"]),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 2) Clasificación (SGDClassifier)\n",
    "# =========================\n",
    "clf = SGDClassifier(random_state=777)\n",
    "\n",
    "pipe_cls = Pipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"model\", clf),\n",
    "])\n",
    "\n",
    "param_dist_cls = {\n",
    "    \"model__loss\": [\"log_loss\", \"modified_huber\", \"hinge\"],\n",
    "    \"model__penalty\": [\"l2\", \"l1\", \"elasticnet\"],\n",
    "    \"model__alpha\": np.logspace(-6, -2, 30),\n",
    "    \"model__l1_ratio\": np.linspace(0, 1, 11),\n",
    "}\n",
    "\n",
    "cv = GroupKFold(n_splits=4)\n",
    "\n",
    "search_cls = RandomizedSearchCV(\n",
    "    pipe_cls,\n",
    "    param_distributions=param_dist_cls,\n",
    "    n_iter=40,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    random_state=777,\n",
    "    error_score=-1000,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "search_cls.fit(X, y_cls, groups=groups)\n",
    "best_cls = search_cls.best_estimator_\n",
    "print(\"Best ROC-AUC (CV):\", search_cls.best_score_)\n",
    "\n",
    "# =========================\n",
    "# 3) Regresión (Ridge o SGDRegressor)\n",
    "# =========================\n",
    "pipe_reg = Pipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"model\", Ridge()),\n",
    "])\n",
    "\n",
    "param_grid_reg = {\n",
    "    \"model__alpha\": np.logspace(-2, 3, 20),\n",
    "}\n",
    "\n",
    "search_reg = GridSearchCV(\n",
    "    pipe_reg,\n",
    "    param_grid=param_grid_reg,\n",
    "    scoring=\"r2\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    error_score=-1000,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "search_reg.fit(X, y_reg, groups=groups)\n",
    "best_reg = search_reg.best_estimator_\n",
    "print(\"Best R2 (CV):\", search_reg.best_score_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
